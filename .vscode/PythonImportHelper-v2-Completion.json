[
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "AsyncWebCrawler",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "CacheMode",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "CrawlerRunConfig",
        "importPath": "crawl4ai",
        "description": "crawl4ai",
        "isExtraImport": true,
        "detail": "crawl4ai",
        "documentation": {}
    },
    {
        "label": "arxiv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "arxiv",
        "description": "arxiv",
        "detail": "arxiv",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "extract_text",
        "importPath": "pdfminer.high_level",
        "description": "pdfminer.high_level",
        "isExtraImport": true,
        "detail": "pdfminer.high_level",
        "documentation": {}
    },
    {
        "label": "SemanticScholar",
        "importPath": "semanticscholar",
        "description": "semanticscholar",
        "isExtraImport": true,
        "detail": "semanticscholar",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "logger",
        "importPath": "log",
        "description": "log",
        "isExtraImport": true,
        "detail": "log",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "rerank_model_url",
        "importPath": "generation.global_config",
        "description": "generation.global_config",
        "isExtraImport": true,
        "detail": "generation.global_config",
        "documentation": {}
    },
    {
        "label": "recall_server_url",
        "importPath": "generation.global_config",
        "description": "generation.global_config",
        "isExtraImport": true,
        "detail": "generation.global_config",
        "documentation": {}
    },
    {
        "label": "tarfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tarfile",
        "description": "tarfile",
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "fitz",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fitz",
        "description": "fitz",
        "detail": "fitz",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "importPath": "local_request_v2",
        "description": "local_request_v2",
        "isExtraImport": true,
        "detail": "local_request_v2",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "asynccontextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "fields",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "BackgroundTasks",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "FileResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "flow_information_sync",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "keep_letters",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "keep_letters",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "flow_information_sync",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "get_md5",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "keep_letters",
        "importPath": "generation.utils",
        "description": "generation.utils",
        "isExtraImport": true,
        "detail": "generation.utils",
        "documentation": {}
    },
    {
        "label": "get_doc_info_from_api",
        "importPath": "generation.websearch_scholar",
        "description": "generation.websearch_scholar",
        "isExtraImport": true,
        "detail": "generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_paper_via_query_from_openalex",
        "importPath": "generation.websearch_scholar",
        "description": "generation.websearch_scholar",
        "isExtraImport": true,
        "detail": "generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "get_doc_info_from_api",
        "importPath": "generation.websearch_scholar",
        "description": "generation.websearch_scholar",
        "isExtraImport": true,
        "detail": "generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "template_extract_keywords_source_aware",
        "importPath": "generation.generation_instructions",
        "description": "generation.generation_instructions",
        "isExtraImport": true,
        "detail": "generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "search_and_crawl_data_only",
        "importPath": "generation.websearch_general",
        "description": "generation.websearch_general",
        "isExtraImport": true,
        "detail": "generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "generation.instruction_pro",
        "description": "generation.instruction_pro",
        "isExtraImport": true,
        "detail": "generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "PAGE_REFINE_PROMPT",
        "importPath": "generation.instruction_pro",
        "description": "generation.instruction_pro",
        "isExtraImport": true,
        "detail": "generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "SIMILARITY_PROMPT",
        "importPath": "generation.instruction_pro",
        "description": "generation.instruction_pro",
        "isExtraImport": true,
        "detail": "generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "run_requests_parallel",
        "importPath": "generation.retrival",
        "description": "generation.retrival",
        "isExtraImport": true,
        "detail": "generation.retrival",
        "documentation": {}
    },
    {
        "label": "rerank_papers_hybrid",
        "importPath": "generation.reranker",
        "description": "generation.reranker",
        "isExtraImport": true,
        "detail": "generation.reranker",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "validator",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ValidationError",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "CHAT_MODEL_NAME",
        "importPath": "generation.generation_config",
        "description": "generation.generation_config",
        "isExtraImport": true,
        "detail": "generation.generation_config",
        "documentation": {}
    },
    {
        "label": "process_ctx",
        "importPath": "generation.engine",
        "description": "generation.engine",
        "isExtraImport": true,
        "detail": "generation.engine",
        "documentation": {}
    },
    {
        "label": "process_paragraph",
        "importPath": "generation.engine",
        "description": "generation.engine",
        "isExtraImport": true,
        "detail": "generation.engine",
        "documentation": {}
    },
    {
        "label": "get_arxiv_main_figure",
        "importPath": "generation.extract_main_figure",
        "description": "generation.extract_main_figure",
        "isExtraImport": true,
        "detail": "generation.extract_main_figure",
        "documentation": {}
    },
    {
        "label": "DocumentRelevanceEvaluator",
        "importPath": "generation.relevance",
        "description": "generation.relevance",
        "isExtraImport": true,
        "detail": "generation.relevance",
        "documentation": {}
    },
    {
        "label": "jsonlines",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jsonlines",
        "description": "jsonlines",
        "detail": "jsonlines",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "nest_asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nest_asyncio",
        "description": "nest_asyncio",
        "detail": "nest_asyncio",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "RequestWrapper",
        "importPath": "request_warp",
        "description": "request_warp",
        "isExtraImport": true,
        "detail": "request_warp",
        "documentation": {}
    },
    {
        "label": "Entrez",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "db_path",
        "importPath": "generation.local_db_v2",
        "description": "generation.local_db_v2",
        "isExtraImport": true,
        "detail": "generation.local_db_v2",
        "documentation": {}
    },
    {
        "label": "ArxivDatabase",
        "importPath": "generation.local_db_v2",
        "description": "generation.local_db_v2",
        "isExtraImport": true,
        "detail": "generation.local_db_v2",
        "documentation": {}
    },
    {
        "label": "openai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openai",
        "description": "openai",
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "InternalServerError",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "RateLimitError",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "APIError",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "OpenAI",
        "importPath": "openai",
        "description": "openai",
        "isExtraImport": true,
        "detail": "openai",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "stop_after_attempt",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "wait_random_exponential",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "before_sleep_log",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "retry_if_exception_type",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "stop_after_attempt",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "wait_random_exponential",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "retry_if_exception_type",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "stop_after_attempt",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "wait_random_exponential",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "before_sleep_log",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "retry_if_exception_type",
        "importPath": "tenacity",
        "description": "tenacity",
        "isExtraImport": true,
        "detail": "tenacity",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "genai",
        "importPath": "google",
        "description": "google",
        "isExtraImport": true,
        "detail": "google",
        "documentation": {}
    },
    {
        "label": "genai",
        "importPath": "google",
        "description": "google",
        "isExtraImport": true,
        "detail": "google",
        "documentation": {}
    },
    {
        "label": "types",
        "importPath": "google.genai",
        "description": "google.genai",
        "isExtraImport": true,
        "detail": "google.genai",
        "documentation": {}
    },
    {
        "label": "HTTPError",
        "importPath": "requests.exceptions",
        "description": "requests.exceptions",
        "isExtraImport": true,
        "detail": "requests.exceptions",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "JSONDecodeError",
        "importPath": "json.decoder",
        "description": "json.decoder",
        "isExtraImport": true,
        "detail": "json.decoder",
        "documentation": {}
    },
    {
        "label": "tabulate",
        "importPath": "tabulate",
        "description": "tabulate",
        "isExtraImport": true,
        "detail": "tabulate",
        "documentation": {}
    },
    {
        "label": "Semaphore",
        "importPath": "gevent.lock",
        "description": "gevent.lock",
        "isExtraImport": true,
        "detail": "gevent.lock",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "CORSMiddleware",
        "importPath": "fastapi.middleware.cors",
        "description": "fastapi.middleware.cors",
        "isExtraImport": true,
        "detail": "fastapi.middleware.cors",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "Lock",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "PaperGenerationPipeline",
        "importPath": "main_workflow_opt_for_paper",
        "description": "main_workflow_opt_for_paper",
        "isExtraImport": true,
        "detail": "main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "save_results",
        "importPath": "main_workflow_opt_for_paper",
        "description": "main_workflow_opt_for_paper",
        "isExtraImport": true,
        "detail": "main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "LangGraphService",
        "importPath": "langgraph_service",
        "description": "langgraph_service",
        "isExtraImport": true,
        "detail": "langgraph_service",
        "documentation": {}
    },
    {
        "label": "func_set_timeout",
        "importPath": "func_timeout",
        "description": "func_timeout",
        "isExtraImport": true,
        "detail": "func_timeout",
        "documentation": {}
    },
    {
        "label": "func_set_timeout",
        "importPath": "func_timeout",
        "description": "func_timeout",
        "isExtraImport": true,
        "detail": "func_timeout",
        "documentation": {}
    },
    {
        "label": "func_set_timeout",
        "importPath": "func_timeout",
        "description": "func_timeout",
        "isExtraImport": true,
        "detail": "func_timeout",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "HTTPAdapter",
        "importPath": "requests.adapters",
        "description": "requests.adapters",
        "isExtraImport": true,
        "detail": "requests.adapters",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "Retry",
        "importPath": "requests.packages.urllib3.util.retry",
        "description": "requests.packages.urllib3.util.retry",
        "isExtraImport": true,
        "detail": "requests.packages.urllib3.util.retry",
        "documentation": {}
    },
    {
        "label": "TTLCache",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "cachedmethod",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "TTLCache",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "cachedmethod",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "TTLCache",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "cachedmethod",
        "importPath": "cachetools",
        "description": "cachetools",
        "isExtraImport": true,
        "detail": "cachetools",
        "documentation": {}
    },
    {
        "label": "operator",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "operator",
        "description": "operator",
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "hashkey",
        "importPath": "cachetools.keys",
        "description": "cachetools.keys",
        "isExtraImport": true,
        "detail": "cachetools.keys",
        "documentation": {}
    },
    {
        "label": "hashkey",
        "importPath": "cachetools.keys",
        "description": "cachetools.keys",
        "isExtraImport": true,
        "detail": "cachetools.keys",
        "documentation": {}
    },
    {
        "label": "hashkey",
        "importPath": "cachetools.keys",
        "description": "cachetools.keys",
        "isExtraImport": true,
        "detail": "cachetools.keys",
        "documentation": {}
    },
    {
        "label": "BaseChatModel",
        "importPath": "langchain_core.language_models.chat_models",
        "description": "langchain_core.language_models.chat_models",
        "isExtraImport": true,
        "detail": "langchain_core.language_models.chat_models",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "langchain_core.pydantic_v1",
        "description": "langchain_core.pydantic_v1",
        "isExtraImport": true,
        "detail": "langchain_core.pydantic_v1",
        "documentation": {}
    },
    {
        "label": "root_validator",
        "importPath": "langchain_core.pydantic_v1",
        "description": "langchain_core.pydantic_v1",
        "isExtraImport": true,
        "detail": "langchain_core.pydantic_v1",
        "documentation": {}
    },
    {
        "label": "Extra",
        "importPath": "langchain_core.pydantic_v1",
        "description": "langchain_core.pydantic_v1",
        "isExtraImport": true,
        "detail": "langchain_core.pydantic_v1",
        "documentation": {}
    },
    {
        "label": "CallbackManagerForLLMRun",
        "importPath": "langchain_core.callbacks.manager",
        "description": "langchain_core.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain_core.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "AsyncCallbackManagerForLLMRun",
        "importPath": "langchain_core.callbacks.manager",
        "description": "langchain_core.callbacks.manager",
        "isExtraImport": true,
        "detail": "langchain_core.callbacks.manager",
        "documentation": {}
    },
    {
        "label": "ChatGeneration",
        "importPath": "langchain_core.outputs",
        "description": "langchain_core.outputs",
        "isExtraImport": true,
        "detail": "langchain_core.outputs",
        "documentation": {}
    },
    {
        "label": "ChatResult",
        "importPath": "langchain_core.outputs",
        "description": "langchain_core.outputs",
        "isExtraImport": true,
        "detail": "langchain_core.outputs",
        "documentation": {}
    },
    {
        "label": "ChatGenerationChunk",
        "importPath": "langchain_core.outputs",
        "description": "langchain_core.outputs",
        "isExtraImport": true,
        "detail": "langchain_core.outputs",
        "documentation": {}
    },
    {
        "label": "LLMResult",
        "importPath": "langchain_core.outputs",
        "description": "langchain_core.outputs",
        "isExtraImport": true,
        "detail": "langchain_core.outputs",
        "documentation": {}
    },
    {
        "label": "RunnableConfig",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "configuration",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "configuration",
        "description": "configuration",
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "MODEL_CONFIGS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "MODEL_CONFIGS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "OUTLINE_REFLECTION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "OUTLINE_MAX_SECTIONS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "OUTLINE_MIN_DEPTH",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "MAX_SECTION_RETRY_NUM",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DO_SELECT_REFLECTION",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_REFLECTION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DO_GLOBAL_REFLECTION",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "GLOBAL_REFLECTION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "GLOBAL_ABSTRACT_CONCLUSION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEBUG_KEY_POINTS_LIMIT",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_RAG_SERVICE_URL",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEBUG",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "GLOBAL_ABSTRACT_CONCLUSION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "MODEL_GEN_ABSTRACT_CONCLUSION",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_GLOBAL_REFLECTION",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "GLOBAL_REFLECTION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "OUTLINE_GENERAOR_MODELS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_OUTLINE",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "MODEL_GEN_QUERY",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_NAME_REFLECTION",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "global_semaphores",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_QUERY_INTENT",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_SUMMARY_MODEL",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "global_semaphores",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_REFLECTION_MAX_TURNS",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_REFLECTION_MODEL_LST",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_SUMMARY_MODEL",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "global_semaphores",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "global_semaphores",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER_IMAGE_EXTRACT",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER_RERANK",
        "importPath": "configuration",
        "description": "configuration",
        "isExtraImport": true,
        "detail": "configuration",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "auto",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "create_reflection_fallback",
        "importPath": "fallback",
        "description": "fallback",
        "isExtraImport": true,
        "detail": "fallback",
        "documentation": {}
    },
    {
        "label": "process_query_async",
        "importPath": "paper_understant_query",
        "description": "paper_understant_query",
        "isExtraImport": true,
        "detail": "paper_understant_query",
        "documentation": {}
    },
    {
        "label": "generate_paper_outline_async",
        "importPath": "paper_outline_opt",
        "description": "paper_outline_opt",
        "isExtraImport": true,
        "detail": "paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "section_writer_async",
        "importPath": "section_writer_opt_local",
        "description": "section_writer_opt_local",
        "isExtraImport": true,
        "detail": "section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "section_writer_async",
        "importPath": "section_writer_opt_local",
        "description": "section_writer_opt_local",
        "isExtraImport": true,
        "detail": "section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "section_writer_async",
        "importPath": "section_writer_opt_local",
        "description": "section_writer_opt_local",
        "isExtraImport": true,
        "detail": "section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "section_reflection",
        "importPath": "section_reflection_opt",
        "description": "section_reflection_opt",
        "isExtraImport": true,
        "detail": "section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "generate_section_summary",
        "importPath": "section_reflection_opt",
        "description": "section_reflection_opt",
        "isExtraImport": true,
        "detail": "section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "global_reflection",
        "importPath": "paper_global_reflection_opt",
        "description": "paper_global_reflection_opt",
        "isExtraImport": true,
        "detail": "paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "generate_abstract_conclusion",
        "importPath": "paper_abstract_conclusion_opt",
        "description": "paper_abstract_conclusion_opt",
        "isExtraImport": true,
        "detail": "paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "process_poolish_data",
        "importPath": "paper_poolish_opt",
        "description": "paper_poolish_opt",
        "isExtraImport": true,
        "detail": "paper_poolish_opt",
        "documentation": {}
    },
    {
        "label": "format_sections_for_global_reflection",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "prepare_sections_data",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "prepare_sections_data",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "safe_invoke",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "aggregate_references",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "generate_mind_map_from_outline",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "safe_invoke",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "SectionData",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "QueryIntent",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "QueryIntent",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "SectionSummary",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Reference",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "SectionContent",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Reference",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Figure",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "SectionContent",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Reference",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "Figure",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "SectionData",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "AzureChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "LOCAL_MODELS",
        "importPath": "local_model_langchain",
        "description": "local_model_langchain",
        "isExtraImport": true,
        "detail": "local_model_langchain",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "JsonOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "chat_models",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "local_chat_models",
        "importPath": "model_factory",
        "description": "model_factory",
        "isExtraImport": true,
        "detail": "model_factory",
        "documentation": {}
    },
    {
        "label": "get_abstract_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_conclusion_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_abstract_conclusion_evaluation_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_conclusion_section_content_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_intro_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_system",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_system_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_paper_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_enhanced_search_query_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_enhanced_search_query_prompt_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_issue_analysis_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_new_key_point_generation_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_research_field_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_prompt_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_strick_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_synthesis_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_reflection_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_reflection_prompt_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_generation_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_generation_prompt_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_improve_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_improve_prompt_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_conclusion_judge_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_seed_outline_template",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_name_refinement_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_subsection_intro_content_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_intent_classification_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_language_detection_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_rewrite_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_type_classification_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_intro_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_evaluation_system_prompts",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_evaluation_system_prompts_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_eval_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_prompt_template",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_prompt_template_v2",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_name_refinement_prompt",
        "importPath": "prompt_manager",
        "description": "prompt_manager",
        "isExtraImport": true,
        "detail": "prompt_manager",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain.schema",
        "description": "langchain.schema",
        "isExtraImport": true,
        "detail": "langchain.schema",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "PydanticOutputParser",
        "importPath": "langchain.output_parsers",
        "description": "langchain.output_parsers",
        "isExtraImport": true,
        "detail": "langchain.output_parsers",
        "documentation": {}
    },
    {
        "label": "aiohttp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "aiohttp",
        "description": "aiohttp",
        "detail": "aiohttp",
        "documentation": {}
    },
    {
        "label": "run_section_writer_actor",
        "importPath": "generation.section_writer_actor",
        "description": "generation.section_writer_actor",
        "isExtraImport": true,
        "detail": "generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "Digraph",
        "importPath": "graphviz",
        "description": "graphviz",
        "isExtraImport": true,
        "detail": "graphviz",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "sent_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "APIModel",
        "importPath": "evaluation.API.model",
        "description": "evaluation.API.model",
        "isExtraImport": true,
        "detail": "evaluation.API.model",
        "documentation": {}
    },
    {
        "label": "APIModel",
        "importPath": "evaluation.API.model",
        "description": "evaluation.API.model",
        "isExtraImport": true,
        "detail": "evaluation.API.model",
        "documentation": {}
    },
    {
        "label": "Queue",
        "importPath": "queue",
        "description": "queue",
        "isExtraImport": true,
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "futures",
        "importPath": "concurrent",
        "description": "concurrent",
        "isExtraImport": true,
        "detail": "concurrent",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "importPath": "evaluation.args",
        "description": "evaluation.args",
        "isExtraImport": true,
        "detail": "evaluation.args",
        "documentation": {}
    },
    {
        "label": "Judge",
        "importPath": "evaluation.agents.judge",
        "description": "evaluation.agents.judge",
        "isExtraImport": true,
        "detail": "evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "SequenceMatcher",
        "importPath": "difflib",
        "description": "difflib",
        "isExtraImport": true,
        "detail": "difflib",
        "documentation": {}
    },
    {
        "label": "extract_pdf_text_from_url",
        "kind": 2,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "def extract_pdf_text_from_url(pdf_url):\n    \"\"\"PDF\"\"\"\n    print(f\"Extracting PDF text from: {pdf_url}\")\n    # \n    configs = [\n        {\"proxies\": proxies, \"timeout\": 30},  # \n        {\"timeout\": 30},  # \n        {\"proxies\": proxies, \"timeout\": 60},  # \n    ]\n    for i, config in enumerate(configs):",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "get_arxiv_metadata",
        "kind": 2,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "def get_arxiv_metadata(arxiv_id):\n    search = arxiv.Search(\n        id_list=[arxiv_id],\n        max_results=100,\n        sort_by=arxiv.SortCriterion.Relevance,\n        sort_order=arxiv.SortOrder.Descending,\n    )\n    result = list(arxiv_client.results(search))[0]\n    # \n    links = {}",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "get_references_by_arxiv_id",
        "kind": 2,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "def get_references_by_arxiv_id(arxiv_id):\n    print(\"get_references_by_arxiv_id\")\n    sch = SemanticScholar()\n    try:\n        paper = sch.get_paper(\n            f\"ARXIV:{arxiv_id}\",\n            fields=[\n                \"references.title\",\n                \"references.authors\",\n                \"references.externalIds\",",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "fix_existing_data",
        "kind": 2,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "def fix_existing_data(file_path):\n    \"\"\"\"\"\"\n    fixed_file = file_path + \".fixed\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as f_in, \\\n         open(fixed_file, \"w\", encoding=\"utf-8\") as f_out:\n        for line in f_in:\n            try:\n                data = json.loads(line.strip())\n                # meta.links\n                if \"meta\" in data and \"links\" in data[\"meta\"]:",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "arxiv_client",
        "kind": 5,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "arxiv_client = arxiv.Client(delay_seconds=0.05)\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\nasync def get_md_info_with_fallback(arxiv_id, max_retries=2):\n    \"\"\"\n    \n    \"\"\"\n    methods = [\n        (\"ar5iv\", f\"https://ar5iv.labs.arxiv.org/html/{arxiv_id}\"),\n        (\"arxiv_vanity\", f\"https://www.arxiv-vanity.com/papers/{arxiv_id}/\"),\n        (\"arxiv_html\", f\"https://browse.arxiv.org/html/{arxiv_id}\"),",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "benchmark.get_paper_info",
        "description": "benchmark.get_paper_info",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\nasync def get_md_info_with_fallback(arxiv_id, max_retries=2):\n    \"\"\"\n    \n    \"\"\"\n    methods = [\n        (\"ar5iv\", f\"https://ar5iv.labs.arxiv.org/html/{arxiv_id}\"),\n        (\"arxiv_vanity\", f\"https://www.arxiv-vanity.com/papers/{arxiv_id}/\"),\n        (\"arxiv_html\", f\"https://browse.arxiv.org/html/{arxiv_id}\"),\n    ]",
        "detail": "benchmark.get_paper_info",
        "documentation": {}
    },
    {
        "label": "remove_citations",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def remove_citations(sent):\n    return (\n        re.sub(r\"\\[\\d+\", \"\", re.sub(r\" \\[\\d+\", \"\", sent))\n        .replace(\" |\", \"\")\n        .replace(\"]\", \"\")\n    )\ndef process_paragraph(text):\n    text = text.replace(\"<cit.>\", \"\")\n    text = remove_citations(text)\n    return text",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "process_paragraph",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def process_paragraph(text):\n    text = text.replace(\"<cit.>\", \"\")\n    text = remove_citations(text)\n    return text\ndef process_ctx(ctxs):\n    logger.debug(\"process ctx\")\n    for one in ctxs:\n        text = one[\"text\"]\n        cleaned_text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n        cleaned_text = process_paragraph(cleaned_text)",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "process_ctx",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def process_ctx(ctxs):\n    logger.debug(\"process ctx\")\n    for one in ctxs:\n        text = one[\"text\"]\n        cleaned_text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n        cleaned_text = process_paragraph(cleaned_text)\n        one[\"text\"] = cleaned_text\n    return ctxs\ndef rerank_paragraphs_bge(\n    query, paragraphs, reranker, norm_cite=False, start_index=0, use_abstract=False",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "rerank_paragraphs_bge",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def rerank_paragraphs_bge(\n    query, paragraphs, reranker, norm_cite=False, start_index=0, use_abstract=False\n):\n    logger.info(\"run rerank_paragraphs_bge ...\")\n    paragraphs = [p for p in paragraphs if p[\"text\"] is not None]\n    logger.debug(f\"use_abstract?: {use_abstract}\")\n    logger.debug(f\"norm_cite?: {norm_cite}\")\n    if use_abstract is True and p[\"text\"] != p[\"abstract\"]:\n        paragraph_texts = [\n            (",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "create_prompt_with_llama3_format",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def create_prompt_with_llama3_format(\n    prompt,\n    system_message=\"You are a helpful AI assistant for scientific literature review. Please carefully follow user's instruction and help them to understand the most recent papers.\",\n):\n    if system_message is not None:\n        formatted_text = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{0}<|eot_id|>\".format(\n            system_message\n        )\n    else:\n        formatted_text = \"<|begin_of_text|>\"",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "load_hf_tokenizer",
        "kind": 2,
        "importPath": "core.generation.engine",
        "description": "core.generation.engine",
        "peekOfCode": "def load_hf_tokenizer(\n    model_name_or_path,\n    tokenizer_name_or_path=None,\n    use_fast_tokenizer=True,\n    padding_side=\"left\",\n    token=os.getenv(\"HF_TOKEN\", None),\n):\n    from transformers import AutoTokenizer\n    # Need to explicitly import the olmo tokenizer.\n    if not tokenizer_name_or_path:",
        "detail": "core.generation.engine",
        "documentation": {}
    },
    {
        "label": "ArxivImageExtractor",
        "kind": 6,
        "importPath": "core.generation.extract_main_figure",
        "description": "core.generation.extract_main_figure",
        "peekOfCode": "class ArxivImageExtractor:\n    \"\"\"arXiv\"\"\"\n    def __init__(\n        self,\n        cache_dir: str = \"./arxiv_cache\",\n        llm_provider: str = \"openai\",\n        image_extraction_model: str = \"Qwen3-8B\",\n    ):\n        \"\"\"\n        ArXiv",
        "detail": "core.generation.extract_main_figure",
        "documentation": {}
    },
    {
        "label": "get_arxiv_main_figure",
        "kind": 2,
        "importPath": "core.generation.extract_main_figure",
        "description": "core.generation.extract_main_figure",
        "peekOfCode": "def get_arxiv_main_figure(\n    arxiv_id_or_info: Union[str, Dict],\n    llm_provider: str = \"local\",\n    image_extraction_model: str = \"Qwen3-8B\",\n) -> Optional[Dict]:\n    \"\"\"\n    arXiv\n    Args:\n        arxiv_id_or_info: arXivID\n        llm_provider: LLM",
        "detail": "core.generation.extract_main_figure",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "core.generation.extract_main_figure",
        "description": "core.generation.extract_main_figure",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\nclass ArxivImageExtractor:\n    \"\"\"arXiv\"\"\"\n    def __init__(\n        self,\n        cache_dir: str = \"./arxiv_cache\",\n        llm_provider: str = \"openai\",\n        image_extraction_model: str = \"Qwen3-8B\",\n    ):\n        \"\"\"",
        "detail": "core.generation.extract_main_figure",
        "documentation": {}
    },
    {
        "label": "paper_info",
        "kind": 5,
        "importPath": "core.generation.extract_main_figure",
        "description": "core.generation.extract_main_figure",
        "peekOfCode": "paper_info = {\n    \"arxivId\": \"2503.21460\",\n    \"arxivUrl\": \"http://arxiv.org/abs/2503.21460v1\",\n    \"title\": \"Large Language Model Agent: A Survey on Methodology, Applications and Challenges\",\n    \"abstract\": \"The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.\",\n    \"authors\": \"Junyu Luo;Weizhi Zhang;Ye Yuan;Yusheng Zhao;Junwei Yang;Yiyang Gu;Bohan Wu;Binqi Chen;Ziyue Qiao;Qingqing Long;Rongcheng Tu;Xiao Luo;Wei Ju;Zhiping Xiao;Yifan Wang;Meng Xiao;Chenwu Liu;Jingyang Yuan;Shichang Zhang;Yiqiao Jin;Fan Zhang;Xian Wu;Hanqing Zhao;Dacheng Tao;Philip S. Yu;Ming Zhang\",\n    \"year\": \"20250327\",\n    \"fieldsOfStudy\": [\"cs.CL\"],\n    \"source\": \"Search From Arxiv\",\n    \"url\": \"http://arxiv.org/abs/2503.21460v1\",",
        "detail": "core.generation.extract_main_figure",
        "documentation": {}
    },
    {
        "label": "retrival_model_url",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "retrival_model_url = f\"http://xxx:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://xxx:5008/search\"\n# \nrerank_model_url = \"http://xxxx:9756/v1/score/rerank\"\n# chat\nchat_system = \"You are a helpful AI assistant for scientific literature review. Please carefully follow user's instruction and help them to understand the most recent papers.\"\n# ...existing code...\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "recall_server_url",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "recall_server_url = \"http://xxx:5008/search\"\n# \nrerank_model_url = \"http://xxxx:9756/v1/score/rerank\"\n# chat\nchat_system = \"You are a helpful AI assistant for scientific literature review. Please carefully follow user's instruction and help them to understand the most recent papers.\"\n# ...existing code...\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "rerank_model_url",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "rerank_model_url = \"http://xxxx:9756/v1/score/rerank\"\n# chat\nchat_system = \"You are a helpful AI assistant for scientific literature review. Please carefully follow user's instruction and help them to understand the most recent papers.\"\n# ...existing code...\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "chat_system",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "chat_system = \"You are a helpful AI assistant for scientific literature review. Please carefully follow user's instruction and help them to understand the most recent papers.\"\n# ...existing code...\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims\n- Analyzing and reformatting content for academic standards",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "chat_system",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "chat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims\n- Analyzing and reformatting content for academic standards\nGuidelines:\n- Always base your responses on the provided scientific literature",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "chat_url",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "chat_url = \"http://:9089/v1/chat/completions\"  # r1-70b\nCHAT_MODEL_NAME = \"Qwen3-14B\"",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "CHAT_MODEL_NAME",
        "kind": 5,
        "importPath": "core.generation.generation_config",
        "description": "core.generation.generation_config",
        "peekOfCode": "CHAT_MODEL_NAME = \"Qwen3-14B\"",
        "detail": "core.generation.generation_config",
        "documentation": {}
    },
    {
        "label": "task_instructions",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "task_instructions = {\n    \"claim_no_context\": (\n        \"Given a scientific claim, answer if the scientific claim is factually correct (true) or not (false). For each scientific claim provided, simply state whether it is true or false. If the statement is supported by the paragraph, answer true; otherwise answer false. You don't need to provide any explanation, just the label.\",\n        \"\\nClaim: \",\n    ),\n    \"claim_gold\": (\n        \"Given a scientific claim and a gold paragraph that may support or contradict with the claim, answer if the scientific claim is factually correct or not. For each scientific claim provided, simply state whether it is true or false. If the statement is supported by the paragraph, answer true; otherwise answer false. You don't need to provide any explanation, just the label.\",\n        \"\\nClaim: \",\n    ),\n    \"claim_full\": (",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "demonstrations",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "demonstrations = {\n    \"claim_no_context\": \"Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this:\\nClaim: 1 in 5 million in UK have abnormal PrP positivity.\\n[Response_Start]false[Response_End]\\nNow please verify the following claim.\",\n    \"claim_gold\": \"Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this: \\nReferences: \\n[0] Title: Prevalent abnormal prion protein in human appendixes after bovine spongiform encephalopathy epizootic: large scale survey Text: OBJECTIVES To carry out a further survey of archived appendix samples to understand better the differences between existing estimates of the prevalence of subclinical infection with prions after the bovine spongiform encephalopathy epizootic and to see whether a broader birth cohort was affected, and to understand better the implications for the management of blood and blood products and for the handling of surgical instruments. DESIGN Irreversibly unlinked and anonymised large scale survey of archived appendix samples. SETTING Archived appendix samples from the pathology departments of 41 UK hospitals participating in the earlier survey, and additional hospitals in regions with lower levels of participation in that survey. SAMPLE 32,441 archived appendix samples fixed in formalin and embedded in paraffin and tested for the presence of abnormal prion protein (PrP). RESULTS Of the 32,441 appendix samples 16 were positive for abnormal PrP, indicating an overall prevalence of 493 per million population (95% confidence interval 282 to 801 per million). The prevalence in those born in 1941-60 (733 per million, 269 to 1596 per million) did not differ significantly from those born between 1961 and 1985 (412 per million, 198 to 758 per million) and was similar in both sexes and across the three broad geographical areas sampled. Genetic testing of the positive specimens for the genotype at PRNP codon 129 revealed a high proportion that were valine homozygous compared with the frequency in the normal population, and in stark contrast with confirmed clinical cases of vCJD, all of which were methionine homozygous at PRNP codon 129. CONCLUSIONS This study corroborates previous studies and suggests a high prevalence of infection with abnormal PrP, indicating vCJD carrier status in the population compared with the 177 vCJD cases to date. These findings have important implications for the management of blood and blood products and for the handling of surgical instruments.\\nClaim: 1 in 5 million in UK have abnormal PrP positivity. \\n[Response_Start]false[Response_End]\\nNow please verify the following claim.\\n\",\n    \"claim_full\": \"\"\"\n    Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this:\n    References:\n    [0] Title: MLQA: Evaluating Cross-lingual Extractive Question Answering Text: Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.\n    [1] Title: XOR QA: Cross-lingual Open-Retrieval Question Answering Text: Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcitywhere languages have few reference articlesand information asymmetrywhere questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering.\n    [2] Title: Unsupervised Cross-lingual Representation Learning at Scale Text: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.\n    Claim: The XOR QA dataset covers eight languages.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_passages_rag",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_passages_rag = \"\"\"\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.\n[1] Title: Reliable, Adaptable, and Attributable Language Models with Retrieval Text: Unlike parametric LMswhich use large-scale text data only during trainingretrieval-augmented LMs leverage an external large-scale collection of documents (datastore) at inference by selecting relevant documents from the datastore (Asai et al., 2023a). Retrieval-augmented LMs can W1: largely reduce factual errors (Mallen et al., 2023), W2: provide better attributions (Gao et al., 2023a), W3: enabling flexible opt-in and out of sequences (Min et al., 2024).\n[2] Title: Atlas: Few-shot Learning with Retrieval Augmented Language Models Text: In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.\n[3] Title: Language Models are Few-Shot Learners Text: Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\n[4] Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories Text:  On both datasets, LMs memorization (RQ1) is often limited to the popular factual knowledge and even GPT-3 davinci-003 fails to answer the majority of the long-tail questions. Moreover, on such questions, scaling up models does not significantly improve the performance. This also suggests that we can predict if LMs memorize certain knowledge based on the information presented in the input question only. We next investigate whether a semi-parametric approach that augments LMs with retrieved evidence can mitigate the low performance on questions about less popular entities (RQ2). Nonparametric memories largely improve performance on long-tail distributions across models.\n[5] Title: Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning Text: Personalization in large language models (LLMs) is increasingly important, aiming to align LLMs interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences.\n[6] Title: RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation Text:  Retrieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2021). Simply prepending retrieved documents to the input without updating the language models (LMs) (Shi et al., 2023b; Ram et al., 2023; Si et al., 2022) allows retrieval augmentation even for black-box LMs, but such approach comes with limitations. First, it increases computational costs as LMs now encode substantially more tokens. Second, even if we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer et al., 2020), these models struggle to use all information in the context, frequently missing information placed in the middle (Liu et al., 2023). Third, prepending a large number of documents in-context can further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a).\n\"\"\"\nexample_question_rag = (",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_question_rag",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_question_rag = (\n    \"How do language models leverage parametric and non-parametric knowledge?\"\n)\nexample_answer_rag = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks.\\n\nParametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-3 may fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities[4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_answer_rag",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_answer_rag = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks.\\n\nParametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-3 may fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities[4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities [4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_feedback = \"\"\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_answer_rag_incorrect",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities [4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_feedback = \"\"\"\nFeedback: Only concrete examples used in the answer are QA results. We should include more results from non QA tasks. Question: What tasks retrieval-augmented LMs have been applied to?\\n\nFeedback: Only one limitation discussed in the answer is efficiency. Question: What are the disadvantages of retrieval-augmented LMs?\\n\nFeedback: The original answer can be improved by adding more logical structure e.g., grouping similar discussions together and add paragraph headers.\\n\n\"\"\"\nexample_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_feedback",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_feedback = \"\"\"\nFeedback: Only concrete examples used in the answer are QA results. We should include more results from non QA tasks. Question: What tasks retrieval-augmented LMs have been applied to?\\n\nFeedback: Only one limitation discussed in the answer is efficiency. Question: What are the disadvantages of retrieval-augmented LMs?\\n\nFeedback: The original answer can be improved by adding more logical structure e.g., grouping similar discussions together and add paragraph headers.\\n\n\"\"\"\nexample_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"\nexample_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_question_peft",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"\nexample_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n\n[3] Title: Towards a Unified View of Parameter-Efficient Transfer Learning Text: To mitigate this issue, a few lightweight alternatives have been proposed to update only a small number of extra parameters while keeping most pretrained parameters frozen. For example, adapter tuning (Houlsby et al., 2019) inserts small neural modules called adapters to each layer of the pretrained network and only the adapters are trained at fine-tuning time. Inspired by the success of prompting methods that control PLMs through textual prompts (Brown et al., 2020; Liu et al., 2021a), prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) prepend an additional l tunable prefix tokens to the input or hidden layers and only train these soft prompts when fine-tuning on downstream tasks.\\n\n[4] Title: I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning  Text: We design an I-Tuning module to connect the pre-trained vision encoder (i.e., CLIP-ViT [7]) and the language decoder (i.e., GPT2 [8]). To align between the language and vision modals, it serves as a cross-modal filter that automatically picks the visual information from the output of the vision encoder and adjusts the output hidden states of the language decoder. During training, we only update the newly introduced parameters in the I-Tuning module, and the parameters of the two pre-trained models are frozen.\\n\n\"\"\"\nexample_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_passages_peft",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n\n[3] Title: Towards a Unified View of Parameter-Efficient Transfer Learning Text: To mitigate this issue, a few lightweight alternatives have been proposed to update only a small number of extra parameters while keeping most pretrained parameters frozen. For example, adapter tuning (Houlsby et al., 2019) inserts small neural modules called adapters to each layer of the pretrained network and only the adapters are trained at fine-tuning time. Inspired by the success of prompting methods that control PLMs through textual prompts (Brown et al., 2020; Liu et al., 2021a), prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) prepend an additional l tunable prefix tokens to the input or hidden layers and only train these soft prompts when fine-tuning on downstream tasks.\\n\n[4] Title: I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning  Text: We design an I-Tuning module to connect the pre-trained vision encoder (i.e., CLIP-ViT [7]) and the language decoder (i.e., GPT2 [8]). To align between the language and vision modals, it serves as a cross-modal filter that automatically picks the visual information from the output of the vision encoder and adjusts the output hidden states of the language decoder. During training, we only update the newly introduced parameters in the I-Tuning module, and the parameters of the two pre-trained models are frozen.\\n\n\"\"\"\nexample_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.\n[1] Rating: 5 Explanation: This paragraph introduces Prefix Tuning, one of the most representative methods in parameter efficient tuning and includes their core empirical results.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_rating_peft",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.\n[1] Rating: 5 Explanation: This paragraph introduces Prefix Tuning, one of the most representative methods in parameter efficient tuning and includes their core empirical results.\n[2] Rating: 3 Explanation: While this paragraph provides a taxonomy of parameter efficient tuning and analysis, it does not provide any details of individual methods. Moreover, this paper's main focus is PEFT for vision models, while the original question asks about parameter efficient tuning for large language models.\n[3] Rating: 4 Explanation: This paragraph briefly introduces multiple parameter efficient tuning methods such as adapter tuning, prefix tuning and prompt tuning. While they do not directly discuss their advantages or disadvantages or more detail about prefix or prompt tuning, still this paragraph gives a useful overview of this area.\n[4] Rating: 1 Explanation: This paragraph introduces a new parameter efficient tuning method to connect a vision encoder and language encoder to make their representations aligned. The question asks about representative approaches of parameter efficient tuning for large language models, and this paragraph topic is substantially different from the question.[Response_End]\\n\n\"\"\"\nprompts_w_references = (\n    \"Provide a detailed, informative answer to the following research-related question. Your answer should be more than one paragraph, offering a comprehensive overview. \"\n    \"Base your answer on multiple pieces of evidence and references, rather than relying on a single reference for a short response. Aim to give a holistic view of the topic. \"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_w_references",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_w_references = (\n    \"Provide a detailed, informative answer to the following research-related question. Your answer should be more than one paragraph, offering a comprehensive overview. \"\n    \"Base your answer on multiple pieces of evidence and references, rather than relying on a single reference for a short response. Aim to give a holistic view of the topic. \"\n    \"Ensure the answer is well-structured, coherent and informative so that real-world scientists can gain a clear understanding of the subject. Rather than simply summarizing multiple papers one by one, try to organize your answers based on similarities and differences between papers. \"\n    \"Make sure to add citations to all citation-worthy statements using the provided references (References), by indicating the citation numbers of the corresponding passages. \"\n    \"More specifically, add the citation number at the end of each relevant sentence e.g., 'This work shows the effectiveness of problem X [1].' when the passage [1] in References provides full support for the statement. \"\n    \"You do not need to add the author names, title or publication year as in the ordinal paper writing, and just mention the citation numbers with your generation. \"\n    \"Not all references may be relevant, so only cite those that directly support the statement. \"\n    \"You only need to indicate the reference number, and you do not need to add Reference list by yourself. \"\n    \"If multiple references support a statement, cite them together (e.g., [1][2]). Yet, for each citation-worthy statement, you only need to add at least one citation, so if multiple eviences support the statement, just add the most relevant citation to the sentence. \"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_demonstration_prompts",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_demonstration_prompts = prompts_w_references.format_map(\n    {\n        \"example_passages\": example_passages_rag,\n        \"example_question\": example_question_rag,\n        \"example_answer\": example_answer_rag,\n    }\n)\ngeneration_instance_prompts_w_references = (\n    generation_demonstration_prompts + \"References:\\n{context}\\nQuestion: {input}\\n\"\n)",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_w_references = (\n    generation_demonstration_prompts + \"References:\\n{context}\\nQuestion: {input}\\n\"\n)\ngeneration_instance_prompts_w_references_zero_shot = (\n    \"Provide a detailed, informative answer to the following research-related question. Your answer should be more than one paragraph, offering a comprehensive overview.\"\n    \"Base your answer on multiple pieces of evidence and references, rather than relying on a single reference for a short response. Aim to give a holistic view of the topic.\"\n    \"Ensure the answer is well-structured, coherent and informative so that real-world scientists can gain a clear understanding of the subject. Rather than simply summarizing multiple papers one by one, try to organize your answers based on similarities and differences between papers.\"\n    \"Make sure to add citations to all citation-worthy statements using the provided references (References). More specifically, add the citation number at the end of each relevant sentence e.g., 'This work shows the effectiveness of problem X [1].' when the passage [1] in References provides full support for the statement.\"\n    \"Not all references may be relevant, so only cite those that directly support the statement.\"\n    \"If multiple references support a statement, cite them together (e.g., [1][2]). Yet, for each citation-worthy statement, you only need to add at least one citation, so if multiple eviences support the statement, just add the most relevant citation to the sentence.\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_zero_shot",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_w_references_zero_shot = (\n    \"Provide a detailed, informative answer to the following research-related question. Your answer should be more than one paragraph, offering a comprehensive overview.\"\n    \"Base your answer on multiple pieces of evidence and references, rather than relying on a single reference for a short response. Aim to give a holistic view of the topic.\"\n    \"Ensure the answer is well-structured, coherent and informative so that real-world scientists can gain a clear understanding of the subject. Rather than simply summarizing multiple papers one by one, try to organize your answers based on similarities and differences between papers.\"\n    \"Make sure to add citations to all citation-worthy statements using the provided references (References). More specifically, add the citation number at the end of each relevant sentence e.g., 'This work shows the effectiveness of problem X [1].' when the passage [1] in References provides full support for the statement.\"\n    \"Not all references may be relevant, so only cite those that directly support the statement.\"\n    \"If multiple references support a statement, cite them together (e.g., [1][2]). Yet, for each citation-worthy statement, you only need to add at least one citation, so if multiple eviences support the statement, just add the most relevant citation to the sentence.\"\n    \"References: \\n{context}\"\n    \"\\nQuestion: {input}\"\n)",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_reranking",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_reranking = \"\"\"\nEvaluate the relevance of passages from scientific papers to aid in crafting informative responses to a given question. Each passage should be assessed for its potential contribution to understanding the question, including clear definitions, method advantages, method comparisons, and concrete experimental results. Each passage should be rated on a scale from 1 to 5, with explanations provided for each rating:\\n\n1 (Completely Irrelevant): The passage is entirely unrelated to the question and does not offer any relevant information. Your answer should be marked as [Response_Start] and [Response_End].\\n\n2 (Somewhat Related): The passage is tangentially related to the question but does not provide any substantial information that could be incorporated into an answer.\\n\n3 (Partially Relevant): Although the passage does not directly address the question, it offers high-level information that could enhance non-essential parts of the answer, such as opening remarks or supplemental experimental results.\\n\n4 (Relevant): The passage provides useful information that could be integrated into essential parts of the answer, contributing significantly to understanding the question.\\n\n5 (Highly Relevant): The passage contains important information, and several sentences could be directly cited and used in the response to provide crucial insights into the question.\\n\n\"\"\"\nranking_example_instance_prompt = \"{prompts_reranking}\\n##\\nQuestion: {example_question}\\nReferences\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "ranking_example_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "ranking_example_instance_prompt = \"{prompts_reranking}\\n##\\nQuestion: {example_question}\\nReferences\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {\n        \"prompts_reranking\": prompts_reranking,\n        \"example_question\": example_question_peft,\n        \"example_paragraph\": example_passages_peft,\n        \"example_rating\": example_rating_peft,\n    }\n)\nranking_instance_prompt = (\n    ranking_example_instance_prompt",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "ranking_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "ranking_instance_prompt = (\n    ranking_example_instance_prompt\n    + \"\\n##\\nQuestion: {question}\\nReferences\\n{passages}\\n\"\n)\n# Updated on June 15\nprompts_reranking_summarization = \"\"\"\nEvaluate the relevance of passages from scientific papers to aid in crafting informative related work section to a given abstract. Each passage should be assessed for its potential contribution to understanding the original abstract, including clear definitions, method advantages, method comparisons, and concrete experimental results. Each passage should be rated on a scale from 1 to 5, with explanations provided for each rating:\\n\n1 (Completely Irrelevant): The passage is entirely unrelated to the ideal related work section given the abstract, and does not offer any relevant information. Your answer should be marked as [Response_Start] and [Response_End].\\n\n2 (Somewhat Related): The passage is tangentially related to the topic discussed in the abstract and may help users to write related work, but does not provide any substantial information that could be incorporated into the related work.\\n\n3 (Partially Relevant): Although the passage may not provide crucial information to write a related work section, it offers some useful information that could enhance non-essential parts of the related work, such as opening remarks or supplemental experimental results, as well as additional exampels to strengthen main arguements.\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_reranking_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_reranking_summarization = \"\"\"\nEvaluate the relevance of passages from scientific papers to aid in crafting informative related work section to a given abstract. Each passage should be assessed for its potential contribution to understanding the original abstract, including clear definitions, method advantages, method comparisons, and concrete experimental results. Each passage should be rated on a scale from 1 to 5, with explanations provided for each rating:\\n\n1 (Completely Irrelevant): The passage is entirely unrelated to the ideal related work section given the abstract, and does not offer any relevant information. Your answer should be marked as [Response_Start] and [Response_End].\\n\n2 (Somewhat Related): The passage is tangentially related to the topic discussed in the abstract and may help users to write related work, but does not provide any substantial information that could be incorporated into the related work.\\n\n3 (Partially Relevant): Although the passage may not provide crucial information to write a related work section, it offers some useful information that could enhance non-essential parts of the related work, such as opening remarks or supplemental experimental results, as well as additional exampels to strengthen main arguements.\\n\n4 (Relevant): The passage provides useful information that could be integrated into essential parts of the related work, contributing significantly to understand and contextualized the abstract.\\n\n5 (Highly Relevant): The passage contains important information, and several sentences could be directly cited and used in the related work to provide crucial insights into the related work users try to write.\\n\n\"\"\"\n# TODO: add demonstrations\nexample_question_summarization = \"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. \"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_question_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_question_summarization = \"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. \"\nexample_passages_summarization = \"\"\"\n[0] Title: CoQA: A Conversational Question Answering Challenge Text: Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\\%, which is 23.4 points behind human performance (88.8\\%), indicating there is ample room for improvement. \\n\n[1] Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text Text: We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\\%, a significant improvement over a simple baseline (20\\%). However, human performance (86.8\\%) is much higher, indicating that the dataset presents a good challenge problem for future research.\\n\n[2] Title: Interpretation of natural language rules in conversational machine reading Text: Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer \"Can I...?\" or \"Do I have to...?\" questions such as \"I am working in Canada. Do I have to carry on paying UK National Insurance?\" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as How long have you been working abroad? when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.\\n\n[3] Title: Passage Re-ranking with BERT Text: Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27\\% (relative) in MRR@10. \\n\n[4] Title: Bidirectional Attention Flow for Machine Comprehension Text: Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.\\n\n\"\"\"\nexample_rating_summarization = \"\"\"\n[Response_Start][0] Rating: 5 Explanation: This paper seems to introduce a dataset that sounds really similar to the proposed dataset and the authors definitely need to discuss how their proposed dataaset is related to this paper. Therefore, the rating is 5.\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_passages_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_passages_summarization = \"\"\"\n[0] Title: CoQA: A Conversational Question Answering Challenge Text: Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\\%, which is 23.4 points behind human performance (88.8\\%), indicating there is ample room for improvement. \\n\n[1] Title: SQuAD: 100,000+ Questions for Machine Comprehension of Text Text: We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\\%, a significant improvement over a simple baseline (20\\%). However, human performance (86.8\\%) is much higher, indicating that the dataset presents a good challenge problem for future research.\\n\n[2] Title: Interpretation of natural language rules in conversational machine reading Text: Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer \"Can I...?\" or \"Do I have to...?\" questions such as \"I am working in Canada. Do I have to carry on paying UK National Insurance?\" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as How long have you been working abroad? when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.\\n\n[3] Title: Passage Re-ranking with BERT Text: Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27\\% (relative) in MRR@10. \\n\n[4] Title: Bidirectional Attention Flow for Machine Comprehension Text: Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.\\n\n\"\"\"\nexample_rating_summarization = \"\"\"\n[Response_Start][0] Rating: 5 Explanation: This paper seems to introduce a dataset that sounds really similar to the proposed dataset and the authors definitely need to discuss how their proposed dataaset is related to this paper. Therefore, the rating is 5.\\n\n[1] Rating: 4 Explanation: This paper introduces a large-scale machine reading comprehension dataset. Although this dataset is not a conversational QA dataset proposed by the original abstract and the set up may be more simple than conversational QA, still this dataset could be useful to cite when the authors discuss the history and different datasets in relavent areas.\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_rating_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_rating_summarization = \"\"\"\n[Response_Start][0] Rating: 5 Explanation: This paper seems to introduce a dataset that sounds really similar to the proposed dataset and the authors definitely need to discuss how their proposed dataaset is related to this paper. Therefore, the rating is 5.\\n\n[1] Rating: 4 Explanation: This paper introduces a large-scale machine reading comprehension dataset. Although this dataset is not a conversational QA dataset proposed by the original abstract and the set up may be more simple than conversational QA, still this dataset could be useful to cite when the authors discuss the history and different datasets in relavent areas.\\n\n[2] Rating: 5 Explanation: This paper presents a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. This paper also considers a conversational QA situation and is highly relavant to the abstract.\\n\n[3] Rating: 2 Explanation: This paper introduces a new method for passage ranking to enhance information retrieval, using a fine-tuned pre-trained encoder model, namely BERT. While this method may or may not be used in the paper of the subject abstract, this method is mainly proposed for IR and is less relevant to the proposed conversational QA setup. Therefore, this paper may not provide useful information to be included in the related work section.\\n\n[4] Rating: 3 Explanation: This paper proposes a new method that performs competitively on a machine reading comprehension dataset, SQuAD. Although this paper could be cited to discuss how a more simple machine reading comprehension task has evolved in terms of datasets and methodologies, given that the main focus of the provided abstract is on conversational QA, the method paper on MRC may not be really crucial.[Response_End]\\n\n\"\"\"\nexample_answer_summarization = \"\"\"\nOur work builds on span based reading comprehension [1] while also incorporating innovations such as curating questions independently of supporting text to reduce trivial lexical overlap.\nConcurrent to our work, [2] proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_answer_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_answer_summarization = \"\"\"\nOur work builds on span based reading comprehension [1] while also incorporating innovations such as curating questions independently of supporting text to reduce trivial lexical overlap.\nConcurrent to our work, [2] proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog.\nAlso concurrently, [0] propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence.\n\"\"\"\n# TODO: add examples\nexample_passages_single_paper = \"\"\"\n[0] We hypothesize that factual knowledge frequently discussed on the web is easily memorized by LMs, while the knowledge that is less discussed may not be well captured and thus they require retrieving external non-parametric memories. We evaluate ten large LMs of three families (i.e., GPT-Neo, OPT, and GPT-3) with varying scales on the open-domain question answering (QA) task in a zero- or few-shot prompting manner.\n[1] We construct a new dataset, PopQA, consisting of 14k questions to cover factual information in the long tail that might have been missed in popular QA datasets Kwiatkowski et al. (2019). We use Wikipedia page views as a measure of popularity and convert knowledge triples from Wikidata, with diverse levels of popularity, into natural language questions, anchored to the original entities and relationship types. We also use EntityQuestions Sciavolino et al. (2021), an open-domain QA dataset with a long-tail distribution.\n[2] Figure 4 (bottom) shows that there is a positive correlation between subject entity popularity and models' accuracy for almost all relationship types. This supports our hypothesis that subject entity popularity can be a reliable indicator of LMs' factual knowledge memorization. In general, the correlations between subject entity popularity and accuracy are stronger for larger LMs; GPT-3 003 shows the highest positive correlation (roughly 0.4) while GPT-Neo-1.3B shows relatively weak positive correlations (approximately 0.1).",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_passages_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_passages_single_paper = \"\"\"\n[0] We hypothesize that factual knowledge frequently discussed on the web is easily memorized by LMs, while the knowledge that is less discussed may not be well captured and thus they require retrieving external non-parametric memories. We evaluate ten large LMs of three families (i.e., GPT-Neo, OPT, and GPT-3) with varying scales on the open-domain question answering (QA) task in a zero- or few-shot prompting manner.\n[1] We construct a new dataset, PopQA, consisting of 14k questions to cover factual information in the long tail that might have been missed in popular QA datasets Kwiatkowski et al. (2019). We use Wikipedia page views as a measure of popularity and convert knowledge triples from Wikidata, with diverse levels of popularity, into natural language questions, anchored to the original entities and relationship types. We also use EntityQuestions Sciavolino et al. (2021), an open-domain QA dataset with a long-tail distribution.\n[2] Figure 4 (bottom) shows that there is a positive correlation between subject entity popularity and models' accuracy for almost all relationship types. This supports our hypothesis that subject entity popularity can be a reliable indicator of LMs' factual knowledge memorization. In general, the correlations between subject entity popularity and accuracy are stronger for larger LMs; GPT-3 003 shows the highest positive correlation (roughly 0.4) while GPT-Neo-1.3B shows relatively weak positive correlations (approximately 0.1).\n[3] As seen in the left column of Figure 4, there are clear overall performance improvements with scale on the PopQA dataset. However, Figure 5 shows that on both PopQA and EntityQuestions, most of scaling's positive effect on parametric knowledge comes from questions with high popularity. Specifically, for the questions about the entities whose log 10 (subject popularity) is larger than 4, there is an improvement in accuracy as model size increases (red and yellow lines), while performance on questions with lower popularity remains relatively constant (blue and green lines). For the 4,000 least popular questions, GPT-Neo 6B, 20B, and GPT-3 davinci-003 have 15\\%, 16\\%, and 19\\% accuracy, respectively.\n[4] Our analysis indicates that even the current state-of-the-art LMs struggle with less popular subjects or certain relationship types, and increasing the model size does not lead to further performance improvements. In light of this, we extend our analysis to non-parametric sources of knowledge, as outlined in Section 2. Specifically, we investigate the effectiveness of retrieval-augmented LMs Borgeaud et al. (2022); Lewis et al. (2020), which leverage non-parametric memories (i.e., retrieved text) to improve performance.\n[5] Figure 7 shows that augmenting LMs with non-parametric memories significantly outperforms unassisted vanilla LMs. A much smaller LM (e.g., GPT-Neo 2.7B) augmented by the Contriever retrieval results outperforms vanilla GPT-3. Large LMs such as GPT-3 also enjoy the benefits of non-parametric memories. Contriever gives 7\\% accuracy gains on top of GPT-3 davinci-003. GenRead shows little-to-no performance improvement over vanilla parametric knowledge for smaller models, while the technique shows sizeable gains for GPT-3, especially davinci-003. In addition to its limited effectiveness with smaller LMs, GenRead has potentially prohibitive inference time costs, with GPT-NeoX 20B taking 70 seconds per query.\n\"\"\"\nexample_question_single_paper = \"What is the authors' conclusion about the effectiveness of language model scaling on long-tail factual knowledge memorization?\"\nexample_answer_single_paper = \"The authors found that on their newly constructed dataset, Pop QA [1], performance on questions with lower popularity (long tail facts) remains relatively constant [3]. The authors concluded model scaling may not help long-tail factual memorization [4].\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_question_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_question_single_paper = \"What is the authors' conclusion about the effectiveness of language model scaling on long-tail factual knowledge memorization?\"\nexample_answer_single_paper = \"The authors found that on their newly constructed dataset, Pop QA [1], performance on questions with lower popularity (long tail facts) remains relatively constant [3]. The authors concluded model scaling may not help long-tail factual memorization [4].\"\nexample_answer_single_paper_no_context = \"The authors found that on their newly constructed dataset, Pop QA, performance on questions with lower popularity (long tail facts) remains relatively constant. The authors concluded model scaling may not help long-tail factual memorization.\"\nranking_example_instance_prompt_summarization = \"{prompts_reranking}\\n##\\nAbstract: {example_question}\\nPassages:\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {\n        \"prompts_reranking\": prompts_reranking_summarization,\n        \"example_question\": example_question_summarization,\n        \"example_paragraph\": example_passages_summarization,\n        \"example_rating\": example_rating_summarization,\n    }",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_answer_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_answer_single_paper = \"The authors found that on their newly constructed dataset, Pop QA [1], performance on questions with lower popularity (long tail facts) remains relatively constant [3]. The authors concluded model scaling may not help long-tail factual memorization [4].\"\nexample_answer_single_paper_no_context = \"The authors found that on their newly constructed dataset, Pop QA, performance on questions with lower popularity (long tail facts) remains relatively constant. The authors concluded model scaling may not help long-tail factual memorization.\"\nranking_example_instance_prompt_summarization = \"{prompts_reranking}\\n##\\nAbstract: {example_question}\\nPassages:\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {\n        \"prompts_reranking\": prompts_reranking_summarization,\n        \"example_question\": example_question_summarization,\n        \"example_paragraph\": example_passages_summarization,\n        \"example_rating\": example_rating_summarization,\n    }\n)",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "example_answer_single_paper_no_context",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "example_answer_single_paper_no_context = \"The authors found that on their newly constructed dataset, Pop QA, performance on questions with lower popularity (long tail facts) remains relatively constant. The authors concluded model scaling may not help long-tail factual memorization.\"\nranking_example_instance_prompt_summarization = \"{prompts_reranking}\\n##\\nAbstract: {example_question}\\nPassages:\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {\n        \"prompts_reranking\": prompts_reranking_summarization,\n        \"example_question\": example_question_summarization,\n        \"example_paragraph\": example_passages_summarization,\n        \"example_rating\": example_rating_summarization,\n    }\n)\nranking_instance_prompt_summarization = (",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "ranking_example_instance_prompt_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "ranking_example_instance_prompt_summarization = \"{prompts_reranking}\\n##\\nAbstract: {example_question}\\nPassages:\\n{example_paragraph}\\n{example_rating}\".format_map(\n    {\n        \"prompts_reranking\": prompts_reranking_summarization,\n        \"example_question\": example_question_summarization,\n        \"example_paragraph\": example_passages_summarization,\n        \"example_rating\": example_rating_summarization,\n    }\n)\nranking_instance_prompt_summarization = (\n    ranking_example_instance_prompt_summarization",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "ranking_instance_prompt_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "ranking_instance_prompt_summarization = (\n    ranking_example_instance_prompt_summarization\n    + \"\\n##\\nAbstract: {question}\\nPassages:\\n{passages}\\n\"\n)\n# Feedback\ninstruction_feedback = \"\"\"\nGiven an answer to a scientific question based on the most recent scientific literature, make a list of feedback. Prioritize the feedback by listing the most critical improvements first. Regarding the content improvements, it is often helpful to ask for more results on or applications to different tasks, elaborate on details of crucial methods, or suggest including other popular methods.\nStylistic improvements can include better organizations or writing enhancements. For each suggested improvement requiring additional information from the literature that is not discussed in passages, formulate a question to guide the search for missing details.\nIf the feedback primarily concerns stylistic or organizational changes, omit the need for an additional question. Your answer should be marked as [Response_Start] and [Response_End].\nEach feedback should be preceded by 'Feedback: ', and additional question should be preceded by 'Question: '. Your question will be used to search additional context, so they should be self-containing and are understandable without additional context.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "instruction_feedback",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "instruction_feedback = \"\"\"\nGiven an answer to a scientific question based on the most recent scientific literature, make a list of feedback. Prioritize the feedback by listing the most critical improvements first. Regarding the content improvements, it is often helpful to ask for more results on or applications to different tasks, elaborate on details of crucial methods, or suggest including other popular methods.\nStylistic improvements can include better organizations or writing enhancements. For each suggested improvement requiring additional information from the literature that is not discussed in passages, formulate a question to guide the search for missing details.\nIf the feedback primarily concerns stylistic or organizational changes, omit the need for an additional question. Your answer should be marked as [Response_Start] and [Response_End].\nEach feedback should be preceded by 'Feedback: ', and additional question should be preceded by 'Question: '. Your question will be used to search additional context, so they should be self-containing and are understandable without additional context.\nHere's an example.\\n\n##\\n\nQuestion: {example_question}\\n\nAnswer: {example_answer}\\n\n[Response_Start]{example_feedback}[Response_End]",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "instruction_feedback_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "instruction_feedback_prompt = instruction_feedback.format_map(\n    {\n        \"example_question\": example_question_rag,\n        \"example_answer\": example_answer_rag_incorrect,\n        \"example_paragraphs\": example_passages_rag,\n        \"example_feedback\": example_feedback,\n    }\n)\nfeedback_example_instance_prompt = (\n    instruction_feedback_prompt + \"Question: {question}\\nAnswer:\\n{answer}\\n\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "feedback_example_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "feedback_example_instance_prompt = (\n    instruction_feedback_prompt + \"Question: {question}\\nAnswer:\\n{answer}\\n\"\n)\nediting_feedback = \"\"\"\nWe provide a question related to recent scientific literature, an answer from a strong language model, and feedback on the answer.\nPlease incorporate the feedback to improve the answer. Only modify the parts that require enhancement as noted in the feedback, keeping the other sentences unchanged.\nDo not omit any crucial information from the original answer unless the feedback specifies that certain sentences are incorrect and should be removed.\nIf you add new paragraphs or discussions, ensure that you are not introducing repetitive content or duplicating ideas already included in the original response.\nUse existing references presented under References to support the new discussions, referring to their citation numbers.\nDo not remove new lines or paragraphs in the original answer, unless the feedback specifies that certain sentences are incorrect and should be removed, or the paragraph organizations should be changed.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "editing_feedback",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "editing_feedback = \"\"\"\nWe provide a question related to recent scientific literature, an answer from a strong language model, and feedback on the answer.\nPlease incorporate the feedback to improve the answer. Only modify the parts that require enhancement as noted in the feedback, keeping the other sentences unchanged.\nDo not omit any crucial information from the original answer unless the feedback specifies that certain sentences are incorrect and should be removed.\nIf you add new paragraphs or discussions, ensure that you are not introducing repetitive content or duplicating ideas already included in the original response.\nUse existing references presented under References to support the new discussions, referring to their citation numbers.\nDo not remove new lines or paragraphs in the original answer, unless the feedback specifies that certain sentences are incorrect and should be removed, or the paragraph organizations should be changed.\nYour answer should be marked as [Response_Start] and [Response_End].\\n\nReferences:\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "editing_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "editing_instance_prompt = (\n    editing_feedback\n    + \"##\\nReferences\\n{passages}\\nQuestion: {question}\\nAnswer:\\n{answer}\\nFeedback:\\n{feedback}\\nEdited Answer:\\n\"\n)\nposthoc_attributions = \"\"\"\nWe give you a statement extracted from an answer to a question related to the most recent scientific literature, and a set of evidence passages.\nIf the statement is fully supported by any of the listed passages in References, insert the citation numbers to the statement.\nIf none of the passages support the statement, do not insert any citation, and leave the original sentence as is.\nIf multiple passages provide sufficient support for the statement, you only need to insert one citation, rather than inserting all of them. Your answer should be marked as [Response_Start] and [Response_End].'\\n\nHere's an example:\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "posthoc_attributions",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "posthoc_attributions = \"\"\"\nWe give you a statement extracted from an answer to a question related to the most recent scientific literature, and a set of evidence passages.\nIf the statement is fully supported by any of the listed passages in References, insert the citation numbers to the statement.\nIf none of the passages support the statement, do not insert any citation, and leave the original sentence as is.\nIf multiple passages provide sufficient support for the statement, you only need to insert one citation, rather than inserting all of them. Your answer should be marked as [Response_Start] and [Response_End].'\\n\nHere's an example:\\n\nStatement: On the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference.\nReferences:\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.\n[1] Title: Reliable, Adaptable, and Attributable Language Models with Retrieval Text: Unlike parametric LMswhich use large-scale text data only during trainingretrieval-augmented LMs leverage an external large-scale collection of documents (datastore) at inference by selecting relevant documents from the datastore (Asai et al., 2023a). Retrieval-augmented LMs can W1: largely reduce factual errors (Mallen et al., 2023), W2: provide better attributions (Gao et al., 2023a), W3: enabling flexible opt-in and out of sequences (Min et al., 2024).",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "posthoc_attributions_paragraph_all",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "posthoc_attributions_paragraph_all = \"\"\"\nWe give you a short paragraph extracted from an answer to a question related to the most recent scientific literature, and a set of evidence passages.\nFind all of the citation-worthy statements without any citations, and insert citation numbers to the statements that are fully supported by any of the provided citations in listed as References.\nIf none of the passages support the statement, do not insert any citation, and leave the original sentence as is, but do your best to insert citation.\nIf multiple passages provide sufficient support for the statement, you only need to insert one citation, rather than inserting all of them. Your answer should be marked as [Response_Start] and [Response_End].'\\n\nHere's an example:\\n\nStatement: Language models store rich knowledge in their parameters during pre-training, resulting in their strong performance on many knowledge-intensive tasks. However, such parametric knowledge based generations are often hard to attribute. Models can also struggle in long-tail knowledge. On the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference.\nReferences:\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.\n[1] Title: Reliable, Adaptable, and Attributable Language Models with Retrieval Text: Unlike parametric LMswhich use large-scale text data only during training; retrieval-augmented LMs leverage an external large-scale collection of documents (datastore) at inference by selecting relevant documents from the datastore (Asai et al., 2023a). Retrieval-augmented LMs can W1: largely reduce factual errors (Mallen et al., 2023), W2: provide better attributions (Gao et al., 2023a), W3: enabling flexible opt-in and out of sequences (Min et al., 2024).",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "posthoc_attributions_paragraph",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "posthoc_attributions_paragraph = \"\"\"\nWe give you a short paragraph extracted from an answer to a question related to the most recent scientific literature, and a set of evidence passages.\nFind all of the citation-worthy statements without any citations, and insert citation numbers to the statements that are fully supported by any of the provided citations in listed as References.\nIf none of the passages support the statement, do not insert any citation number, and leave the original sentence as is.\nYou onlyneed to add a citation number if applicable, and do not need to modify the original text. Do not directly insert text from the relevant evidence.\nIf multilpe passages provide sufficient support for the statement, you only need to insert one citation, rather than inserting all of them. Your answer should be marked as [Response_Start] and [Response_End].'\\n\nHere's an example:\\n\nStatement: Language models store rich knowledge in their parameters during pre-training, resulting in their strong performance on many knowledge-intensive tasks. However, such parametric knowledge based generations are often hard to attribute. Models can also struggle in long-tail knowledge. On the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference.\nReferences:\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "posthoc_attribution_with_citations",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "posthoc_attribution_with_citations = \"\"\"\nWe give you a statement extracted from an answer to a question related to the most recent scientific literature, and a cited evidence passage.\nEvaluate if the statement is fully supported by the citation, and answer with 'Yes' (the statement is fully supported by the citation) or 'No' (otherwise), Your rating and explanations should be indicated as 'Rating:' and 'Explanation:', after Your answer should be marked, and your response should be marked as [Response_Start] and [Response_End].\nAfter the rating, please explain why it is / is not fully supported.\\n\n##\\n\nStatement: Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length.\nReferences:\\n\nTitle: RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation Text:  Retrieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2021). Simply prepending retrieved documents to the input without updating the language models (LMs) (Shi et al., 2023b; Ram et al., 2023; Si et al., 2022) allows retrieval augmentation even for black-box LMs, but such approach comes with limitations. First, it increases computational costs as LMs now encode substantially more tokens. Second, even if we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer et al., 2020), these models struggle to use all information in the context, frequently missing information placed in the middle (Liu et al., 2023). Third, prepending a large number of documents in-context can further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a).\n[Response_Start]Rating: Yes\\n\nExplanation: The cited passage explicitly mentions that retrieval-augmented LMs will introduce additional inference time latency. [Response_End]\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "editing_feedback_with_retrieval",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "editing_feedback_with_retrieval = \"\"\"\nWe provide you with a question related to recent scientific literature, an answer from a strong language model, feedback on the answer, and relevant retrieved passages to address the feedback.\nYour task is to incorporate the feedback to improve the answer by including new results or details from the retrieved passages (References:). When adding new information, avoid copying entire passages; instead, summarize the key information from the suggested papers to address the feedback.\nFor instance, instead of copying the text from the original data, 'We found the empirical results X' to support the discussions without evidence in the original answer, say 'Former work found that X'.\nOnly modify the parts mentioned in the feedback, keeping the rest of the answer intact.\nYour improved answer should be marked with [Response_Start] and [Response_End].\nQuestion: What are the advantages of retrieval-augmented LMs?\nAnswer: Retrieval-augmented LMs have been effective in various use cases, including reducing hallucinations [0] and enabling efficient adaptations to new data, such as temporal shifts [1]. Empirical results suggest they reduce hallucinations by 30% [2].\nFeedback: The answer provides solid empirical results on hallucination reduction but lacks data on efficient adaptations. Please include empirical results for that aspect as well.\nReferences:",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "editing_with_retrieval_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "editing_with_retrieval_instance_prompt = (\n    editing_feedback_with_retrieval\n    + \"##\\nQuestion: {question}\\nAnswer:\\n{answer}\\nFeedback:\\n{feedback}\\nReferences:\\n{retrieved_passages}\\nEdited Answer:\\n\"\n)\nprompts_w_references_single_paper = (\n    \"Answer a question based on the following scientific paper. Your answer should sufficiently answer the question, citing specific paragraphs from the papers that provide full support for the statement. \"\n    \"Your answer is likely to be one or more than one sentences.\"\n    \"All of citation-worthy statements in your answer need to be supported by one of the references we provide as 'References:'. References should be formatted as [0], [1], [2], ..., [n].\"\n    \"Not all of the references are useful, and you only need to cite the references that support the sentences. \"\n    \"You only need to cite one paragraph for each citation worthy statement.\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_w_references_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_w_references_single_paper = (\n    \"Answer a question based on the following scientific paper. Your answer should sufficiently answer the question, citing specific paragraphs from the papers that provide full support for the statement. \"\n    \"Your answer is likely to be one or more than one sentences.\"\n    \"All of citation-worthy statements in your answer need to be supported by one of the references we provide as 'References:'. References should be formatted as [0], [1], [2], ..., [n].\"\n    \"Not all of the references are useful, and you only need to cite the references that support the sentences. \"\n    \"You only need to cite one paragraph for each citation worthy statement.\"\n    \"Your answer should be marked as [Response_Start] and [Response_End].\"\n    \"Here's an example:\\n##\\n\"\n    \"References: \\n{example_passages}\"\n    \"\\nQuestion: {example_question}\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_w_references_single_paper_zero_shot",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_w_references_single_paper_zero_shot = \"Answer the following question based on a scientific paper. We provide a set of paragraphs from the paper, indicated as 'References'. Ensure that each answer includes citations that provide sufficient evidence to support it, using citation numbers for citation-worthy statements.\\n\"\ngeneration_demonstration_prompts_single_paper = (\n    prompts_w_references_single_paper.format_map(\n        {\n            \"example_passages\": example_passages_single_paper,\n            \"example_question\": example_question_single_paper,\n            \"example_answer\": example_answer_single_paper,\n        }\n    )\n)",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_demonstration_prompts_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_demonstration_prompts_single_paper = (\n    prompts_w_references_single_paper.format_map(\n        {\n            \"example_passages\": example_passages_single_paper,\n            \"example_question\": example_question_single_paper,\n            \"example_answer\": example_answer_single_paper,\n        }\n    )\n)\ngeneration_instance_prompts_w_references_single_paper = (",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_single_paper",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_w_references_single_paper = (\n    generation_demonstration_prompts_single_paper\n    + \"References:\\n {context}\\nQuestion: {input}\\n\"\n)\ngeneration_instance_prompts_w_references_single_paper_zero_shot = (\n    prompts_w_references_single_paper_zero_shot\n    + \"\\nReferences:\\n{context}\\nQuestion: {input}\\n\"\n)\npromts_w_references_single_paper_no_context = (\n    \"Answer a question based on the following scientific paper. \"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_single_paper_zero_shot",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_w_references_single_paper_zero_shot = (\n    prompts_w_references_single_paper_zero_shot\n    + \"\\nReferences:\\n{context}\\nQuestion: {input}\\n\"\n)\npromts_w_references_single_paper_no_context = (\n    \"Answer a question based on the following scientific paper. \"\n    \"Your answer is likely to be one or more than one sentences.\"\n    \"Your answer should be marked as [Response_Start] and [Response_End].\"\n    \"Here's an example:\\n##\\n\"\n    \"\\nQuestion: {example_question}\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "promts_w_references_single_paper_no_context",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "promts_w_references_single_paper_no_context = (\n    \"Answer a question based on the following scientific paper. \"\n    \"Your answer is likely to be one or more than one sentences.\"\n    \"Your answer should be marked as [Response_Start] and [Response_End].\"\n    \"Here's an example:\\n##\\n\"\n    \"\\nQuestion: {example_question}\"\n    \"\\n[Response_Start]{example_answer}[Response_End]\\nNow, please answer this question:\\n##\\n\"\n)\ngeneration_demonstration_prompts_single_paper_no_context = (\n    promts_w_references_single_paper_no_context.format_map(",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_demonstration_prompts_single_paper_no_context",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_demonstration_prompts_single_paper_no_context = (\n    promts_w_references_single_paper_no_context.format_map(\n        {\n            \"example_question\": example_question_single_paper,\n            \"example_answer\": example_answer_single_paper_no_context,\n        }\n    )\n)\ngeneration_instance_prompts_w_references_single_paper_no_context = (\n    generation_demonstration_prompts_single_paper_no_context + \"\\nQuestion: {input}\\n\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_single_paper_no_context",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_w_references_single_paper_no_context = (\n    generation_demonstration_prompts_single_paper_no_context + \"\\nQuestion: {input}\\n\"\n)\npromts_w_references_summarization = (\n    \"Given an abstract of an academic paper and a set of passsages from relevant papers, generate a related work section summarizing relevant related work.\"\n    \"Not all of the passages are relevant, so please carefully read the passages and only use passages that are related.\"\n    \"All of citation-worthy statements need to be supported by one of the references we provide as 'References' and appropriate citation numbers should be added at the last of the sentences.\"\n    \"References should be formatted as [0], [1], [2], ..., [n].\"\n    \"Your answer should be marked as [Response_Start] and [Response_End].\"\n    \"Here's an example:\\n##\\n\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "promts_w_references_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "promts_w_references_summarization = (\n    \"Given an abstract of an academic paper and a set of passsages from relevant papers, generate a related work section summarizing relevant related work.\"\n    \"Not all of the passages are relevant, so please carefully read the passages and only use passages that are related.\"\n    \"All of citation-worthy statements need to be supported by one of the references we provide as 'References' and appropriate citation numbers should be added at the last of the sentences.\"\n    \"References should be formatted as [0], [1], [2], ..., [n].\"\n    \"Your answer should be marked as [Response_Start] and [Response_End].\"\n    \"Here's an example:\\n##\\n\"\n    \"References: \\n{example_passages}\"\n    \"\\nAbstract: {example_question}\"\n    \"\\n[Response_Start]{example_answer}[Response_End]\\nNow, please generate another related work given the following abstract.\\n##\\n\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_demonstration_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_demonstration_summarization = promts_w_references_summarization.format_map(\n    {\n        \"example_passages\": example_passages_summarization,\n        \"example_question\": example_question_summarization,\n        \"example_answer\": example_answer_summarization,\n    }\n)\ngeneration_instance_prompts_summarization = (\n    generation_demonstration_summarization\n    + \"References:\\n {context}\\nAbstract: {input}\\n\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_summarization",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "generation_instance_prompts_summarization = (\n    generation_demonstration_summarization\n    + \"References:\\n {context}\\nAbstract: {input}\\n\"\n)\nprompts_w_references_summarization_zero_shot = \"Given an abstract of an academic paper and a set of passages from relevant papers, generate a related work section summarizing relevant related work. All of citation-worthy statements need to be supported by one of the references we provide as 'References' and appropriate citation numbers should be added at the last of the sentences. References should be formatted as [0], [1], [2], ..., [n].\\nReferences: {context}\\nAbstract: {input}\"\nkeyword_extraction_prompt = \"\"\"\nSuggest semantic scholar search APIs to retrieve relevant papers to answer the following question related to the most recent NLP research. The search queries must be short, and commma separated. Here's an example. I'll show one example and the test instance you should suggest the search queries. Your response should be marked with [Response_Start] and [Response_End].\\n\n##\\n\nQuestion: How have prior work incorporated personality attributes to train personalized dialogue generation models?\\n\n[Response_Start]personalized dialogue generation, personalized language models, personalized dialogue[Response_End]\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "prompts_w_references_summarization_zero_shot",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "prompts_w_references_summarization_zero_shot = \"Given an abstract of an academic paper and a set of passages from relevant papers, generate a related work section summarizing relevant related work. All of citation-worthy statements need to be supported by one of the references we provide as 'References' and appropriate citation numbers should be added at the last of the sentences. References should be formatted as [0], [1], [2], ..., [n].\\nReferences: {context}\\nAbstract: {input}\"\nkeyword_extraction_prompt = \"\"\"\nSuggest semantic scholar search APIs to retrieve relevant papers to answer the following question related to the most recent NLP research. The search queries must be short, and commma separated. Here's an example. I'll show one example and the test instance you should suggest the search queries. Your response should be marked with [Response_Start] and [Response_End].\\n\n##\\n\nQuestion: How have prior work incorporated personality attributes to train personalized dialogue generation models?\\n\n[Response_Start]personalized dialogue generation, personalized language models, personalized dialogue[Response_End]\\n\n##\\n\nQuestion: How do retrieval-augmented LMs perform well in knowledge-intensive tasks?\\n\n[Response_Start]retrieval-augmented LMs, knowledge-intensive tasks, large language models for knowledge-intensive tasks, retrieval-augmented generation[Response_End]\\n\n##\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "keyword_extraction_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "keyword_extraction_prompt = \"\"\"\nSuggest semantic scholar search APIs to retrieve relevant papers to answer the following question related to the most recent NLP research. The search queries must be short, and commma separated. Here's an example. I'll show one example and the test instance you should suggest the search queries. Your response should be marked with [Response_Start] and [Response_End].\\n\n##\\n\nQuestion: How have prior work incorporated personality attributes to train personalized dialogue generation models?\\n\n[Response_Start]personalized dialogue generation, personalized language models, personalized dialogue[Response_End]\\n\n##\\n\nQuestion: How do retrieval-augmented LMs perform well in knowledge-intensive tasks?\\n\n[Response_Start]retrieval-augmented LMs, knowledge-intensive tasks, large language models for knowledge-intensive tasks, retrieval-augmented generation[Response_End]\\n\n##\\n\nQuestion: {question}\\n",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "final_processing",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "final_processing = \"\"\"\nGiven the following answer to a question, reduce the repetitive discussions or less important details to make the answer more concise yet keep all of the important information relevat to the question kept. Do not loose any of the citation numbers originally provided by the answer. If there's any issue with organization, such as repetitive discussions, redundant information, disconnected discussions, please edit the text without changing the main content and do not loose any of the citation. If there's no issue, just copy the original text. Your answer should be marked as [Response_Start] and [Response_End].\nQuestion: {question}\\n\nAnswer: {answer}\n\"\"\"\njudge_section_should_give_an_system = (\n    \"You are a helpful academic writing assistant that provides answers in JSON format.\"\n)\njudge_section_should_give_an_figure = \"\"\"Determine if this academic paper section needs an image: \"{section_name}\"\nAnswer with a JSON object containing two fields:",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "judge_section_should_give_an_system",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "judge_section_should_give_an_system = (\n    \"You are a helpful academic writing assistant that provides answers in JSON format.\"\n)\njudge_section_should_give_an_figure = \"\"\"Determine if this academic paper section needs an image: \"{section_name}\"\nAnswer with a JSON object containing two fields:\n1. \"need_image\": a string (yes/no)\n2. \"reason\": a brief explanation (1-2 sentences)\nRespond ONLY with valid JSON. Format:\n{{\"need_image\": yes/no, \"reason\": \"explanation\"}}\nResponse:\"\"\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "judge_section_should_give_an_figure",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "judge_section_should_give_an_figure = \"\"\"Determine if this academic paper section needs an image: \"{section_name}\"\nAnswer with a JSON object containing two fields:\n1. \"need_image\": a string (yes/no)\n2. \"reason\": a brief explanation (1-2 sentences)\nRespond ONLY with valid JSON. Format:\n{{\"need_image\": yes/no, \"reason\": \"explanation\"}}\nResponse:\"\"\"\nformat_reflection_prompt = \"\"\"You are an expert scientific editor. Your task is to analyze the following text, which represents a section of a scientific paper, and ensure it adheres to standard academic writing formats and structure.\nAnalyze the text for:\n- Coherence and logical flow between paragraphs.",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "format_reflection_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "format_reflection_prompt = \"\"\"You are an expert scientific editor. Your task is to analyze the following text, which represents a section of a scientific paper, and ensure it adheres to standard academic writing formats and structure.\nAnalyze the text for:\n- Coherence and logical flow between paragraphs.\n- Appropriate paragraph breaks.\n- Clear topic sentences for paragraphs.\n- Overall structure suitable for a paper section.\n- Avoidance of overly conversational or informal language.\n**Constraint:** The text contains citations in the format `[number]`, like `[1]`, `[2, 3]`, `[4-6]`. **It is absolutely critical that you DO NOT change the position or the numbers within these citations.** If you rewrite any part of the text, the citations must remain attached to the exact same preceding word or phrase as in the original text.\n**Input Text:**\n```",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "context_refine_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "context_refine_prompt = \"\"\"\nYou will be given a markdown document. Please rewrite it according to the following requirements:\n- Remove all headings (lines starting with #, ##, etc.).\n- Remove any redundant or repetitive content.\n- Remove any paragraph that appears to be a conclusion (e.g., starts with \"In conclusion\", \"Therefore\", \"To summarize\", etc.).\n- Keep the main ideas and factual content intact.\n- Rewrite the result in a clear, concise, and logically structured academic style suitable for a research paper section.\n- Output the rewritten content as plain text only. Do not include explanations, markdown, or any commentary  only the revised version.\nOriginal text:\n\\\"\\\"\\\"",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "validate_figure_caption_prompt",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "validate_figure_caption_prompt = \"\"\"\nEvaluate whether the following figure caption is suitable for the section titled \"{section_name}\".\nCaption content: {caption}\nPlease answer only \"yes\" or \"no\", and briefly state the reason (no more than one sentence).\nConsider the following aspects for your evaluation:\n- **Relevance:** Is the caption relevant to the section's theme (\"{section_name}\")?\n- **Clarity & Conciseness:** Is the caption clear, concise, and easy to understand?\n- **Completeness:** Does the caption adequately describe the figure's content?\n- **Accuracy:** Is the information presented in the caption factually correct?\n- **Grammar & Syntax:** Is the caption grammatically correct and well-formed?",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "template_extract_keywords_source_aware",
        "kind": 5,
        "importPath": "core.generation.generation_instructions",
        "description": "core.generation.generation_instructions",
        "peekOfCode": "template_extract_keywords_source_aware = \"\"\"Extract optimal search keywords from the given research question, specifically optimized for '{source}' academic database. Your task is to generate concise, comma-separated query terms that will maximize relevant paper retrieval in this specific platform.\n### Source-Specific Guidelines:\n#### If targeting Semantic Scholar:\n- Focus on technical terminology and core concepts\n- Include methodological terms\n- Consider author-centric keywords if prominent researchers are known\n- Emphasize computer science and AI terminology where relevant\n#### If targeting OpenAlex:\n- Prioritize broader academic terms\n- Include interdisciplinary connections",
        "detail": "core.generation.generation_instructions",
        "documentation": {}
    },
    {
        "label": "GOOGLE_KEY",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "GOOGLE_KEY = \"xxx\"  # find here:\nS2_API_KEY = \"\"  # semantic scholar api\nDO_REFERENCE_SEARCH = True\nDO_REFERENCE_SEARCH = False\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n# bge embedding model\nretrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "S2_API_KEY",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "S2_API_KEY = \"\"  # semantic scholar api\nDO_REFERENCE_SEARCH = True\nDO_REFERENCE_SEARCH = False\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n# bge embedding model\nretrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"\n# ",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "DO_REFERENCE_SEARCH",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "DO_REFERENCE_SEARCH = True\nDO_REFERENCE_SEARCH = False\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n# bge embedding model\nretrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"\n# \nrerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "DO_REFERENCE_SEARCH",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "DO_REFERENCE_SEARCH = False\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n# bge embedding model\nretrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"\n# \nrerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"\n# chat",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n# bge embedding model\nretrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"\n# \nrerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"\n# chat\nchat_url = \"http://0.0.0.0:9089/v1/chat/completions\"  # r1-70b",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "retrival_model_url",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "retrival_model_url = \"http://0.0.0.0:9655/v1/emb/encoder\"\n# \n# recall_server_url = recall_server_url\nrecall_server_url = \"http://0.0.0.0:5008/search\"\n# \nrerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"\n# chat\nchat_url = \"http://0.0.0.0:9089/v1/chat/completions\"  # r1-70b\nCHAT_MODEL_NAME = \"r1-llama70b\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "recall_server_url",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "recall_server_url = \"http://0.0.0.0:5008/search\"\n# \nrerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"\n# chat\nchat_url = \"http://0.0.0.0:9089/v1/chat/completions\"  # r1-70b\nCHAT_MODEL_NAME = \"r1-llama70b\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "rerank_model_url",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "rerank_model_url = \"http://0.0.0.0:9756/v1/score/rerank\"\n# chat\nchat_url = \"http://0.0.0.0:9089/v1/chat/completions\"  # r1-70b\nCHAT_MODEL_NAME = \"r1-llama70b\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "chat_url",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "chat_url = \"http://0.0.0.0:9089/v1/chat/completions\"  # r1-70b\nCHAT_MODEL_NAME = \"r1-llama70b\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "CHAT_MODEL_NAME",
        "kind": 5,
        "importPath": "core.generation.global_config",
        "description": "core.generation.global_config",
        "peekOfCode": "CHAT_MODEL_NAME = \"r1-llama70b\"",
        "detail": "core.generation.global_config",
        "documentation": {}
    },
    {
        "label": "RESPONSE_START_DELIMITER",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "RESPONSE_START_DELIMITER = \"[Response_Start]\"\nRESPONSE_END_DELIMITER = \"[Response_End]\"\nREFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "RESPONSE_END_DELIMITER",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "RESPONSE_END_DELIMITER = \"[Response_End]\"\nREFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "REFERENCES_HEADER",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "REFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims\n- Analyzing and reformatting content for academic standards",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "REVISED_ANSWER_HEADER",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "REVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\nchat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims\n- Analyzing and reformatting content for academic standards\nGuidelines:",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "chat_system",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "chat_system = \"\"\"You are an expert AI assistant specialized in scientific writing and research. You help researchers create high-quality academic content based on recent scientific literature.\nYour capabilities include:\n- Generating comprehensive responses to scientific questions using provided research papers\n- Writing well-structured academic sections with proper citations\n- Providing constructive feedback on scientific writing\n- Incorporating feedback to improve academic content\n- Adding proper citations and attributions to scientific claims\n- Analyzing and reformatting content for academic standards\nGuidelines:\n- Always base your responses on the provided scientific literature",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "task_instructions",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "task_instructions = {\n    \"claim_no_context\": (\n        \"Given a scientific claim, answer if the scientific claim is factually correct (true) or not (false). For each scientific claim provided, simply state whether it is true or false. If the statement is supported by the paragraph, answer true; otherwise answer false. You don't need to provide any explanation, just the label.\",\n        \"\\nClaim: \",\n    ),\n    \"claim_gold\": (\n        \"Given a scientific claim and a gold paragraph that may support or contradict with the claim, answer if the scientific claim is factually correct or not. For each scientific claim provided, simply state whether it is true or false. If the statement is supported by the paragraph, answer true; otherwise answer false. You don't need to provide any explanation, just the label.\",\n        \"\\nClaim: \",\n    ),\n    \"claim_full\": (",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "demonstrations",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "demonstrations = {\n    \"claim_no_context\": \"Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this:\\nClaim: 1 in 5 million in UK have abnormal PrP positivity.\\n[Response_Start]false[Response_End]\\nNow please verify the following claim.\",\n    \"claim_gold\": \"Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this: \\nReferences: \\n[0] Title: Prevalent abnormal prion protein in human appendixes after bovine spongiform encephalopathy epizootic: large scale survey Text: OBJECTIVES To carry out a further survey of archived appendix samples to understand better the differences between existing estimates of the prevalence of subclinical infection with prions after the bovine spongiform encephalopathy epizootic and to see whether a broader birth cohort was affected, and to understand better the implications for the management of blood and blood products and for the handling of surgical instruments. DESIGN Irreversibly unlinked and anonymised large scale survey of archived appendix samples. SETTING Archived appendix samples from the pathology departments of 41 UK hospitals participating in the earlier survey, and additional hospitals in regions with lower levels of participation in that survey. SAMPLE 32,441 archived appendix samples fixed in formalin and embedded in paraffin and tested for the presence of abnormal prion protein (PrP). RESULTS Of the 32,441 appendix samples 16 were positive for abnormal PrP, indicating an overall prevalence of 493 per million population (95% confidence interval 282 to 801 per million). The prevalence in those born in 1941-60 (733 per million, 269 to 1596 per million) did not differ significantly from those born between 1961 and 1985 (412 per million, 198 to 758 per million) and was similar in both sexes and across the three broad geographical areas sampled. Genetic testing of the positive specimens for the genotype at PRNP codon 129 revealed a high proportion that were valine homozygous compared with the frequency in the normal population, and in stark contrast with confirmed clinical cases of vCJD, all of which were methionine homozygous at PRNP codon 129. CONCLUSIONS This study corroborates previous studies and suggests a high prevalence of infection with abnormal PrP, indicating vCJD carrier status in the population compared with the 177 vCJD cases to date. These findings have important implications for the management of blood and blood products and for the handling of surgical instruments.\\nClaim: 1 in 5 million in UK have abnormal PrP positivity. \\n[Response_Start]false[Response_End]\\nNow please verify the following claim.\\n\",\n    \"claim_full\": \"\"\"\n    Your answer must be marked by special tokens, [Response_Start] and [Response_End]. For example, the input and output looks like this:\n    References:\n    [0] Title: MLQA: Evaluating Cross-lingual Extractive Question Answering Text: Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.\n    [1] Title: XOR QA: Cross-lingual Open-Retrieval Question Answering Text: Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcitywhere languages have few reference articlesand information asymmetrywhere questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering.\n    [2] Title: Unsupervised Cross-lingual Representation Learning at Scale Text: This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.\n    Claim: The XOR QA dataset covers eight languages.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_passages_rag",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_passages_rag = \"\"\"\n[0] Title: Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models Text: Roberts et al. (2020) shows that T5 (Raffel et al., 2020) can perform a new task formulation, closedbook QA. Concretely, T5 can produce answers to questions without access to any corpus at inference time, instead producing answers based on its model parameters, tuned to remember information digested in pretraining.\n[1] Title: Reliable, Adaptable, and Attributable Language Models with Retrieval Text: Unlike parametric LMswhich use large-scale text data only during trainingretrieval-augmented LMs leverage an external large-scale collection of documents (datastore) at inference by selecting relevant documents from the datastore (Asai et al., 2023a). Retrieval-augmented LMs can W1: largely reduce factual errors (Mallen et al., 2023), W2: provide better attributions (Gao et al., 2023a), W3: enabling flexible opt-in and out of sequences (Min et al., 2024).\n[2] Title: Atlas: Few-shot Learning with Retrieval Augmented Language Models Text: In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.\n[3] Title: Language Models are Few-Shot Learners Text: Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.\n[4] Title: When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories Text:  On both datasets, LMs memorization (RQ1) is often limited to the popular factual knowledge and even GPT-3 davinci-003 fails to answer the majority of the long-tail questions. Moreover, on such questions, scaling up models does not significantly improve the performance. This also suggests that we can predict if LMs memorize certain knowledge based on the information presented in the input question only. We next investigate whether a semi-parametric approach that augments LMs with retrieved evidence can mitigate the low performance on questions about less popular entities (RQ2). Nonparametric memories largely improve performance on long-tail distributions across models.\n[5] Title: Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning Text: Personalization in large language models (LLMs) is increasingly important, aiming to align LLMs interactions, content, and recommendations with individual user preferences. Recent advances in LLM personalization have spotlighted effective prompt design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior patterns and preferences.\n[6] Title: RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation Text:  Retrieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks (Kwiatkowski et al., 2019; Petroni et al., 2021). Simply prepending retrieved documents to the input without updating the language models (LMs) (Shi et al., 2023b; Ram et al., 2023; Si et al., 2022) allows retrieval augmentation even for black-box LMs, but such approach comes with limitations. First, it increases computational costs as LMs now encode substantially more tokens. Second, even if we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer et al., 2020), these models struggle to use all information in the context, frequently missing information placed in the middle (Liu et al., 2023). Third, prepending a large number of documents in-context can further confuse LMs with irrelevant information, degrading model performances (Mallen et al., 2022; Shi et al., 2023a).\n\"\"\"\nexample_question_rag = (",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_question_rag",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_question_rag = (\n    \"How do language models leverage parametric and non-parametric knowledge?\"\n)\nexample_answer_rag = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks.\\n\nParametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-3 may fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities[4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_answer_rag",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_answer_rag = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks.\\n\nParametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-3 may fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities[4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities [4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_feedback = \"\"\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_answer_rag_incorrect",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_answer_rag_incorrect = \"\"\"\nLanguage models leverage both parametric and non-parametric knowledge to perform various tasks. Parametric knowledge refers to the information stored in the model's parameters, which are learned during training [0]. This type of knowledge allows language models to perform tasks such as closed-book question answering, where the model produces answers based on its internal knowledge without accessing any external corpus [0]. However, language models' memorization of parametric knowledge is often limited to popular factual knowledge, and even large models like GPT-4 often fail to answer the majority of long-tail questions [4].\\n\nOn the other hand, non-parametric knowledge is retrieved from an external source, such as a large-scale collection of documents, during inference [1]. This type of knowledge is used in retrieval-augmented language models, which can reduce factual errors, provide better attributions, and enable flexible opt-in and out of sequences [1]. Retrieval-augmented language models have been shown to be effective in few-shot learning scenarios, where they can learn knowledge-intensive tasks with very few training examples [2]. For example, the Atlas model, a retrieval-augmented language model, can reach over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters [2]. Moreover, even without training, simply combining off-the-shelf LMs such as GPT3 with retrieval augmentation can significantly improve performance in long-tail and have been shown to mitigate the low performance on questions about less popular entities [4]. However, retrieval-augmented LMs have several limitations. Specifically, retrieval-augmented LMs can make inference much more inefficient due to increased context length [6].\\n\n\"\"\"\nexample_feedback = \"\"\"\nFeedback: Only concrete examples used in the answer are QA results. We should include more results from non QA tasks. Question: What tasks retrieval-augmented LMs have been applied to?\\n\nFeedback: Only one limitation discussed in the answer is efficiency. Question: What are the disadvantages of retrieval-augmented LMs?\\n\nFeedback: The original answer can be improved by adding more logical structure e.g., grouping similar discussions together and add paragraph headers.\\n\n\"\"\"\nexample_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_feedback",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_feedback = \"\"\"\nFeedback: Only concrete examples used in the answer are QA results. We should include more results from non QA tasks. Question: What tasks retrieval-augmented LMs have been applied to?\\n\nFeedback: Only one limitation discussed in the answer is efficiency. Question: What are the disadvantages of retrieval-augmented LMs?\\n\nFeedback: The original answer can be improved by adding more logical structure e.g., grouping similar discussions together and add paragraph headers.\\n\n\"\"\"\nexample_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"\nexample_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_question_peft",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_question_peft = \"Discuss various parameter-efficient fine-tuning (PEFT) techniques for large language models, highlighting their strengths and weaknesses.\"\nexample_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n\n[3] Title: Towards a Unified View of Parameter-Efficient Transfer Learning Text: To mitigate this issue, a few lightweight alternatives have been proposed to update only a small number of extra parameters while keeping most pretrained parameters frozen. For example, adapter tuning (Houlsby et al., 2019) inserts small neural modules called adapters to each layer of the pretrained network and only the adapters are trained at fine-tuning time. Inspired by the success of prompting methods that control PLMs through textual prompts (Brown et al., 2020; Liu et al., 2021a), prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) prepend an additional l tunable prefix tokens to the input or hidden layers and only train these soft prompts when fine-tuning on downstream tasks.\\n\n[4] Title: I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning  Text: We design an I-Tuning module to connect the pre-trained vision encoder (i.e., CLIP-ViT [7]) and the language decoder (i.e., GPT2 [8]). To align between the language and vision modals, it serves as a cross-modal filter that automatically picks the visual information from the output of the vision encoder and adjusts the output hidden states of the language decoder. During training, we only update the newly introduced parameters in the I-Tuning module, and the parameters of the two pre-trained models are frozen.\\n\n\"\"\"\nexample_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_passages_peft",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_passages_peft = \"\"\"\n[0] Title: Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs Text: As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge efficiently.\\n\n[1] Title: Prefix-Tuning: Optimizing Continuous Prompts for Generation Text: In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\\n\n[2] Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey Text: This paper aims to provide a comprehensive and systematic study of PEFT methods in the vision domain, particularly focusing on transformer-based pre-trained models ranging from the year 2019 to the year 2023. As shown in Fig. 1, existing visual PEFT methods could be categorized into addition-based tuning, partial-based tuning, and unified-based tuning. In section 2, we will define the problem of PEFT, introduce popular backbones, and discuss pre-training methods. In section 3, a detailed taxonomy and in-depth analysis of the PEFT methods will be presented. The real-world applications of PEFT will be introduced in section 4. Finally, in section 5, we will point out future research challenges.\\n\n[3] Title: Towards a Unified View of Parameter-Efficient Transfer Learning Text: To mitigate this issue, a few lightweight alternatives have been proposed to update only a small number of extra parameters while keeping most pretrained parameters frozen. For example, adapter tuning (Houlsby et al., 2019) inserts small neural modules called adapters to each layer of the pretrained network and only the adapters are trained at fine-tuning time. Inspired by the success of prompting methods that control PLMs through textual prompts (Brown et al., 2020; Liu et al., 2021a), prefix tuning (Li & Liang, 2021) and prompt tuning (Lester et al., 2021) prepend an additional l tunable prefix tokens to the input or hidden layers and only train these soft prompts when fine-tuning on downstream tasks.\\n\n[4] Title: I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning  Text: We design an I-Tuning module to connect the pre-trained vision encoder (i.e., CLIP-ViT [7]) and the language decoder (i.e., GPT2 [8]). To align between the language and vision modals, it serves as a cross-modal filter that automatically picks the visual information from the output of the vision encoder and adjusts the output hidden states of the language decoder. During training, we only update the newly introduced parameters in the I-Tuning module, and the parameters of the two pre-trained models are frozen.\\n\n\"\"\"\nexample_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.\n[1] Rating: 5 Explanation: This paragraph introduces Prefix Tuning, one of the most representative methods in parameter efficient tuning and includes their core empirical results.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_rating_peft",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_rating_peft = \"\"\"\n[Response_Start][0] Rating: 3 Explanation: This paragraph discusses a high-level overview and goal of parameter efficient tuning but does not mention any particular methods of parameter efficient tuning and thus may not be super helpful. This could still be useful to discuss general advantages of PEFT.\n[1] Rating: 5 Explanation: This paragraph introduces Prefix Tuning, one of the most representative methods in parameter efficient tuning and includes their core empirical results.\n[2] Rating: 3 Explanation: While this paragraph provides a taxonomy of parameter efficient tuning and analysis, it does not provide any details of individual methods. Moreover, this paper's main focus is PEFT for vision models, while the original question asks about parameter efficient tuning for large language models.\n[3] Rating: 4 Explanation: This paragraph briefly introduces multiple parameter efficient tuning methods such as adapter tuning, prefix tuning and prompt tuning. While they do not directly discuss their advantages or disadvantages or more detail about prefix or prompt tuning, still this paragraph gives a useful overview of this area.\n[4] Rating: 1 Explanation: This paragraph introduces a new parameter efficient tuning method to connect a vision encoder and language encoder to make their representations aligned. The question asks about representative approaches of parameter efficient tuning for large language models, and this paragraph topic is substantially different from the question.[Response_End]\\n\n\"\"\"\n## NOTE: Feedback\ninstruction_feedback = \"\"\"\nYou are an expert scientific reviewer. Given an answer to a scientific question based on recent literature, provide constructive feedback to improve the response quality.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "instruction_feedback",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "instruction_feedback = \"\"\"\nYou are an expert scientific reviewer. Given an answer to a scientific question based on recent literature, provide constructive feedback to improve the response quality.\n**Feedback Guidelines:**\n1. **Content Quality**: Assess completeness, accuracy, and depth of coverage\n2. **Evidence Support**: Evaluate citation quality and supporting evidence\n3. **Structure & Clarity**: Review organization, flow, and readability\n4. **Scope & Balance**: Check for missing perspectives or important details\n**Feedback Format:**\n- Start each point with \"Feedback: \"\n- For content gaps requiring additional literature, add \"Question: \" with a self-contained search query",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_feedback_improved",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_feedback_improved = \"\"\"\nFeedback: The answer focuses primarily on QA applications but lacks discussion of other important tasks where retrieval-augmented LMs leverage both parametric and non-parametric knowledge. Question: How do language models combine parametric and non-parametric knowledge for text generation and summarization tasks?\nFeedback: Only computational efficiency is mentioned as a limitation. The answer should include other significant drawbacks such as retrieval quality dependence and potential noise introduction. Question: What are the main limitations of combining parametric and non-parametric knowledge in language models?\nFeedback: The response would benefit from better organization with clear subsections for advantages, applications, and limitations to improve readability.\nFeedback: Add more quantitative results beyond the single Atlas example to strengthen the empirical evidence about the effectiveness of combining parametric and non-parametric approaches.\n\"\"\"\n# \nexample_answer_rag_improved = \"\"\"\nLanguage models employ two distinct but complementary approaches to leverage knowledge: parametric and non-parametric methods, each with unique advantages and limitations for different applications.\n**Parametric Knowledge Utilization**",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "example_answer_rag_improved",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "example_answer_rag_improved = \"\"\"\nLanguage models employ two distinct but complementary approaches to leverage knowledge: parametric and non-parametric methods, each with unique advantages and limitations for different applications.\n**Parametric Knowledge Utilization**\nParametric knowledge refers to information encoded directly within a model's learned parameters during pre-training [0]. This approach enables models to perform closed-book question answering, where they generate responses based solely on internalized knowledge without accessing external sources during inference [0]. Large language models like GPT-3 demonstrate this capability effectively, achieving 64.3% accuracy on TriviaQA in zero-shot settings and improving to 71.2% in few-shot scenarios [3]. However, parametric knowledge has significant limitations, particularly for long-tail factual information. Research shows that even advanced models like GPT-3 davinci-003 fail to answer the majority of questions about less popular entities, with performance improvements from scaling plateauing for uncommon knowledge [4].\n**Non-Parametric Knowledge Integration**\nNon-parametric approaches address these limitations by retrieving relevant information from external document collections during inference [1]. Retrieval-augmented language models demonstrate substantial advantages, including reduced factual errors, improved attribution capabilities, and flexible knowledge updating without retraining [1]. The effectiveness of this approach is exemplified by Atlas, which achieves over 42% accuracy on Natural Questions using only 64 training examples, outperforming a 540B parameter model by 3% despite having 50x fewer parameters [2]. This demonstrates how retrieval augmentation can achieve superior performance with dramatically improved parameter efficiency.\n**Challenges and Trade-offs**\nDespite their benefits, retrieval-augmented approaches introduce computational overhead due to increased context length requirements [6]. Additionally, models may struggle to effectively utilize all retrieved information, particularly when relevant details are positioned in the middle of long contexts [6]. The choice between parametric and non-parametric approaches often depends on the specific application requirements, balancing factors such as inference efficiency, knowledge coverage, and the need for attributable responses.\n\"\"\"\n# Updated feedback prompt with improved structure",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "instruction_feedback_prompt",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "instruction_feedback_prompt = instruction_feedback.format_map(\n    {\n        \"example_question\": example_question_rag,\n        \"example_answer\": example_answer_rag_incorrect,\n        \"example_feedback\": example_feedback_improved,\n    }\n)\n# Optimized feedback instance prompt with clearer instructions\nfeedback_example_instance_prompt = (\n    instruction_feedback_prompt + \"\\nQuestion: {question}\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "feedback_example_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "feedback_example_instance_prompt = (\n    instruction_feedback_prompt + \"\\nQuestion: {question}\\n\"\n    \"Answer: {answer}\\n\"\n    \"References (for context): {references}\\n\"\n)\nediting_feedback = \"\"\"\nWe provide a question related to recent scientific literature, an answer from a strong language model, and feedback on the answer.\nYour task is to incorporate the feedback to improve the answer while maintaining the original structure and content quality.\n**Guidelines:**\n1. **Selective Modification**: Only modify the parts that require enhancement as noted in the feedback. Keep other sentences unchanged.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "editing_feedback",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "editing_feedback = \"\"\"\nWe provide a question related to recent scientific literature, an answer from a strong language model, and feedback on the answer.\nYour task is to incorporate the feedback to improve the answer while maintaining the original structure and content quality.\n**Guidelines:**\n1. **Selective Modification**: Only modify the parts that require enhancement as noted in the feedback. Keep other sentences unchanged.\n2. **Content Preservation**: Do not omit any crucial information from the original answer unless the feedback explicitly specifies that certain sentences are incorrect and should be removed.\n3. **Avoid Redundancy**: If you add new paragraphs or discussions, ensure you are not introducing repetitive content or duplicating ideas already included in the original response.\n4. **Citation Integration**: Use existing references presented under \"References\" to support new discussions, referring to their citation numbers in the format [X].\n5. **Structure Preservation**: Do not remove new lines or paragraphs in the original answer, unless the feedback specifically indicates that certain sentences are incorrect and should be removed, or the paragraph organization should be changed.\n6. **Evidence-Based Enhancement**: When adding new information, base it on the provided references and cite appropriately.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "editing_instance_prompt",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "editing_instance_prompt = (\n    editing_feedback\n    + \"\\nReferences:\\n{passages}\\nQuestion: {question}\\nOriginal Answer:\\n{answer}\\nFeedback:\\n{feedback}\\nEdited Answer:\\n\"\n)\n# \nprompts_w_references = (\n    \"You are an expert academic researcher. Provide a comprehensive, well-structured answer to the following research question based on the provided scientific literature. \"\n    \"Follow these guidelines for an excellent response:\\n\\n\"\n    \"**Content Requirements:**\\n\"\n    \" Write multiple paragraphs (minimum 2-3) offering a thorough overview of the topic\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "prompts_w_references",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "prompts_w_references = (\n    \"You are an expert academic researcher. Provide a comprehensive, well-structured answer to the following research question based on the provided scientific literature. \"\n    \"Follow these guidelines for an excellent response:\\n\\n\"\n    \"**Content Requirements:**\\n\"\n    \" Write multiple paragraphs (minimum 2-3) offering a thorough overview of the topic\\n\"\n    \" Base your answer on evidence from multiple references rather than single sources\\n\"\n    \" Synthesize information across papers to show relationships, similarities, and differences\\n\"\n    \" Include specific details, methodologies, results, and implications where relevant\\n\"\n    \" Organize content logically by themes, approaches, or chronological development\\n\\n\"\n    \"**Citation Requirements:**\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "prompts_w_references_v2",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "prompts_w_references_v2 = \"\"\"You are an expert academic researcher. Write a comprehensive, analytically rich, and academically rigorous response to the following research question, strictly based on the provided scientific literature. Your response will be evaluated along six critical dimensions: formal academic writing, clarity, non-redundancy, critical analysis, originality, and foresight.\nFollow these instructions to ensure the highest standard of quality:\n---\n**[1] Substantive Content & Structure**\n Write at least **three logically structured paragraphs**, each focused on a distinct aspect of the topic (e.g., methodologies, findings, or debates).\n **Synthesize** evidence from multiple sources. Emphasize contrasts, agreements, or trends across the literature.\n **Critique** methodologies, assumptions, or conclusions where relevant. Offer alternatives or highlight key limitations.\n **Include technical details**: models, experiments, frameworks, or results when possible. Avoid vague generalizations.\n **Begin** with a clear topic introduction.\n **Ensure** smooth transitions between paragraphs.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "prompts_w_references_v3",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "prompts_w_references_v3 = \"\"\"You are an expert academic researcher. Write a comprehensive, analytically rich, and academically rigorous response to the following research question, based strictly on the provided scientific literature. Your response will be evaluated on six dimensions: **academic formality, clarity, redundancy, critical analysis, originality, and future research insight.**\nFollow the instructions below meticulously:\n---\n[1] **Content Organization & Structural Logic**\n Write **at least three logically cohesive paragraphs**, each addressing a distinct, non-overlapping thematic aspect (e.g., methods, findings, controversies).\n Within each paragraph:\n  - Maintain **thematic focus**.\n  - Ensure **literature synthesis**, not summary. Compare results, contrast perspectives, and identify patterns.\n  - Provide **smooth transitions** between ideas and paragraphs. Avoid abrupt thematic jumps.\n Begin with a concise introduction framing the topic and scope. End with a **forward-looking conclusion**, proposing implications or future directions.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "generation_demonstration_prompts",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "generation_demonstration_prompts = prompts_w_references_v3.format_map(\n    {\n        \"example_passages\": example_passages_rag,\n        \"example_question\": example_question_rag,\n        \"example_answer\": example_answer_rag_improved,\n    }\n)\n# \ngeneration_instance_prompts_w_references = (\n    generation_demonstration_prompts + \"References:\\n{context}\\n\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "generation_instance_prompts_w_references = (\n    generation_demonstration_prompts + \"References:\\n{context}\\n\\n\"\n    \"Question: {input}\\n\"\n)\n# \ngeneration_instance_prompts_w_references_zero_shot = (\n    \"You are an expert academic researcher. Provide a comprehensive, well-structured answer to the research question based on the provided scientific literature.\\n\\n\"\n    \"**Guidelines:**\\n\"\n    \" Write multiple paragraphs offering thorough coverage of the topic\\n\"\n    \" Synthesize information across multiple references to show relationships and differences\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_zero_shot",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "generation_instance_prompts_w_references_zero_shot = (\n    \"You are an expert academic researcher. Provide a comprehensive, well-structured answer to the research question based on the provided scientific literature.\\n\\n\"\n    \"**Guidelines:**\\n\"\n    \" Write multiple paragraphs offering thorough coverage of the topic\\n\"\n    \" Synthesize information across multiple references to show relationships and differences\\n\"\n    \" Include specific methodologies, results, and implications where relevant\\n\"\n    \" Organize content logically by themes or approaches\\n\"\n    \" Cite all factual claims using format [X] at the end of relevant sentences\\n\"\n    \" Only cite references that directly support your statements\\n\"\n    \" Write in formal academic style suitable for researchers\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "generation_instance_prompts_w_references_enhanced",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "generation_instance_prompts_w_references_enhanced = (\n    \"You are a senior academic researcher tasked with writing a comprehensive literature review response. \"\n    \"Create an authoritative answer that demonstrates deep understanding of the research landscape.\\n\\n\"\n    \"**Excellence Criteria:**\\n\"\n    \"1. **Comprehensiveness:** Cover all major aspects and approaches mentioned in the literature\\n\"\n    \"2. **Synthesis:** Connect findings across papers, highlighting convergent and divergent results\\n\"\n    \"3. **Critical Analysis:** Discuss strengths, limitations, and research gaps where appropriate\\n\"\n    \"4. **Methodological Insight:** Include details about experimental setups, datasets, and evaluation metrics\\n\"\n    \"5. **Future Implications:** Consider broader impact and future research directions\\n\\n\"\n    \"**Structure Expectations:**\\n\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "judge_section_should_give_an_system",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "judge_section_should_give_an_system = (\n    \"You are a helpful academic writing assistant that provides answers in JSON format.\"\n)\njudge_section_should_give_an_figure = \"\"\"Determine if this academic paper section needs an image: \"{section_name}\"\nAnswer with a JSON object containing two fields:\n1. \"need_image\": a string (yes/no)\n2. \"reason\": a brief explanation (1-2 sentences)\nRespond ONLY with valid JSON. Format:\n{{\"need_image\": yes/no, \"reason\": \"explanation\"}}\nResponse:\"\"\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "judge_section_should_give_an_figure",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "judge_section_should_give_an_figure = \"\"\"Determine if this academic paper section needs an image: \"{section_name}\"\nAnswer with a JSON object containing two fields:\n1. \"need_image\": a string (yes/no)\n2. \"reason\": a brief explanation (1-2 sentences)\nRespond ONLY with valid JSON. Format:\n{{\"need_image\": yes/no, \"reason\": \"explanation\"}}\nResponse:\"\"\"\nformat_reflection_prompt= \"\"\"You are an expert scientific editor. Your task is to analyze the following text, which represents a section of a scientific paper, and ensure it adheres to standard academic writing formats and structure.\nAnalyze the text for:\n- Coherence and logical flow between paragraphs.",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "context_refine_prompt",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "context_refine_prompt = \"\"\"\nYou will be given a markdown document. Please rewrite it according to the following requirements:\n- Remove all headings (lines starting with #, ##, etc.).\n- Remove any redundant or repetitive content.\n- Remove any paragraph that appears to be a conclusion (e.g., starts with \"In conclusion\", \"Therefore\", \"To summarize\", etc.).\n- Keep the main ideas and factual content intact.\n- Rewrite the result in a clear, concise, and logically structured academic style suitable for a research paper section.\n- Output the rewritten content as plain text only. Do not include explanations, markdown, or any commentary  only the revised version.\nOriginal text:\n\\\"\\\"\\\"",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "validate_figure_caption_prompt",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "validate_figure_caption_prompt = \"\"\"\nEvaluate whether the following figure caption is suitable for the section titled \"{section_name}\".\nCaption content: {caption}\nPlease answer only \"yes\" or \"no\", and briefly state the reason (no more than one sentence).\nConsider the following aspects for your evaluation:\n- **Relevance:** Is the caption relevant to the section's theme (\"{section_name}\")?\n- **Clarity & Conciseness:** Is the caption clear, concise, and easy to understand?\n- **Completeness:** Does the caption adequately describe the figure's content?\n- **Accuracy:** Is the information presented in the caption factually correct?\n- **Grammar & Syntax:** Is the caption grammatically correct and well-formed?",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "template_extract_keywords_source_aware",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "template_extract_keywords_source_aware = \"\"\"Extract optimal search keywords from the given research question, specifically optimized for '{source}' academic database. Your task is to generate concise, comma-separated query terms that will maximize relevant paper retrieval in this specific platform.\n### Source-Specific Guidelines:\n#### If targeting Semantic Scholar:\n- Focus on technical terminology and core concepts\n- Include methodological terms\n- Consider author-centric keywords if prominent researchers are known\n- Emphasize computer science and AI terminology where relevant\n#### If targeting OpenAlex:\n- Prioritize broader academic terms\n- Include interdisciplinary connections",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "PAGE_REFINE_PROMPT",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "PAGE_REFINE_PROMPT = \"\"\"Analyze and process the following web page content related to '{topic}'. Output the main body text, removing image links, website URLs, advertisements, meaningless repeated characters, etc. Summarization of the content is prohibited, and all information related to the topic should be retained.\nOriginal web page content:\n{raw_content}\n[Output requirements]\n- Title: <TITLE>Your title</TITLE>\n- Filtered text: <CONTENT>Filtered text</CONTENT>\n\"\"\"\nSIMILARITY_PROMPT = \"\"\"Evaluate the quality of the following content retrieved from the internet based on the given topic, and give a suitable title about the content. Provide a critical and strict assessment.\nTopic: {topic}\nContent: {content}",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "SIMILARITY_PROMPT",
        "kind": 5,
        "importPath": "core.generation.instruction_pro",
        "description": "core.generation.instruction_pro",
        "peekOfCode": "SIMILARITY_PROMPT = \"\"\"Evaluate the quality of the following content retrieved from the internet based on the given topic, and give a suitable title about the content. Provide a critical and strict assessment.\nTopic: {topic}\nContent: {content}\nEvaluate the content based on the following dimensions:\n1. **Relevance to the topic**: Assess whether the content can be considered a subset or expansion of the topic.\n2. **Usability for writing about the topic**: Consider factors such as text length (e.g., very short texts have lower reference value), presence of garbled characters, and overall text quality.\nProvide a rationale for your evaluation before assigning scores. Score each dimension on a scale of 0-100, where 0 indicates no relevance and 100 indicates perfect relevance. Calculate the final average score after scoring each dimension.\nEnclose the scores in `<SCORE></SCORE>` tags. For example: `<SCORE>78</SCORE>`\nEnclose the title in `<TITLE></TITLE>` tags. For example: `<TITLE>Title</TITLE>`\nExample response:",
        "detail": "core.generation.instruction_pro",
        "documentation": {}
    },
    {
        "label": "ArxivDatabase",
        "kind": 6,
        "importPath": "core.generation.local_db_v2",
        "description": "core.generation.local_db_v2",
        "peekOfCode": "class ArxivDatabase:\n    \"\"\"A SQLite database manager for arXiv papers.\n    Provides functionality to store and retrieve arXiv paper metadata using SQLite.\n    Supports batch operations and context management.\n    Attributes:\n        db_path (str): Path to the SQLite database file\n        batch_size (int): Number of records to process in a single transaction\n    \"\"\"\n    def __init__(self, db_path: str, batch_size: int = 1000):\n        \"\"\"Initialize database connection and create table if needed.",
        "detail": "core.generation.local_db_v2",
        "documentation": {}
    },
    {
        "label": "db_path",
        "kind": 5,
        "importPath": "core.generation.local_db_v2",
        "description": "core.generation.local_db_v2",
        "peekOfCode": "db_path = \"./database/new.db\"\n# with ArxivDatabase(db_path) as db:\n#     all_records = db.get_all()\n#     print(f\"Total Records: {len(all_records)}\")\n# \n# if __name__ == \"__main__\":\n#     # \n#     db_path = \"./database/arxiv_data.db\"\n#     jsonl_file = \"./database/id2docs.jsonl\"\n#     db = ArxivDatabase(db_path)",
        "detail": "core.generation.local_db_v2",
        "documentation": {}
    },
    {
        "label": "Document",
        "kind": 6,
        "importPath": "core.generation.relevance",
        "description": "core.generation.relevance",
        "peekOfCode": "class Document:\n    \"\"\"Document structure with title and text content\"\"\"\n    title: str\n    text: str\n    doc_id: Optional[str] = None\n    metadata: Optional[Dict[str, Any]] = None\n    def __post_init__(self):\n        if self.doc_id is None:\n            self.doc_id = str(hash(self.title + self.text))\n@dataclass",
        "detail": "core.generation.relevance",
        "documentation": {}
    },
    {
        "label": "RelevanceResult",
        "kind": 6,
        "importPath": "core.generation.relevance",
        "description": "core.generation.relevance",
        "peekOfCode": "class RelevanceResult:\n    \"\"\"Result of relevance evaluation\"\"\"\n    document: Document\n    relevance_score: float\n    reasoning: Optional[str] = None\n    rank: Optional[int] = None\nclass DocumentRelevanceEvaluator:\n    \"\"\"LLM-based document relevance evaluator\"\"\"\n    def __init__(self, model_name: str = \"Qwen3-32B\", batch_size: int = 5, max_workers: int = 3):\n        \"\"\"",
        "detail": "core.generation.relevance",
        "documentation": {}
    },
    {
        "label": "DocumentRelevanceEvaluator",
        "kind": 6,
        "importPath": "core.generation.relevance",
        "description": "core.generation.relevance",
        "peekOfCode": "class DocumentRelevanceEvaluator:\n    \"\"\"LLM-based document relevance evaluator\"\"\"\n    def __init__(self, model_name: str = \"Qwen3-32B\", batch_size: int = 5, max_workers: int = 3):\n        \"\"\"\n        Initialize the evaluator\n        Args:\n            model_name: Name of the LLM model to use\n            batch_size: Number of documents to process in parallel\n            max_workers: Maximum number of worker threads\n        \"\"\"",
        "detail": "core.generation.relevance",
        "documentation": {}
    },
    {
        "label": "calculate_final_score",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def calculate_final_score(relevance: float, authority: float, timeliness: float) -> float:\n    \"\"\"\n    Calculate final score using predefined weights.\n    Args:\n        relevance: Relevance score (1-10)\n        authority: Authority score (1-10)\n        timeliness: Timeliness score (1-10)\n    Returns:\n        Final weighted score (1-10)\n    \"\"\"",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "extract_scores_from_response_robust",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def extract_scores_from_response_robust(response: str, num_papers: int) -> List[Tuple[int, float]]:\n    \"\"\"\n    More robust version of extract_scores_from_response with multiple fallback patterns.\n    Now extracts individual scores and calculates final score locally.\n    Args:\n        response: LLM response text\n        num_papers: Expected number of papers\n    Returns:\n        List of (paper_index, final_score) tuples\n    \"\"\"",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "rerank_papers_with_llm",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def rerank_papers_with_llm(\n    papers: List[Dict[str, Any]],\n    query: str,\n    model_name: str = \"Qwen3-32B-long-ctx\",\n    max_papers: int = 20,\n    max_retries: int = 5,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Rerank papers using LLM based on relevance, authority, and timeliness.\n    Final score is calculated locally, not by LLM.",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "format_papers_for_ranking",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def format_papers_for_ranking(papers: List[Dict[str, Any]]) -> str:\n    \"\"\"Format papers for LLM input\"\"\"\n    formatted_papers = []\n    for i, paper in enumerate(papers):\n        # Extract basic information\n        title = paper.get(\"title\", \"Unknown Title\")\n        authors = paper.get(\"authors\", \"\")\n        if isinstance(authors, list):\n            authors = \"; \".join([(author.get(\"name\", str(author)) if isinstance(author, dict) else str(author)) for author in authors])\n        abstract = paper.get(\"text\", paper.get(\"abstract\", \"\"))[:2000]  # Limit abstract length",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "extract_scores_from_response",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def extract_scores_from_response(response: str, num_papers: int) -> List[Tuple[int, float]]:\n    \"\"\"Extract paper IDs and scores from LLM response\"\"\"\n    scores = []\n    try:\n        # Try to extract from FINAL RANKING section\n        ranking_pattern = r\"FINAL RANKING:(.*?)(?:\\n\\n|\\Z)\"\n        ranking_match = re.search(ranking_pattern, response, re.DOTALL)\n        if ranking_match:\n            ranking_section = ranking_match.group(1)\n            # Pattern: \"1. Paper ID: 3 - Score: 8.5\"",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "try_simplified_rerank",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def try_simplified_rerank(papers: List[Dict[str, Any]], query: str, model_name: str) -> List[Tuple[int, float]]:\n    \"\"\"\n    Try a simplified reranking approach when the main method fails.\n    Args:\n        papers: List of paper dictionaries\n        query: The search query\n        model_name: Name of the LLM model to use\n    Returns:\n        List of (paper_index, score) tuples\n    \"\"\"",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "extract_scores_from_response",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def extract_scores_from_response(response: str, num_papers: int) -> List[Tuple[int, float]]:\n    \"\"\"Extract paper IDs and scores from LLM response with fallback methods\"\"\"\n    return extract_scores_from_response_robust(response, num_papers)\ndef rerank_papers_batch(\n    papers: List[Dict[str, Any]],\n    query: str,\n    batch_size: int = 10,\n    model_name: str = \"Qwen3-32B\",\n) -> List[Dict[str, Any]]:\n    \"\"\"",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "rerank_papers_batch",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def rerank_papers_batch(\n    papers: List[Dict[str, Any]],\n    query: str,\n    batch_size: int = 10,\n    model_name: str = \"Qwen3-32B\",\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Rerank papers in batches for better performance with large lists.\n    Args:\n        papers: List of paper dictionaries",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "simple_rule_based_rerank",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def simple_rule_based_rerank(papers: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Simple rule-based reranking as fallback when LLM fails.\n    Args:\n        papers: List of paper dictionaries\n        query: The search query\n    Returns:\n        List of reranked papers\n    \"\"\"\n    logger.info(\"Using rule-based reranking as fallback\")",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "rerank_papers_hybrid",
        "kind": 2,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "def rerank_papers_hybrid(\n    papers: List[Dict[str, Any]],\n    query: str,\n    model_name: str = \"Qwen3-14B\",\n    use_llm: bool = True,\n    max_papers_for_llm: int = 20,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Hybrid reranking: use LLM for top papers, rule-based for the rest.\n    Args:",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "RERANK_TEMPLATE",
        "kind": 5,
        "importPath": "core.generation.reranker",
        "description": "core.generation.reranker",
        "peekOfCode": "RERANK_TEMPLATE = \"\"\"You are an expert academic researcher tasked with ranking research papers based on their relevance, authority, and timeliness for a given query.\nQuery: \"{query}\"\nPapers to rank (ordered by original retrieval score):\n{papers}\nPlease evaluate each paper based on the following criteria:\n1. **Relevance (40%)**: How well does the paper's title, abstract, and content match the query?\n2. **Authority (35%)**: Consider citation count, venue prestige, and author reputation.\n3. **Timeliness (25%)**: More recent papers are generally preferred, but groundbreaking older papers may rank higher.\nFor each paper, provide ONLY the individual scores (1-10) for each criterion:\nOutput format for each paper:",
        "detail": "core.generation.reranker",
        "documentation": {}
    },
    {
        "label": "retrival_offline",
        "kind": 2,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "def retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        return {}\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        retrival_inp = {\n            \"query\": query,\n            \"domains\": \"local\",\n            \"n_docs\": RECALL_DOCS,\n            \"search_type\": RETRIVAL_TYPE,",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "retrival_online",
        "kind": 2,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "def retrival_online(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        response = get_doc_info_from_api(query)\n        if response:\n            output = {}\n            for _id, paper in response.items():\n                paper[\"authors\"] = \";\".join(one[\"name\"] for one in paper[\"authors\"])\n                paper[\"url\"] = paper[\"arxivUrl\"]",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "extract_keywords",
        "kind": 2,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "def extract_keywords(query: str, source: str = \"semantic\") -> List[str]:\n    \"\"\"Extract keywords from query optimized for a specific source.\"\"\"\n    # LLM_MODEL_NAME = \"Qwen3-32B\"\n    LLM_MODEL_NAME = \"Qwen3-14B\"\n    query = query.lower()\n    model_inp = template_extract_keywords_source_aware.format(\n        user_query=query, source=source\n    )\n    for _ in range(10):\n        try:",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "retrival_online_openalex",
        "kind": 2,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "def retrival_online_openalex(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        logger.info(f\"retrival online openalex: {data}\")\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        query2kwds = extract_keywords(query, source=\"openalex\")\n        logger.info(f\"Extracted keywords for OpenAlex: {query2kwds}\")\n        output = {}\n        for kwd in query2kwds[:3]:\n            if not kwd.strip():",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "convert_crawl_results_to_ctxs",
        "kind": 2,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "def convert_crawl_results_to_ctxs(crawl_results: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"\n    ctxs\n    Args:\n        crawl_results: search_and_crawl_data_only\n    Returns:\n        ctxs\n    \"\"\"\n    ctxs = []\n    if not crawl_results or not crawl_results.get(\"success\"):",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\n#  FastAPI \napp = FastAPI()\n# retrival\nRETRIVAL_TYPE = \"bm25\"  # bm25 emb\nRETRIVAL_TYPE = \"emb\"  # bm25 emb\nRECALL_DOCS = 5\n#  1 API \ndef retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "app = FastAPI()\n# retrival\nRETRIVAL_TYPE = \"bm25\"  # bm25 emb\nRETRIVAL_TYPE = \"emb\"  # bm25 emb\nRECALL_DOCS = 5\n#  1 API \ndef retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        return {}\n        query = data[\"query\"]",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "RETRIVAL_TYPE",
        "kind": 5,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "RETRIVAL_TYPE = \"bm25\"  # bm25 emb\nRETRIVAL_TYPE = \"emb\"  # bm25 emb\nRECALL_DOCS = 5\n#  1 API \ndef retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        return {}\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        retrival_inp = {",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "RETRIVAL_TYPE",
        "kind": 5,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "RETRIVAL_TYPE = \"emb\"  # bm25 emb\nRECALL_DOCS = 5\n#  1 API \ndef retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        return {}\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        retrival_inp = {\n            \"query\": query,",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "RECALL_DOCS",
        "kind": 5,
        "importPath": "core.generation.retrival",
        "description": "core.generation.retrival",
        "peekOfCode": "RECALL_DOCS = 5\n#  1 API \ndef retrival_offline(data: Dict[str, Any]) -> Dict[str, Any]:\n    try:\n        return {}\n        query = data[\"query\"]\n        task_id = data.get(\"task_id\", \"example_id\")\n        retrival_inp = {\n            \"query\": query,\n            \"domains\": \"local\",",
        "detail": "core.generation.retrival",
        "documentation": {}
    },
    {
        "label": "OpenScholarAPIError",
        "kind": 6,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "class OpenScholarAPIError(Exception):\n    \"\"\"Custom exception class for OpenScholarAPI errors.\"\"\"\n    pass\nclass LLMCommunicationError(OpenScholarAPIError):\n    \"\"\"Exception raised for errors during communication with the LLM.\"\"\"\n    pass\nclass DataProcessingError(OpenScholarAPIError):\n    \"\"\"Exception raised for errors during data processing.\"\"\"\n    pass\nclass OpenScholarAPI:",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "LLMCommunicationError",
        "kind": 6,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "class LLMCommunicationError(OpenScholarAPIError):\n    \"\"\"Exception raised for errors during communication with the LLM.\"\"\"\n    pass\nclass DataProcessingError(OpenScholarAPIError):\n    \"\"\"Exception raised for errors during data processing.\"\"\"\n    pass\nclass OpenScholarAPI:\n    \"\"\"\n    A class to interact with scientific literature APIs for tasks like retrieval,\n    ranking, and response generation. Enhanced with improved error handling,",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "DataProcessingError",
        "kind": 6,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "class DataProcessingError(OpenScholarAPIError):\n    \"\"\"Exception raised for errors during data processing.\"\"\"\n    pass\nclass OpenScholarAPI:\n    \"\"\"\n    A class to interact with scientific literature APIs for tasks like retrieval,\n    ranking, and response generation. Enhanced with improved error handling,\n    data structures, and logical flow.\n    \"\"\"\n    def __init__(",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "OpenScholarAPI",
        "kind": 6,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "class OpenScholarAPI:\n    \"\"\"\n    A class to interact with scientific literature APIs for tasks like retrieval,\n    ranking, and response generation. Enhanced with improved error handling,\n    data structures, and logical flow.\n    \"\"\"\n    def __init__(\n        self,\n        client=None,  # Consider type hinting if possible\n        api_model_name: Optional[str] = \"Qwen3-8B\",  # Use constant default",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "ResponseRagChat",
        "kind": 6,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "class ResponseRagChat(BaseModel):\n    request_id: str\n    query: str\n    status: int\n    message: str = None\n    ctx: List[Dict[str, Any]] = None\n    output: str = None\n    time_cost: Optional[Dict[str, Any]] = None\n    main_figure_base64: str = None\n    main_figure_caption: str = None",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "RESPONSE_START_DELIMITER",
        "kind": 5,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "RESPONSE_START_DELIMITER = \"[Response_Start]\"\nRESPONSE_END_DELIMITER = \"[Response_End]\"\nREFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\n# Load spaCy model once\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    logger.error(\n        \"spaCy model 'en_core_web_sm' not found. Please download it: python -m spacy download en_core_web_sm\"",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "RESPONSE_END_DELIMITER",
        "kind": 5,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "RESPONSE_END_DELIMITER = \"[Response_End]\"\nREFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\n# Load spaCy model once\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    logger.error(\n        \"spaCy model 'en_core_web_sm' not found. Please download it: python -m spacy download en_core_web_sm\"\n    )",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "REFERENCES_HEADER",
        "kind": 5,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "REFERENCES_HEADER = \"References:\"\nREVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\n# Load spaCy model once\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    logger.error(\n        \"spaCy model 'en_core_web_sm' not found. Please download it: python -m spacy download en_core_web_sm\"\n    )\n    # Depending on the application's criticality, you might want to exit or use a fallback",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "REVISED_ANSWER_HEADER",
        "kind": 5,
        "importPath": "core.generation.section_writer_actor",
        "description": "core.generation.section_writer_actor",
        "peekOfCode": "REVISED_ANSWER_HEADER = \"Here is the revised answer:\\n\\n\"\n# Load spaCy model once\ntry:\n    nlp = spacy.load(\"en_core_web_sm\")\nexcept OSError:\n    logger.error(\n        \"spaCy model 'en_core_web_sm' not found. Please download it: python -m spacy download en_core_web_sm\"\n    )\n    # Depending on the application's criticality, you might want to exit or use a fallback\n    nlp = None  # Or raise an exception",
        "detail": "core.generation.section_writer_actor",
        "documentation": {}
    },
    {
        "label": "load_jsonlines",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def load_jsonlines(file):\n    try:\n        with jsonlines.open(file, 'r') as jsonl_f:\n            lst = [obj for obj in jsonl_f]\n    except:\n        lst = []\n        with open(file) as f:\n            for line in f:\n                lst.append(json.loads(line))\n    return lst",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "save_file_jsonl",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def save_file_jsonl(data, fp):\n    with jsonlines.open(fp, mode='w') as writer:\n        writer.write_all(data)\ndef extract_titles(text):\n    # Regular expression pattern to match the titles\n    pattern = r'\\[\\d+\\]\\s(.+)'  # Matches [number] followed by any characters\n    # Find all matches of the pattern in the reference text\n    if \"References:\" not in text:\n        return []\n    else:",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "extract_titles",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def extract_titles(text):\n    # Regular expression pattern to match the titles\n    pattern = r'\\[\\d+\\]\\s(.+)'  # Matches [number] followed by any characters\n    # Find all matches of the pattern in the reference text\n    if \"References:\" not in text:\n        return []\n    else:\n        reference_text = text.split(\"References:\")[1]\n        print(reference_text)\n        matches = re.findall(pattern, reference_text)",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "save_tsv_dict",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def save_tsv_dict(data, fp, fields):\n    # build dir\n    dir_path = os.path.dirname(fp)\n    os.makedirs(dir_path, exist_ok=True)\n    # writing to csv file\n    with open(fp, 'w') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fields, delimiter='\\t',)\n        writer.writeheader()\n        writer.writerows(data)\ndef keep_letters(s):",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "keep_letters",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def keep_letters(s):\n    letters = [c for c in s if c.isalpha()]\n    result = \"\".join(letters)\n    return result.lower()\ndef get_md5(string):\n    #  MD5 \n    md5_hash = hashlib.md5()\n    # MD5 \n    md5_hash.update(string.encode(\"utf-8\"))\n    #  16 ",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "get_md5",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def get_md5(string):\n    #  MD5 \n    md5_hash = hashlib.md5()\n    # MD5 \n    md5_hash.update(string.encode(\"utf-8\"))\n    #  16 \n    return md5_hash.hexdigest()\ndef flow_information_sync(\n    task_id: str,\n    status: str = \"processing\",",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "flow_information_sync",
        "kind": 2,
        "importPath": "core.generation.utils",
        "description": "core.generation.utils",
        "peekOfCode": "def flow_information_sync(\n    task_id: str,\n    status: str = \"processing\",\n    base_url: str = \"\",\n    content: str = None,\n    is_deal: bool = False,\n    flowchart_data: dict = None,\n    report_content: str = None,\n) -> dict:\n    \"\"\"",
        "detail": "core.generation.utils",
        "documentation": {}
    },
    {
        "label": "GeneralCrawer",
        "kind": 6,
        "importPath": "core.generation.websearch_general",
        "description": "core.generation.websearch_general",
        "peekOfCode": "class GeneralCrawer:\n    # Configuration constants\n    MAX_CONCURRENT_CRAWLS = 10\n    MAX_CONCURRENT_PROCESSES = 10\n    # Document processing constants\n    DEFAULT_SIMILARITY_THRESHOLD = 80\n    DEFAULT_MIN_LENGTH = 350\n    DEFAULT_MAX_LENGTH = 20000\n    def __init__(self, model=\"Qwen3-8B\", infer_type=\"local\"):\n        \"\"\"",
        "detail": "core.generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "searxng_search",
        "kind": 2,
        "importPath": "core.generation.websearch_general",
        "description": "core.generation.websearch_general",
        "peekOfCode": "def searxng_search(query, instance_url=\"https://searx.namejeff.xyz\", num_results=5):\n    search_url = f\"{instance_url}/search\"\n    params = {\n        \"q\": query,\n        \"format\": \"json\",\n        \"language\": \"zh-CN\",\n        \"categories\": \"news\",\n        \"safesearch\": 1,\n    }\n    try:",
        "detail": "core.generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "get_random_headers",
        "kind": 2,
        "importPath": "core.generation.websearch_general",
        "description": "core.generation.websearch_general",
        "peekOfCode": "def get_random_headers():\n    user_agents = [\n        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\",\n        \"Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/117.0\",\n        \"Mozilla/5.0\",\n        #  user-agent \n    ]\n    return {\n        \"User-Agent\": random.choice(user_agents),",
        "detail": "core.generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "web_search",
        "kind": 2,
        "importPath": "core.generation.websearch_general",
        "description": "core.generation.websearch_general",
        "peekOfCode": "def web_search(query, engine=\"auto\", api_key=None, num_results=5, use_proxy=False):\n    \"\"\"\n     Bing  SerpAPI\n    Args:\n        query (str): \n        engine (str):  (\"auto\", \"bing\", \"serpapi\")\n        api_key (str, optional): SerpAPI \n        num_results (int): \n        use_proxy (bool): \n    Returns:",
        "detail": "core.generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "core.generation.websearch_general",
        "description": "core.generation.websearch_general",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\nclass GeneralCrawer:\n    # Configuration constants\n    MAX_CONCURRENT_CRAWLS = 10\n    MAX_CONCURRENT_PROCESSES = 10\n    # Document processing constants\n    DEFAULT_SIMILARITY_THRESHOLD = 80\n    DEFAULT_MIN_LENGTH = 350\n    DEFAULT_MAX_LENGTH = 20000\n    def __init__(self, model=\"Qwen3-8B\", infer_type=\"local\"):",
        "detail": "core.generation.websearch_general",
        "documentation": {}
    },
    {
        "label": "fetch_pubmed_json",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def fetch_pubmed_json(pmid_list):\n    \"\"\"\n    Retrieve structured PubMed paper information with enhanced metadata extraction.\n    Extracts comprehensive information including:\n    - ArXiv identifiers (when available)\n    - Publication dates (year, month, day)\n    - Research fields/MeSH terms\n    - DOI identifiers\n    - Journal information\n    - Citation counts (when available)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_from_pubmed",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_from_pubmed(query, max_results=10):\n    \"\"\" PubMed \"\"\"\n    # 1.  PubMed \n    handle = Entrez.esearch(\n        db=\"pubmed\",\n        term=query,\n        retmax=max_results,\n        sort=\"relevance\",\n        httppost={\"proxies\": proxies},  # \n    )",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "fetch_reference_openalex",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def fetch_reference_openalex(ref_id):\n    \"\"\"\"\"\"\n    try:\n        ref_url = f\"https://api.openalex.org/works/{ref_id}\"\n        ref_response = requests.get(ref_url)\n        if ref_response.status_code != 200:\n            logger.error(\n                f\"Reference Failed to retrieve reference data: {ref_response.status_code}\"\n            )\n            return None",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_doc_via_url_parallel_openalex",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_doc_via_url_parallel_openalex(referenced_works, max_workers=4, timeout=10):\n    \"\"\"\"\"\"\n    references = []\n    if len(referenced_works) > 100:\n        referenced_works = random.sample(referenced_works, 20)\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_ref = {\n            executor.submit(fetch_reference_openalex, ref_id): ref_id\n            for ref_id in referenced_works\n        }",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_doc_via_url_from_openalex",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_doc_via_url_from_openalex(data):\n    referenced_works = data\n    references = []\n    for ref_id in referenced_works:\n        try:\n            ref_url = f\"https://api.openalex.org/works/{ref_id}\"\n            ref_response = requests.get(ref_url, timeout=10)\n            if ref_response.status_code == 200:\n                ref_data = ref_response.json()\n                is_open = ref_data.get(\"open_access\", {}).get(\"is_oa\", False)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_paper_via_query_from_openalex",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_paper_via_query_from_openalex(\n    keyword, page=1, per_page=10, search_reference=False\n):\n    \"\"\"openalex\"\"\"\n    try:\n        logger.info(\"search_paper_via_query_from_openalex\")\n        base_url = \"https://api.openalex.org/works\"\n        current_year = datetime.now().year\n        start_year = current_year - 20\n        # print(f\"start_year: {start_year}\")",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_paper_via_query_from_semantic",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_paper_via_query_from_semantic(query, max_paper_num=15, end_date=None):\n    \"\"\"\n    semantic sholarqueryquery\n    find schema info here:\n    https://api.semanticscholar.org/api-docs/#tag/Paper-Data/operation/get_graph_paper_relevance_search\n    citationCount: \n    referenceCount: \n    \"\"\"\n    logger.info(\"run search_paper_via_query_from_semantic...\")\n    if \"Search queries: \" in query:",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "google_search_arxiv_id",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def google_search_arxiv_id(query, try_num=4, num=10, end_date=\"\"):\n    \"\"\"googlearxiv id, api2500\"\"\"\n    # refer from: https://serper.dev/playground\n    url = \"https://google.serper.dev/search\"\n    search_query = f\"{query} site:arxiv.org\"\n    # logger.info(f\"end_date: {end_date}\")\n    if end_date != \"\":\n        try:\n            end_date = datetime.strptime(end_date, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n            search_query = f\"{query} before:{end_date} site:arxiv.org\"",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "get_doc_info_from_semantic_scholar_by_arxivid",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def get_doc_info_from_semantic_scholar_by_arxivid(\n    arxiv_id: str, try_num: int = 5, timeout: float = 10.0, raise_on_error: bool = False\n) -> Optional[Dict[str, Any]]:\n    \"\"\"\n     arXiv ID  Semantic Scholar \n    Args:\n        arxiv_id (str): arXiv  ID \"1706.03762\"\n        try_num (int):  5\n        timeout (float):  20.0\n        raise_on_error (bool):  False",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "get_openalex_info",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def get_openalex_info(arxiv_id):\n    # \n    url = f\"https://api.openalex.org/works/https://arxiv.org/abs/{arxiv_id}\"\n    response = requests.get(url, proxies=proxies)\n    if response.status_code == 200:\n        data = response.json()\n        return {\n            \"title\": data.get(\"title\"),\n            \"cited_by_count\": data.get(\"cited_by_count\"),\n            \"referenced_works\": data.get(\"referenced_works\"),",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_paper_from_arxiv_by_arxiv_id",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_paper_from_arxiv_by_arxiv_id(arxiv_id, try_num=5):\n    \"\"\"\n    Search paper by arxiv id from Arxiv.\n    :param arxiv_id: arxiv id of the paper\n    :return: paper list\n    \"\"\"\n    logger.info(\"search_paper_from_arxiv_by_arxiv_id ... \")\n    search = arxiv.Search(\n        query=\"\",\n        id_list=[arxiv_id],",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "search_paper_from_arxiv_by_arxiv_id_bsz",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def search_paper_from_arxiv_by_arxiv_id_bsz(arxiv_ids, max_retries=3):\n    \"\"\"\n     Arxiv ID\n    :param arxiv_ids:  ID \n    :param max_retries: \n    :return: Dict[str, Dict] -> {arxivId: paper_info}\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            search = arxiv.Search(",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "parallel_search_search_paper_from_arxiv",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def parallel_search_search_paper_from_arxiv(\n    arxiv_id_list: List[str],\n    batch_size: int = 5,\n    max_workers: int = 2\n) -> Dict[str, Dict]:\n    \"\"\"\n     Arxiv \n    \"\"\"\n    if not arxiv_id_list:\n        return {}",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "get_doc_info_from_api",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def get_doc_info_from_api(query, try_num=5, end_date=\"\"):\n    \"\"\"\n    querygooglearxiv_idarxiv\n    \"\"\"\n    try:\n        arxiv_lst = google_search_arxiv_id(\n            query=query, try_num=try_num, end_date=end_date\n        )\n        logger.info(f\"arxiv_lst: {len(arxiv_lst)}\")\n        doc_info = parallel_search_search_paper_from_arxiv(arxiv_lst)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "get_doc_info_from_api",
        "kind": 2,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "def get_doc_info_from_api(query, try_num=5, end_date=\"\"):\n    \"\"\"\n    querygooglearxiv_idarxiv\n    \"\"\"\n    try:\n        arxiv_lst = google_search_arxiv_id(\n            query=query, try_num=try_num, end_date=end_date\n        )\n        logger.info(f\"arxiv_lst: {len(arxiv_lst)}\")\n        doc_info = parallel_search_search_paper_from_arxiv(arxiv_lst)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "Entrez.email",
        "kind": 5,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "Entrez.email = \"xxx@163.com\"\nproxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\narxiv_client = arxiv.Client(delay_seconds=0.05)\n# Register at: https://serper.dev/\nGOOGLE_SERPER_KEY = os.getenv(\"GOOGLE_SERPER_KEY\", \"xxx\")\ndef fetch_pubmed_json(pmid_list):\n    \"\"\"\n    Retrieve structured PubMed paper information with enhanced metadata extraction.\n    Extracts comprehensive information including:\n    - ArXiv identifiers (when available)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "proxies",
        "kind": 5,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "proxies = {\"http\": \"http://localhost:1080\", \"https\": \"http://localhost:1080\"}\narxiv_client = arxiv.Client(delay_seconds=0.05)\n# Register at: https://serper.dev/\nGOOGLE_SERPER_KEY = os.getenv(\"GOOGLE_SERPER_KEY\", \"xxx\")\ndef fetch_pubmed_json(pmid_list):\n    \"\"\"\n    Retrieve structured PubMed paper information with enhanced metadata extraction.\n    Extracts comprehensive information including:\n    - ArXiv identifiers (when available)\n    - Publication dates (year, month, day)",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "arxiv_client",
        "kind": 5,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "arxiv_client = arxiv.Client(delay_seconds=0.05)\n# Register at: https://serper.dev/\nGOOGLE_SERPER_KEY = os.getenv(\"GOOGLE_SERPER_KEY\", \"xxx\")\ndef fetch_pubmed_json(pmid_list):\n    \"\"\"\n    Retrieve structured PubMed paper information with enhanced metadata extraction.\n    Extracts comprehensive information including:\n    - ArXiv identifiers (when available)\n    - Publication dates (year, month, day)\n    - Research fields/MeSH terms",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "GOOGLE_SERPER_KEY",
        "kind": 5,
        "importPath": "core.generation.websearch_scholar",
        "description": "core.generation.websearch_scholar",
        "peekOfCode": "GOOGLE_SERPER_KEY = os.getenv(\"GOOGLE_SERPER_KEY\", \"xxx\")\ndef fetch_pubmed_json(pmid_list):\n    \"\"\"\n    Retrieve structured PubMed paper information with enhanced metadata extraction.\n    Extracts comprehensive information including:\n    - ArXiv identifiers (when available)\n    - Publication dates (year, month, day)\n    - Research fields/MeSH terms\n    - DOI identifiers\n    - Journal information",
        "detail": "core.generation.websearch_scholar",
        "documentation": {}
    },
    {
        "label": "OpenAIRequest",
        "kind": 6,
        "importPath": "core.request_warp._openai",
        "description": "core.request_warp._openai",
        "peekOfCode": "class OpenAIRequest:\n    def __init__(self, model):\n        self.client = OpenAI(\n            api_key=os.environ.get(\"OPENAI_API_KEY\"),\n            base_url=os.environ.get(\"OPENAI_API_BASE\"),\n        )\n        self.model = model\n    @retry(\n        wait=wait_random_exponential(multiplier=2, max=60),\n        stop=stop_after_attempt(100),",
        "detail": "core.request_warp._openai",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.request_warp._openai",
        "description": "core.request_warp._openai",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass OpenAIRequest:\n    def __init__(self, model):\n        self.client = OpenAI(\n            api_key=os.environ.get(\"OPENAI_API_KEY\"),\n            base_url=os.environ.get(\"OPENAI_API_BASE\"),\n        )\n        self.model = model\n    @retry(\n        wait=wait_random_exponential(multiplier=2, max=60),",
        "detail": "core.request_warp._openai",
        "documentation": {}
    },
    {
        "label": "GoogleRequest",
        "kind": 6,
        "importPath": "core.request_warp.google",
        "description": "core.request_warp.google",
        "peekOfCode": "class GoogleRequest:\n    def __init__(self, model: str):\n        self.client = genai.Client(\n            api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n            )\n        self.model = model\n    @retry(\n        wait=wait_random_exponential(multiplier=2, max=60),\n        stop=stop_after_attempt(10),\n        retry=retry_if_exception_type(Exception)  # ",
        "detail": "core.request_warp.google",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.request_warp.google",
        "description": "core.request_warp.google",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# proxy = \"http://127.0.0.1:7890\"\n# os.environ[\"HTTP_PROXY\"]  = proxy\n# os.environ[\"http_proxy\"]  = proxy\n# os.environ[\"HTTPS_PROXY\"] = proxy\n# os.environ[\"https_proxy\"] = proxy\nfrom google import genai\nclass GoogleRequest:\n    def __init__(self, model: str):\n        self.client = genai.Client(",
        "detail": "core.request_warp.google",
        "documentation": {}
    },
    {
        "label": "LocalRequest",
        "kind": 6,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "class LocalRequest:\n    def __init__(self, model=\"xxx\"):\n        self.model = model\n        logger.warning(\n            f\"Token counter is not supported in LocalRequest, each request will be counted as 1 token\"\n        )\n    @retry(\n        wait=wait_random_exponential(multiplier=2, max=60),\n        stop=stop_after_attempt(30),\n        retry=retry_if_exception_type(",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "file_path = \"./local_request_v2.py\"\n# 2. \nmodule_name = \"local_request\"\n# 3. \nspec = importlib.util.spec_from_file_location(module_name, file_path)\nyour_script = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = your_script\nspec.loader.exec_module(your_script)\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "module_name",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "module_name = \"local_request\"\n# 3. \nspec = importlib.util.spec_from_file_location(module_name, file_path)\nyour_script = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = your_script\nspec.loader.exec_module(your_script)\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError\nfrom tenacity import (\n    retry,",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "spec",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "spec = importlib.util.spec_from_file_location(module_name, file_path)\nyour_script = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = your_script\nspec.loader.exec_module(your_script)\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "your_script",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "your_script = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = your_script\nspec.loader.exec_module(your_script)\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n    before_sleep_log,",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "sys.modules[module_name]",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "sys.modules[module_name] = your_script\nspec.loader.exec_module(your_script)\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n    before_sleep_log,\n    retry_if_exception_type,",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.request_warp.local",
        "description": "core.request_warp.local",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass LocalRequest:\n    def __init__(self, model=\"xxx\"):\n        self.model = model\n        logger.warning(\n            f\"Token counter is not supported in LocalRequest, each request will be counted as 1 token\"\n        )\n    @retry(\n        wait=wait_random_exponential(multiplier=2, max=60),\n        stop=stop_after_attempt(30),",
        "detail": "core.request_warp.local",
        "documentation": {}
    },
    {
        "label": "track_completion_calls",
        "kind": 2,
        "importPath": "core.request_warp.token_counter",
        "description": "core.request_warp.token_counter",
        "peekOfCode": "def track_completion_calls(cls):\n    original_completion = cls.completion\n    def _completion_counter():\n        cls._calls_count += 1\n        logger.info(f\"Current total count of openai api calls: {cls._calls_count}\")\n        return\n    def token_logger(self, *args, **kwargs):\n        answer = original_completion(self, *args, **kwargs)\n        _completion_counter()\n        table_data = []",
        "detail": "core.request_warp.token_counter",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.request_warp.token_counter",
        "description": "core.request_warp.token_counter",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef track_completion_calls(cls):\n    original_completion = cls.completion\n    def _completion_counter():\n        cls._calls_count += 1\n        logger.info(f\"Current total count of openai api calls: {cls._calls_count}\")\n        return\n    def token_logger(self, *args, **kwargs):\n        answer = original_completion(self, *args, **kwargs)\n        _completion_counter()",
        "detail": "core.request_warp.token_counter",
        "documentation": {}
    },
    {
        "label": "RequestWrapper",
        "kind": 6,
        "importPath": "core.request_warp.wrapper",
        "description": "core.request_warp.wrapper",
        "peekOfCode": "class RequestWrapper:\n    _connection_semaphore = {}\n    _calls_count = 0  # api\n    _token_usage_history = []  # token\n    def __init__(self, model=\"xxx\", infer_type=\"local\", connection=20, port=None):\n        self.request_pool = None\n        self.model = model\n        self._connection_semaphore[model] = Semaphore(connection)\n        self.request_pool = LocalRequest(model=model)\n    def completion(self, message, **kwargs):",
        "detail": "core.request_warp.wrapper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.request_warp.wrapper",
        "description": "core.request_warp.wrapper",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass RequestWrapper:\n    _connection_semaphore = {}\n    _calls_count = 0  # api\n    _token_usage_history = []  # token\n    def __init__(self, model=\"xxx\", infer_type=\"local\", connection=20, port=None):\n        self.request_pool = None\n        self.model = model\n        self._connection_semaphore[model] = Semaphore(connection)\n        self.request_pool = LocalRequest(model=model)",
        "detail": "core.request_warp.wrapper",
        "documentation": {}
    },
    {
        "label": "PaperGenerationRequest",
        "kind": 6,
        "importPath": "core.app",
        "description": "core.app",
        "peekOfCode": "class PaperGenerationRequest(BaseModel):\n    \"\"\"Request model for paper generation\"\"\"\n    user_query: str = Field(..., description=\"User's research query\")\n    user_name: str = Field(default=\"researcher\", description=\"Username\")\n    task_id: Optional[str] = Field(default=None, description=\"Custom task ID\")\n    output_dir: str = Field(default=\"temp\", description=\"Output directory\")\n    rag_service_url: Optional[str] = Field(default=None, description=\"RAG service URL\")\n    # Pipeline configuration\n    outline_max_reflections: int = Field(default=1, description=\"Max outline reflections\")\n    outline_max_sections: int = Field(default=5, description=\"Max sections in outline\")",
        "detail": "core.app",
        "documentation": {}
    },
    {
        "label": "TaskStatusResponse",
        "kind": 6,
        "importPath": "core.app",
        "description": "core.app",
        "peekOfCode": "class TaskStatusResponse(BaseModel):\n    \"\"\"Response model for task status\"\"\"\n    task_id: str\n    status: str  # \"running\", \"completed\", \"error\", \"not_found\"\n    progress: Dict[str, Any]\n    result: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n    created_at: str\n    updated_at: str\nasync def run_paper_generation_task(",
        "detail": "core.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "core.app",
        "description": "core.app",
        "peekOfCode": "app = FastAPI(\n    title=\"Academic Paper Generation API\",\n    description=\"API for generating academic papers using AI\",\n    version=\"1.0.0\"\n)\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # In production, specify allowed origins\n    allow_credentials=True,",
        "detail": "core.app",
        "documentation": {}
    },
    {
        "label": "task_lock",
        "kind": 5,
        "importPath": "core.app",
        "description": "core.app",
        "peekOfCode": "task_lock = Lock()\nclass PaperGenerationRequest(BaseModel):\n    \"\"\"Request model for paper generation\"\"\"\n    user_query: str = Field(..., description=\"User's research query\")\n    user_name: str = Field(default=\"researcher\", description=\"Username\")\n    task_id: Optional[str] = Field(default=None, description=\"Custom task ID\")\n    output_dir: str = Field(default=\"temp\", description=\"Output directory\")\n    rag_service_url: Optional[str] = Field(default=None, description=\"RAG service URL\")\n    # Pipeline configuration\n    outline_max_reflections: int = Field(default=1, description=\"Max outline reflections\")",
        "detail": "core.app",
        "documentation": {}
    },
    {
        "label": "ModelConfig",
        "kind": 6,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "class ModelConfig:\n    \"\"\"Configuration for LLM models\"\"\"\n    url: str\n    max_len: int\n    temperature: float = 0.8\n    model_name: str = \"\"\n    top_p: float = 0.9\n    top_k: int = 20\n    min_p: int = 0\n    retry_attempts: int = 20",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "GlobalSemaphorePool",
        "kind": 6,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "class GlobalSemaphorePool:\n    \"\"\"\"\"\"\n    # \n    rag_semaphore = asyncio.Semaphore(2)\n    # reflection_semaphore\n    section_reflection_semaphore = asyncio.Semaphore(3)\n    # \n    section_name_refine_semaphore = asyncio.Semaphore(2)\n# \nglobal_semaphores = GlobalSemaphorePool()",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "HOST",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "HOST = os.getenv(\"LOCAL_LLM_HOST\", \"http://0.0.0.0\")\nprint(f\"Using LLM host: {HOST}\")\n# Model configurations\nMODEL_CONFIGS = {\n    \"llama3-70b\": ModelConfig(\n        url=f\"{HOST}:9087/v1/chat/completions\",\n        max_len=131072,\n    ),\n    \"Qwen3-8B\": ModelConfig(\n        url=f\"{HOST}:9096/v1\",",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "MODEL_CONFIGS",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "MODEL_CONFIGS = {\n    \"llama3-70b\": ModelConfig(\n        url=f\"{HOST}:9087/v1/chat/completions\",\n        max_len=131072,\n    ),\n    \"Qwen3-8B\": ModelConfig(\n        url=f\"{HOST}:9096/v1\",\n        max_len=131072,\n        model_name=\"Qwen/Qwen3-8B\",\n        think_bool=False,",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_QUERY_INTENT",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "DEFAULT_MODEL_FOR_QUERY_INTENT = (\n    \"Qwen3-32B\"  # Model used for generating search queries from outline content points\n)\n# -- OUTLINE --\nDEFAULT_MODEL_FOR_OUTLINE: str = \"Qwen3-32B\"  # Changed from gpt-4-32k to Qwen25-72B\n# Currently using only Qwen model for outline generation\nOUTLINE_GENERAOR_MODELS: List[str] = [\n    \"Qwen3-32B\",\n    \"Qwen3-14B\",\n]",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER_IMAGE_EXTRACT:str",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "DEFAULT_MODEL_FOR_SECTION_WRITER_IMAGE_EXTRACT:str = \"Qwen3-14B\"\nDEFAULT_MODEL_FOR_SECTION_WRITER_RERANK:str = \"Qwen3-14B\"\nSECTION_SUMMARY_MODEL: str = \"Qwen3-32B\"  # gpt-4-32k\nSECTION_REFLECTION_MODEL_LST = [\"Qwen3-32B\", \"llama3-70b\"]\n# ---- GLOBAL REFLECTION ----\nDO_GLOBAL_REFLECTION: bool = True  # Enable global reflection on entire paper\nGLOBAL_REFLECTION_MAX_TURNS: int = 1  # Maximum number of global reflection turns\nDEFAULT_MODEL_FOR_GLOBAL_REFLECTION: str = \"Qwen3-32B\"\n# --- ABSTRACT & CONCLUSION ---\nGLOBAL_ABSTRACT_CONCLUSION_MAX_TURNS: int = 1",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODEL_FOR_SECTION_WRITER_RERANK:str",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "DEFAULT_MODEL_FOR_SECTION_WRITER_RERANK:str = \"Qwen3-14B\"\nSECTION_SUMMARY_MODEL: str = \"Qwen3-32B\"  # gpt-4-32k\nSECTION_REFLECTION_MODEL_LST = [\"Qwen3-32B\", \"llama3-70b\"]\n# ---- GLOBAL REFLECTION ----\nDO_GLOBAL_REFLECTION: bool = True  # Enable global reflection on entire paper\nGLOBAL_REFLECTION_MAX_TURNS: int = 1  # Maximum number of global reflection turns\nDEFAULT_MODEL_FOR_GLOBAL_REFLECTION: str = \"Qwen3-32B\"\n# --- ABSTRACT & CONCLUSION ---\nGLOBAL_ABSTRACT_CONCLUSION_MAX_TURNS: int = 1\nMODEL_GEN_ABSTRACT_CONCLUSION: str = \"Qwen3-32B\"",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "SECTION_REFLECTION_MODEL_LST",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "SECTION_REFLECTION_MODEL_LST = [\"Qwen3-32B\", \"llama3-70b\"]\n# ---- GLOBAL REFLECTION ----\nDO_GLOBAL_REFLECTION: bool = True  # Enable global reflection on entire paper\nGLOBAL_REFLECTION_MAX_TURNS: int = 1  # Maximum number of global reflection turns\nDEFAULT_MODEL_FOR_GLOBAL_REFLECTION: str = \"Qwen3-32B\"\n# --- ABSTRACT & CONCLUSION ---\nGLOBAL_ABSTRACT_CONCLUSION_MAX_TURNS: int = 1\nMODEL_GEN_ABSTRACT_CONCLUSION: str = \"Qwen3-32B\"\n# -- POOLISH --\nDEFAULT_MODEL_FOR_SECTION_NAME_REFLECTION: str = \"Qwen3-32B\"",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "rag_semaphore",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "rag_semaphore = asyncio.Semaphore(SEMAPHORE_LIMIT)\n# \nclass GlobalSemaphorePool:\n    \"\"\"\"\"\"\n    # \n    rag_semaphore = asyncio.Semaphore(2)\n    # reflection_semaphore\n    section_reflection_semaphore = asyncio.Semaphore(3)\n    # \n    section_name_refine_semaphore = asyncio.Semaphore(2)",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "global_semaphores",
        "kind": 5,
        "importPath": "core.configuration",
        "description": "core.configuration",
        "peekOfCode": "global_semaphores = GlobalSemaphorePool()",
        "detail": "core.configuration",
        "documentation": {}
    },
    {
        "label": "example_outline",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_outline = {\n    \"user_query\": \"recent advances in natural language processing\",\n    \"title\": \"Recent Advances in Natural Language Processing\",\n    \"abstract\": \"\",\n    \"research_field\": {\n        \"field\": \"Computer Science\",\n        \"paper_type\": \"survey\",\n        \"topic\": \"Recent Advances in Natural Language Processing\",\n    },\n    \"final_outline\": {",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_section_write",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_section_write = {\n    \"Definition and overview of AI agent frameworks\": {\n        \"search_query\": \"AI agent frameworks capabilities overview\",\n        \"section_key_point\": \"Definition and overview of AI agent frameworks\",\n        \"section_text\": \"AI agent frameworks have evolved significantly over the years, with various architectures and capabilities being developed to tackle complex tasks [4]. The primary objectives of these frameworks are to enable AI agents to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities [4]. One of the key aspects of AI agent frameworks is their ability to integrate large language models (LLMs) with dedicated modules for perception, planning, and tool use [4]. For instance, the MLGym framework and benchmark [3] evaluate and develop LLM agents on AI research tasks, such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, and analyzing results. The framework consists of 13 diverse and open-ended AI research tasks from various domains, including computer vision, natural language processing, reinforcement learning, and game theory [3].\\n\\nThe MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks [3]. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements [3].\\n\\nThis survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities [4]. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design [4]. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal [4].\\n\\nThis paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use [6]. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety [6]. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems [6].\\n\\nLifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments [5]. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges [5]. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents [5]. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment [5]. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance [5].\\n\\nLarge language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains [7]. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions [7]. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception [7]. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks [7]. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions [7]. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks [7]. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction [7].\\n\\nLarge language models (LLMs) have recently demonstrated remarkable capabilities across domains, tasks, and languages [8]. Such human-level agents require semantic comprehension and instruction-following capabilities, which exactly fall into the strengths of LLMs [8]. Although there have been several initial attempts to build human-level agents based on LLMs, the theoretical foundation remains a challenging open problem [8]. In this paper, we propose a novel theoretical cognitive architecture, the Unified Mind Model (UMM), which offers guidance to facilitate the rapid creation of autonomous agents with human-level cognitive abilities [8]. Specifically, our UMM starts with the global workspace theory and further leverage LLMs to enable the agent with various cognitive abilities, such as multi-modal perception, planning, reasoning, tool use, learning, memory, reflection and motivation [8]. Building upon UMM, we then develop an agent-building engine, MindOS, which allows users to quickly create domain-/task-specific autonomous agents without any programming effort [8].\\n\\nIntellAgent is a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively [9]. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations [9]. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics [9]. IntellAgent represents a paradigm shift in evaluating conversational AI [9]. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints [9]. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics [9]. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization [9]. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration [9]. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment [9].\\n\\nAI Agent, powered by large language models (LLMs) as its cognitive core, is an intelligent agentic system capable of autonomously controlling and determining the execution paths under user's instructions [10]. With the burst of capabilities of LLMs and various plugins, such as RAG, text-to-image/video/3D, etc., the potential of AI Agents has been vastly expanded, with their capabilities growing stronger by the day [10]. However, at the intersection between AI and web3, there is currently no ideal agentic framework that can seamlessly integrate web3 applications into AI agent functionalities [10]. In this paper, we propose Eliza, the first open-source web3-friendly Agentic framework that makes the deployment of web3 applications effortless [10]. We emphasize that every aspect of Eliza is a regular Typescript program under the full control of its user, and it seamlessly integrates with web3 (i.e., reading and writing blockchain data, interacting with smart contracts, etc.) [10]. Furthermore, we show how stable performance is achieved through the pragmatic implementation of the key components of Eliza's runtime [10]. Our code is publicly available at https://github.com/ai16z/eliza [10].\\n\\nAI agents are defined as artificial entities to perceive the environment, make decisions and take actions [11]. Inspired by the 6 levels of autonomous driving by Society of Automotive Engineers, the AI agents are also categorized based on utilities and strongness, as the following levels: L0, no AI, with tools taking into account perception plus actions; L1, using rule-based AI; L2, making rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision making; L3, applying LLM-based AI instead of IL/RL-based AI, additionally setting up memory & reflection; L4, based on L3, facilitating autonomous learning & generalization; L5, based on L4, appending personality of emotion and character and collaborative behavior with multi-agents [11].\\n\\n\\n\\nAnother significant aspect of AI agent frameworks is their ability to learn, adapt, and operate autonomously in complex environments . This requires new interaction protocols, delegation strategies, and responsibility distribution frameworks. For example, the AI Agents: Evolution, Architecture, and Real-World Applications paper  examines the evolution, architecture, and practical applications of AI agents, emphasizing both theoretical foundations and real-world deployments.\\n\\nFurthermore, AI agent frameworks can be categorized based on their utilities and strength, as follows: L0, no AI, with tools taking into account perception plus actions [0]; L1, using rule-based AI [0]; L2, making rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision making [0]; L3, applying LLM-based AI instead of IL/RL-based AI, additionally setting up memory & reflection [0]; L4, based on L3, facilitating autonomous learning & generalization [0]; L5, based on L4, appending personality of emotion and character and collaborative behavior with multi-agents [0]. \\n\\nRecent advancements in AI agent implementations have focused on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities [4]. This includes the development of frameworks such as AutoAgent [1], a fully-automated and zero-code framework for LLM agents, and IntellAgent [9], a multi-agent framework for evaluating conversational AI systems. Additionally, Eliza [10] is a web3-friendly agentic framework that seamlessly integrates web3 applications into AI agent functionalities.\\n\\nIn summary, AI agent frameworks have evolved to integrate LLMs, learn, adapt, and operate autonomously in complex environments. They can be categorized based on their utilities and strength, and are being developed to tackle complex tasks such as AI research, human-AI teaming, and decision-making . Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities.\\n\\nRecent studies have also explored the integration of LLMs with multimodal perception, planning, and tool use, enabling AI agents to tackle complex tasks in various domains [7]. For example, the CATP-LLM framework [0] empowers LLMs for cost-aware tool planning, while the Unified Mind Model (UMM) [8] provides a novel theoretical cognitive architecture for creating autonomous agents with human-level cognitive abilities. These frameworks and architectures demonstrate the potential of LLMs in multimodal tasks and highlight the need for further research in this area.\\n\\nMoreover, researchers have proposed frameworks for lifelong learning of LLM-based agents, enabling them to continuously adapt in dynamic environments [5]. These frameworks incorporate techniques such as multimodal perception, planning, and tool use, as well as memory and reflection modules, to enable continuous adaptation and mitigate catastrophic forgetting. For example, CATP-LLM [0] empowers LLMs for cost-aware tool planning, while AutoAgent [1] enables users to create and deploy LLM agents through Natural Language Alone. Additionally, frameworks such as Meta MLGym [3] and MLGym-Bench [3] provide a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. Furthermore, the Unified Mind Model [8] offers guidance to facilitate the rapid creation of autonomous agents with human-level cognitive abilities. \\n\\nIn conclusion, AI agent frameworks have evolved to integrate LLMs, learn, adapt, and operate autonomously in complex environments. They can be categorized based on their utilities and strength, and are being developed to tackle complex tasks such as AI research, human-AI teaming, and decision-making . Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities.\\n\\nFurthermore, AI agent frameworks have been applied in various real-world applications, including enterprise, personal assistance, and specialized domains [4]. For instance, AI agents have been used in customer service, healthcare, and finance to provide personalized assistance and decision-making support [1]. Additionally, AI agents have been used in robotics to enhance task planning and execution [7]. Moreover, AI agents have been used in web3 applications to provide seamless integration with AI functionalities [10]. Overall, AI agents have shown great potential in various domains and continue to be an active area of research [6]. \\n\\nOverall, AI agent frameworks have made significant progress in recent years, with various architectures and capabilities being developed to tackle complex tasks [4]. The integration of LLMs, multimodal perception, planning, and tool use has enabled AI agents to learn, adapt, and operate autonomously in complex environments [4]. Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities [4]. AI agents have also been applied to various domains, including enterprise, personal assistance, and specialized domains [6]. Additionally, there is a growing interest in integrating LLMs with multimodal perception and planning to enable AI agents to interact with complex environments [7]. Furthermore, researchers have proposed frameworks such as AutoAgent [1] and Eliza [10] to enable the development of AI agents without programming expertise. Overall, AI agent frameworks have made significant progress in recent years, with various architectures and capabilities being developed to tackle complex tasks [4]. The integration of LLMs, multimodal perception, planning, and tool use has enabled AI agents to learn, adapt, and operate autonomously in complex environments [4]. Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities [4]. AI agents have also been applied to various domains, including enterprise, personal assistance, and specialized domains [6]. Additionally, there is a growing interest in integrating LLMs with multimodal perception and planning to enable AI agents to interact with complex environments [7]. Furthermore, researchers have proposed frameworks such as AutoAgent [1] and Eliza [10] to enable the development of AI agents without programming expertise. AI agents have also been categorized into levels based on their capabilities, with L0 being no AI, L1 using rule-based AI, L2 making rule-based AI replaced by IL/RL-based AI, L3 applying LLM-based AI, L4 based on L3 facilitating autonomous learning and generalization, and L5 based on L4 appending personality and emotion and collaborative behavior with multi-agents [11]. Overall, AI agent frameworks have made significant progress in recent years, with various architectures and capabilities being developed to tackle complex tasks [4]. The integration of LLMs, multimodal perception, planning, and tool use has enabled AI agents to learn, adapt, and operate autonomously in complex environments [4]. Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities [4]. AI agents have also been applied to various domains, including enterprise, personal assistance, and specialized domains [6]. Additionally, there is a growing interest in integrating LLMs with multimodal perception and planning to enable AI agents to interact with complex environments [7]. Furthermore, researchers have proposed frameworks such as AutoAgent [1] and Eliza [10] to enable the development of AI agents without programming expertise. AI agents have also been categorized into levels based on their capabilities, with L0 being no AI, L1 using rule-based AI, L2 making rule-based AI replaced by IL/RL-based AI, L3 applying LLM-based AI, L4 based on L3 facilitating autonomous learning and generalization, and L5 based on L4 appending personality and emotion and collaborative behavior with multi-agents [11]. Overall, AI agent frameworks have made significant progress in recent years, with various architectures and capabilities being developed to tackle complex tasks [4]. The integration of LLMs, multimodal perception, planning, and tool use has enabled AI agents to learn, adapt, and operate autonomously in complex environments [4]. Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities [4]. AI agents have also been applied to various domains, including enterprise, personal assistance, and specialized domains [6]. Additionally, there is a growing interest in integrating LLMs with multimodal perception and planning to enable AI agents to interact with complex environments [7]. Furthermore, researchers have proposed frameworks such as AutoAgent [1] and Eliza [10] to enable the development of AI agents without programming expertise. AI agents have also been categorized into levels based on their capabilities, with L0 being no AI, L1 using rule-based AI, L2 making rule-based AI replaced by IL/RL-based AI, L3 applying LLM-based AI, L4 based on L3 facilitating autonomous learning and generalization, and L5 based on L4 appending personality and emotion and collaborative behavior with multi-agents [11]. Overall, AI agent frameworks have made significant progress in recent years, with various architectures and capabilities being developed to tackle complex tasks [4]. The integration of LLMs, multimodal perception, planning, and tool use has enabled AI agents to learn, adapt, and operate autonomously in complex environments [4]. Recent advancements in AI agent implementations have focused on their ability to achieve complex goals, and new frameworks have been developed to support these capabilities [4]. AI agents have also been applied to various domains, including enterprise, personal assistance, and specialized domains [6]. Additionally, there is a growing interest in integrating LLMs with multimodal perception and planning to enable AI agents to interact with complex environments [7]. Furthermore, researchers have proposed frameworks such as AutoAgent [1] and Eliza [10] to enable the development of AI agents without programming expertise. AI agents have also been categorized into levels based on their capabilities, with L0 being no AI, L1 using rule-based AI, L2 making rule-based AI replaced by IL/RL-based AI, L3 applying LLM-based AI, L4 based on L3 facilitating autonomous learning and generalization, and L5 based on L4 appending personality and emotion and collaborative behavior with multi-agents [11]. \",\n        \"main_figure_data\": \"\",\n        \"main_figure_caption\": \"\",\n        \"reportIndexList\": [\n            {\n                \"title\": \"CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_section_reflection_inp",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_section_reflection_inp = {\n    \"query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n    \"title\": \"Recent Advances in Natural Language Processing\",\n    \"section_name\": \"Definition and scope of NLP.\",\n    \"section_index\": 0,\n    \"parent_section\": \"Introduction\",\n    \"section_key_point\": \"Definition and scope of NLP.\",\n    \"section_text\": \"Natural Language Processing (NLP) is a field of computer science and artificial intelligence that deals with the interaction between computers and humans in natural language . It is a multidisciplinary field that combines computer science, linguistics, and cognitive psychology to enable computers to understand, interpret, and generate human language . NLP has a wide range of applications, including machine translation, sentiment analysis, information extraction, summarization, and question answering . NLP has been applied in various domains, including healthcare , finance , and customer service, to analyze and understand human language and generate human-like responses .\\n\\nThe scope of NLP is broad and includes various tasks such as language modeling, text classification, named entity recognition, and machine translation . NLP has been used in healthcare for clinical notes analysis, diagnosis coding, and automated text classification . In finance, NLP is used for text classification, sentiment analysis, and opinion analysis . NLP has also been applied in customer service for chatbots, content generation, and language translation .\\n\\nAccording to academic sources, NLP has a significant impact on various fields, including natural language understanding, natural language generation, and human-computer interaction . NLP can be used to analyze sentiment and emotions expressed on social media, which can be useful in healthcare and clinical analytics . NLP also has the potential to improve reviewing processes, such as peer review, by assisting in tasks such as manuscript submission, review, and revision .\\n\\nHowever, NLP also has limitations and challenges, such as data acquisition and licensing, operationalization and experimentation, and ethical issues . Furthermore, NLP relies on linguistics in various aspects, including resources, evaluation, low-resource settings, interpretability, explanation, and the study of language .\\nNLP has been used to explain and interpret complex data, such as medical data, and has been applied in various domains, including healthcare, finance, and customer service . Explainable NLP (XNLP) is a crucial aspect of NLP, as it provides transparency and trustworthiness in decision-making processes .\\nReinforcement learning (RL) has emerged as a powerful approach for tackling complex medical decision-making problems, such as treatment planning, personalized medicine, and optimizing the scheduling of surgeries and appointments . RL has gained significant attention in the field of NLP due to its ability to learn optimal strategies for tasks such as dialogue systems, machine translation, and question-answering .\\nLarge language models, such as ChatGPT, have been widely adopted and have found diverse applications, including chatbots, content generation, language translation, recommendations, and medical applications . ChatGPT excels in generating human-like responses, comprehending natural language, and adapting contextually .\\n\",\n    \"main_figure_data\": \"\",\n    \"main_figure_caption\": \"\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_section_reflection_out",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_section_reflection_out = {\n    \"section_name\": \"Definition and overview of AI agent frameworks\",\n    \"section_content\": {\n        \"search_query\": \"What is the definition of AI agent frameworks, and how does LangGraph fit into this definition?\",\n        \"section_key_point\": \"Definition and overview of AI agent frameworks\",\n        \"section_text\": \"An AI agent framework is a software architecture that enables the development and deployment of autonomous agents that can interact with their environment and make decisions based on their perception and goals [2]. These frameworks typically provide a set of tools and components that allow developers to build, train, and manage AI agents, including natural language processing, machine learning, and decision-making capabilities [0][1][3][4][5][6].\\n\\nLangGraph is a modular framework for enhancing machine translation using large language models (LLMs) [6]. It is designed to simplify the creation and management of agents and their workflows, enabling efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention [4]. LangGraph integrates with LLMs, such as GPT-4o, to ensure accurate, contextually relevant translations while maintaining modularity, scalability, and context retention [6]. It also supports dynamic state management, enabling agents to maintain dialogue context and automates complex workflows by linking agents and facilitating their collaboration [6]. LangGraph is a graph-based framework built on LangChain, which simplifies the creation and management of these agents and their workflows [6]. \\n\\nLangGraph can be seen as an AI agent framework because it provides a set of tools and components that allow developers to build, train, and manage AI agents, including natural language processing, machine learning, and decision-making capabilities . It enables the creation of modular agents that can interact with their environment and make decisions based on their perception and goals, and it supports dynamic state management and workflow orchestration . LangGraph's ability to integrate with LLMs and other components, such as Spark, makes it a versatile and powerful tool for building AI agents .\\n\\nLangGraph's graph-based architecture enables modular and scalable AI agents. The framework's use of graph-structured workflows allows for the efficient creation and modification of tools, agents, and workflows without coding requirements or manual intervention . This modularity and scalability make LangGraph a valuable tool for building AI agents that can interact with their environment and make decisions based on their perception and goals .\\n\\nLangGraph has been applied in various areas, including dialogue systems and decision-making tasks [0][1][2][3][4][5][6]. For example, it has been used in multi-agent collaborative intelligence for adaptive reasoning and temporal planning [0], and it has been integrated with authenticated delegation and authorized AI agents for secure task delegation [1]. Additionally, LangGraph has been used in agentic retrieval-augmented generation for complex task management and multistep reasoning [2]. Its applications in these areas demonstrate its potential for building AI agents that can interact with their environment and make decisions based on their perception and goals [2]. LangGraph has also been applied in machine translation, where it has been used to enhance the automation and effectiveness of translation tasks [6]. In this context, LangGraph has been used to create modular agents that can perform specific translation tasks, such as translating between particular languages [6]. These agents leverage the powerful semantic capabilities of large language models to ensure accurate and contextually relevant translations [6]. Overall, LangGraph has been shown to be a versatile and effective tool for building AI agents that can interact with their environment and perform a wide range of tasks [2][3][4][5][6]. \\n\\nLangGraph's integration with Spark and other components, such as Kafka, enables high-performance execution and efficient handling of large-scale data streams [3]. The framework's ability to support real-time state streaming, debugging via LangGraph Studio, and efficient handling of large-scale data streams make it ideal for adaptive decision-making [3]. Experimental results confirm the system's ability to classify inquiries, detect sentiment trends, and escalate complex issues for manual review, demonstrating a synergistic blend of LLM capabilities and human oversight [3].\\n\\nLangGraph's graph-based architecture also enables the creation of scalable and user-friendly AI solutions. The framework's use of graph-structured workflows allows for the efficient creation and modification of tools, agents, and workflows without coding requirements or manual intervention . This modularity and scalability make LangGraph a valuable tool for building AI agents that can interact with their environment and make decisions based on their perception and goals .\\n\\nIn summary, LangGraph is a modular framework for enhancing machine translation using large language models (LLMs) [6]. It provides a set of tools and components that allow developers to build, train, and manage AI agents, including natural language processing, machine learning, and decision-making capabilities [6]. LangGraph's graph-based architecture enables modular and scalable AI agents, and its integration with Spark and other components enables high-performance execution and efficient handling of large-scale data streams [6]. Its applications in various areas demonstrate its potential for building AI agents that can interact with their environment and make decisions based on their perception and goals [6]. \",\n        \"main_figure_data\": \"\",\n        \"main_figure_caption\": \"\",\n        \"reportIndexList\": [\n            {",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_global_reflection_in",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_global_reflection_in = {\n    \"paper_title\": \"Recent Advances in Natural Language Processing\",\n    \"user_query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n    \"outline\": {\n        \"Introduction\": {\n            \"section_index\": 0,\n            \"section_title\": \"Introduction\",\n            \"key_points\": [\n                \"Definition and scope of NLP.\",\n                \"Key components of NLP: syntax, semantics, and pragmatics.\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_global_reflection_in",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_global_reflection_in = {'paper_title': 'AI Agent Frameworks: Capabilities and Use Cases of LangGraph', 'user_query': 'Give an overview of capabilities and use case these AI agent Frameworks: LangGraph', 'outline': {'title': 'AI Agent Frameworks: Capabilities and Use Cases of LangGraph', 'sections': {'Introduction': {'section_index': 0, 'section_title': 'Introduction', 'is_conclusion': False, 'key_points': ['Definition and evolution of AI agent frameworks.', 'Comparison of popular AI agent frameworks in terms of capabilities and applications.', 'Key components and functionalities of AI agent frameworks.', \"Overview of LangGraph's architecture and design principles.\", 'Key features that differentiate LangGraph from other AI agent frameworks.', 'Development history and key contributors to LangGraph.'], 'subsection_info': {'Definition and evolution of AI agent frameworks.': {'section_index': 0, 'section_title': 'Definition and evolution of AI agent frameworks.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Comparison of popular AI agent frameworks in terms of capabilities and applications.': {'section_index': 1, 'section_title': 'Comparison of popular AI agent frameworks in terms of capabilities and applications.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Key components and functionalities of AI agent frameworks.': {'section_index': 2, 'section_title': 'Key components and functionalities of AI agent frameworks.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Introduction to LangGraph': {'section_index': 1, 'section_title': 'Introduction to LangGraph', 'is_conclusion': False, 'key_points': [\"Overview of LangGraph's architecture and design principles.\", 'Key features that differentiate LangGraph from other AI agent frameworks.', 'Development history and key contributors to LangGraph.'], 'subsection_info': {}}}}, 'Core Capabilities of LangGraph': {'section_index': 1, 'section_title': 'Core Capabilities of LangGraph', 'is_conclusion': False, 'key_points': ['Handling of complex linguistic structures and context.', 'Support for multiple languages and dialects.', 'Tokenization, parsing, and semantic analysis techniques.', 'Methods for integrating external knowledge graphs and databases.', 'Impact of knowledge integration on language understanding and generation.', 'Techniques for real-time knowledge retrieval and updating.', 'Dialogue management and state tracking mechanisms.', 'Handling of user intent and emotion in conversations.', 'Natural language generation for coherent and contextually relevant responses.'], 'subsection_info': {'Handling of complex linguistic structures and context.': {'section_index': 0, 'section_title': 'Handling of complex linguistic structures and context.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Support for multiple languages and dialects.': {'section_index': 1, 'section_title': 'Support for multiple languages and dialects.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Tokenization, parsing, and semantic analysis techniques.': {'section_index': 2, 'section_title': 'Tokenization, parsing, and semantic analysis techniques.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Knowledge Integration': {'section_index': 1, 'section_title': 'Knowledge Integration', 'is_conclusion': False, 'key_points': ['Methods for integrating external knowledge graphs and databases.', 'Impact of knowledge integration on language understanding and generation.', 'Techniques for real-time knowledge retrieval and updating.'], 'subsection_info': {}}, 'Conversational Agents': {'section_index': 2, 'section_title': 'Conversational Agents', 'is_conclusion': False, 'key_points': ['Dialogue management and state tracking mechanisms.', 'Handling of user intent and emotion in conversations.', 'Natural language generation for coherent and contextually relevant responses.'], 'subsection_info': {}}}}, 'Use Cases of LangGraph': {'section_index': 2, 'section_title': 'Use Cases of LangGraph', 'is_conclusion': False, 'key_points': ['Use in chatbots for customer support.', 'Application in virtual assistants for personal and professional tasks.', 'Integration with IoT devices for voice-controlled interactions.', 'Development of chatbots for mental health support.', 'Building conversational interfaces for smart home devices.', 'Creation of virtual assistants for financial advisory services.', 'Enhancement of existing knowledge graphs with new information.', 'Automated construction of knowledge graphs from textual data.', 'Use in semantic search engines and recommendation systems.'], 'subsection_info': {'Use in chatbots for customer support.': {'section_index': 0, 'section_title': 'Use in chatbots for customer support.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Application in virtual assistants for personal and professional tasks.': {'section_index': 1, 'section_title': 'Application in virtual assistants for personal and professional tasks.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Integration with IoT devices for voice-controlled interactions.': {'section_index': 2, 'section_title': 'Integration with IoT devices for voice-controlled interactions.', 'is_conclusion': False, 'key_points': [], 'subsection_info': {}}, 'Conversational Agents': {'section_index': 1, 'section_title': 'Conversational Agents', 'is_conclusion': False, 'key_points': ['Development of chatbots for mental health support.', 'Building conversational interfaces for smart home devices.', 'Creation of virtual assistants for financial advisory services.'], 'subsection_info': {}}, 'Knowledge Graph Construction': {'section_index': 2, 'section_title': 'Knowledge Graph Construction', 'is_conclusion': False, 'key_points': ['Enhancement of existing knowledge graphs with new information.', 'Automated construction of knowledge graphs from textual data.', 'Use in semantic search engines and recommendation systems.'], 'subsection_info': {}}}}, 'Challenges and Future Directions': {'section_index': 3, 'section_title': 'Challenges and Future Directions', 'is_conclusion': True, 'key_points': [], 'subsection_info': {}}}}, 'sections_content': {'Introduction': [{'section_index': 0, 'parent_section': 'Introduction', 'search_query': 'How has the integration of large language models impacted the capabilities of AI agent frameworks?', 'section_point': 'Definition and evolution of AI agent frameworks.', 'section_text': 'The integration of large language models (LLMs) has significantly impacted the capabilities of AI agent frameworks, enabling them to perform diverse tasks across various domains [0]. LLMs have transformed AI by providing human-like text comprehension and generation capabilities, making them proficient in tasks such as customer service, healthcare, and more [0]. The integration of LLMs has led to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives [0]. LLMs have enabled AI agents to reason, generate text, and perform complex tasks that were previously challenging for traditional AI systems [3]. They have also improved the performance of recommender systems and search engines by providing more accurate and personalized recommendations [4]. Moreover, LLMs have been applied to various domains, including agent-based modeling and simulation [0], professional services [1], and information retrieval [4]. Furthermore, LLMs have been used to create Professional Agents (PAgents), an application framework that harnesses LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies [1]. Additionally, LLMs have been integrated into multi-agent systems, enabling them to collaborate and communicate effectively [2]. However, the integration of LLMs also poses challenges, such as multimodality, human value alignment, hallucinations, and evaluation [3]. Techniques like prompting, reasoning, tool utilization, and in-context learning are being explored to enhance the capabilities of LLM-based autonomous agents [3]. In summary, the integration of LLMs has revolutionized AI agent frameworks, enabling them to perform diverse tasks, but also raises concerns about safety and effectiveness [3]. ', 'section_summary': ['The integration of large language models (LLMs) has significantly enhanced the capabilities of AI agent frameworks, enabling them to perform diverse tasks across various domains, improve the performance of recommender systems, and facilitate collaboration in multi-agent systems. Their application has led to the creation of Professional Agents, autonomous agents with specialized competencies. Despite these advancements, the integration of LLMs presents challenges related to multimodality, human value alignment, and evaluation, necessitating further exploration and enhancement techniques.'], 'main_figure_data': '', 'main_figure_caption': '', 'reportIndexList': [{'title': 'Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives', 'authors': 'Chen Gao;Xiaochong Lan;Nian Li;Yuan Yuan;Jingtao Ding;Zhilun Zhou;Fengli Xu;Yong Li', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2312.11970v1'}, {'title': 'Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies', 'authors': 'Zhixuan Chu;Yan Wang;Feng Zhu;Lu Yu;Longfei Li;Jinjie Gu', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2402.03628v1'}, {'title': 'Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things', 'authors': 'Talha Zeeshan;Abhishek Kumar;Susanna Pirttikangas;Sasu Tarkoma', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2501.00906v2'}, {'title': 'Exploring Autonomous Agents through the Lens of Large Language Models: A Review', 'authors': 'Saikat Barua', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2404.04442v1'}, {'title': 'A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval', 'authors': 'Yu Zhang;Shutong Qiao;Jiaqi Zhang;Tzu-Heng Lin;Chen Gao;Yong Li', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2503.05659v2'}, {'title': 'Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies', 'authors': 'Zhixuan Chu;Yan Wang;Feng Zhu;Lu Yu;Longfei Li;Jinjie Gu', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2402.03628v1'}, {'title': 'A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval', 'authors': 'Yu Zhang;Shutong Qiao;Jiaqi Zhang;Tzu-Heng Lin;Chen Gao;Yong Li', 'conference': '', 'source': 'Search From Arxiv', 'url': 'http://arxiv.org/abs/2503.05659v2'}]}]}, 'rag_service_url': 'http://120.92.91.62:9528/chat', 'max_iterations': 1}\nexample_global_reflection_out = {\n    \"paper_title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"meets_requirements\": False,\n    \"final_feedback\": \"OVERALL ASSESSMENT: Paper needs improvement. Key issues identified: The paper has several areas that need improvement to meet high academic standards.; The paper lacks completeness and details.\",\n    \"sections_content\": {\n        \"Introduction\": [\n            {\n                \"parent_section\": \"Introduction\",\n                \"search_query\": \"LangGraph in AI agent frameworks: its significance, importance of language-based graph processing, and supporting scholarly articles\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_global_reflection_out",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_global_reflection_out = {\n    \"paper_title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"meets_requirements\": False,\n    \"final_feedback\": \"OVERALL ASSESSMENT: Paper needs improvement. Key issues identified: The paper has several areas that need improvement to meet high academic standards.; The paper lacks completeness and details.\",\n    \"sections_content\": {\n        \"Introduction\": [\n            {\n                \"parent_section\": \"Introduction\",\n                \"search_query\": \"LangGraph in AI agent frameworks: its significance, importance of language-based graph processing, and supporting scholarly articles\",\n                \"section_point\": \"Definition and overview of AI agent frameworks\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_abstract_conclusion_inp",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_abstract_conclusion_inp = {\n    \"paper_title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"user_query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n    \"outline\": {\n        \"user_query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n        \"title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n        \"abstract\": \"This survey paper provides a comprehensive overview of LangGraph, an AI agent framework designed to facilitate the development of language-based graph processing and reasoning agents. We explore the core capabilities of LangGraph, including its support for natural language processing (NLP), graph data structures, and reasoning algorithms. Additionally, we delve into various use cases and applications of LangGraph, highlighting its versatility and effectiveness in different domains. The paper also discusses the challenges and future directions for the framework, aiming to provide researchers and practitioners with a clear understanding of its potential and limitations.\",\n        \"research_field\": {\n            \"field\": \"Computer Science\",\n            \"paper_type\": \"survey\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_abstract_conclusion_out",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_abstract_conclusion_out = {\n    \"Abstract\": \"This paper critically examines LangGraph, a cutting-edge artificial intelligence (AI) agent framework, with a specific focus on its capabilities and use cases. LangGraph is founded on language-based graph processing which enhances its relevance in various computing applications. A thorough exploration of its key functionalities such as natural language query handling, text parsing, semantic analysis, support for dynamic graphs, and integration with notable NLP models such as spaCy and BERT is undertaken. Additionally, efficient graph traversal and manipulation algorithms as well as optimization techniques pertinent to large-scale graphs are substantiated with benchmarks and performance metrics.\\n\\nThe research then delves into the implementations of LangGraph in arenas such as semantic web, information retrieval, recommendation systems, and network security. Illustrative cases exhibit its proficiency in the automated construction of knowledge graphs, detection of malicious network activities, and deriving personalized recommendations based on graph data. Furthermore, LangGraph's potential in handling user feedback, behavior analysis, and identifying influential nodes in social networks is elaborated.\\n\\nHowever, the research also acknowledges the existence of certain challenges and limitations pertaining to LangGraph. These encompass scalability issues with colossal graphs, privacy and security dilemmas, complexity aspects of natural language understanding, and constrains related to integrating with established systems and data formats. As a contribution to future works, possible directions and improvements for these AI agent frameworks are postulated. This study invariably upholds the considerable potential of LangGraph and similar AI agent frameworks in transforming numerous technological and business processes.\",\n    \"Conclusion\": \"In conclusion, this paper scrutinizes the capabilities and use cases of AI agent framework, in particular, LangGraph. The research identified significant functionalities of LangGraph, such as handling natural language queries, support for dynamic graphs, efficient graph traversal, and incorporation with notable NLP models like spaCy and BERT. The amalgamation of these capabilities substantiates LangGraph's proficiency in various domains, such as semantic web, information retrieval, recommendation systems, and network security. The added ability to identify malicious network activities and derive personalized recommendations is also noteworthy.\\n\\nBacktracking to the aim of this research, it's clear that LangGraph and similar AI agent frameworks hold transformative potential for many technological and business processes. Cases validating this included its implementation in automated knowledge graph construction, behavior analysis, and identification of influential nodes in social networks, amongst others.\\n\\nNonetheless, it's important to remember this advancement is not without challenges. The research highlighted certain limitations, including scalability issues with larger graphs, complexities in natural language understanding, and intricate integration with current systems and data formats. Added to these are the associated privacy and security concerns requiring careful management and mitigation. \\n\\nThese limitations indicate potential directions for future research. To fortify the utility of LangGraph and similar frameworks, it is paramount to focus on these areas and foster improvements. Future studies can explore the optimization of large-scale graph processing, advancements in natural language understanding, and techniques to enhance system integration seamlessly. \\n\\nTo encapsulate, the significance of LangGraph and other such AI agent frameworks cannot be overstated. They offer a new paradigm for handling complex data streams and can potentially revolutionize numerous processes, notwithstanding the challenges that remain. In moving forward, it is hoped that relentless research will continue to leverage their capabilities and consolidate their place in our data-driven world.\",\n    \"meets_requirements\": False,\n    \"reflection_count\": 3,\n}\nexample_poolish_inp = {\n    \"paper_title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"user_query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n    \"outline\": {",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_poolish_inp",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_poolish_inp = {\n    \"paper_title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"user_query\": \"Give an overview of capabilities and use case these AI agent Frameworks: LangGraph\",\n    \"outline\": {\n        \"Introduction\": {\n            \"section_index\": 0,\n            \"section_title\": \"Introduction\",\n            \"key_points\": [\n                \"Definition and overview of AI agent frameworks\",\n                \"Overview of LangGraph and its significance in the field\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_abstract_info_for_poolish",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_abstract_info_for_poolish = {'Abstract': \"This paper critically examines the capabilities and use cases of LangGraph, an AI agent framework. The researchers delve into the historical context, evolution, and the integral role of AI agent frameworks, setting the stage for the in-depth analysis of LangGraph. The paper presents a detailed comparison of LangGraph with other prominent AI agent frameworks, highlighting the significance of graph-based representations in AI.\\n\\nThe authors delve deeply into the theoretical foundations of LangGraph, exploring graph algorithms and their relevance to AI. Fundamental concepts of graph theory and the types of graphs and their properties are examined. This exploration extends to the architecture of LangGraph, its components, and modules.\\n\\nThe paper further explores the technical implementation and capabilities of LangGraph, from data structures and algorithms to the programming languages and libraries used in its construction. The study emphasizes LangGraph's ability to enhance natural language understanding and generation, and its potential for graph-based knowledge representation and reasoning. A series of case studies showcase LangGraph's effectiveness, with performance tests and comparisons providing a comprehensive evaluation of the framework.\\n\\nThe practical applications and use cases of LangGraph are also investigated, ranging from personalized treatment recommendations to algorithmic trading and investment analysis. The study identifies potential challenges, including model training and deployment complexity, data privacy, security concerns, and scalability issues in large-scale applications. The paper concludes with a discussion on the future directions of LangGraph. This research provides valuable insights into the capabilities, use cases, and potential of LangGraph as an AI agent framework.\", 'Conclusion': \"In conclusion, this study provides an in-depth examination of the capabilities and use cases of LangGraph, an AI agent framework. The research started by setting the stage with a historical perspective, defining AI agent frameworks, and comparing LangGraph with other prominent frameworks. The importance of graph-based representations in AI and the unique features and design principles of LangGraph were emphasized, aligning with the research objectives.\\n\\nThe theoretical foundations of LangGraph were explored, illuminating the relevance of graph algorithms to AI, fundamental concepts of graph theory, and the types of graphs and their properties. The study also delved into the architecture and components of LangGraph, highlighting the interplay between language models and graph structures. These findings pertain directly to the research questions concerning the theoretical underpinnings of LangGraph and its architecture.\\n\\nThe technical implementation and capabilities of LangGraph were thoroughly investigated, including the data structures and algorithms implemented, programming languages and libraries used, and considerations for scalability and efficiency. The framework's capacities for natural language understanding and generation, and graph-based knowledge representation and reasoning were underscored, alongside its potential for integration with other AI systems and tools. A series of case studies and performance tests offered a comprehensive evaluation of LangGraph, contributing to the research objectives regarding LangGraph's capabilities.\\n\\nThe application and use cases of LangGraph were also discussed, ranging from healthcare to finance and education. Despite the numerous potential use cases, the study acknowledges the complexity in model training and deployment, data privacy and security concerns, and scalability issues in large-scale applications as limitations of the current work. These challenges also suggest avenues for future research, particularly in enhancing LangGraph's scalability and addressing security concerns.\\n\\nThis research adds to the growing body of knowledge on AI agent frameworks, specifically the graph-based LangGraph. It provides a comprehensive foundation for further exploration and development within this domain, demonstrating the framework's potential for substantial contributions to various sectors. The study underscores the importance of continuous research in AI technologies and frameworks, as they hold immense potential for shaping our future society and economy.\", 'meets_requirements': False, 'reflection_count': 3}\nexample_section_info_for_poolish = {\n    \"Introduction\": [\n        {\n            \"section_index\": 0,\n            \"parent_section\": \"Introduction\",\n            \"search_query\": \"langgraph ai agent frameworks comparison prominent alternatives\",\n            \"section_point\": \"Comparison with other prominent AI agent frameworks.\",\n            \"section_text\": \"The emergence of Large Language Model (LLM) agents has revolutionized the field of artificial intelligence, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments . However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions . To address this challenge, several AI agent frameworks have been proposed.\\n\\nOne such framework is IntellAgent , a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations . This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics .\\n\\nAnother framework is AutoAgent , a fully-automated and zero-code framework for LLM agents that enables users to create and deploy LLM agents through natural language alone [4]. AutoAgent comprises four key components: agentic system utilities, LLM-powered actionable engine, self-managing file system, and self-play agent customization module [4]. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention [4]. AutoAgent also serves as a versatile multi-agent system for General AI Assistants and has shown consistently superior performance compared to many alternative LLM-based solutions [4].\\n\\nIn addition, Eliza is a web3-friendly AI agent operating system that makes the deployment of web3 applications effortless and seamlessly integrates with web3 [1]. Eliza emphasizes that every aspect of it is a regular Typescript program under the full control of its user and provides stable performance through the pragmatic implementation of its key components [1].\\n\\nFurthermore, the design and evaluation of multi-agent collaboration frameworks, such as the one presented in [3], have emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents [3]. This report addresses the challenges of designing the collaboration protocols and evaluating the effectiveness of these systems by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework [3]. The framework demonstrates the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to-end goal success rates of 90% [3]. \\n\\nIn summary, several AI agent frameworks have been proposed to address the challenges of evaluating and developing LLM agents, including IntellAgent [2], AutoAgent [4], and Eliza [1]. These frameworks provide innovative approaches to evaluating conversational AI systems, enabling users to create and deploy LLM agents through natural language alone, and making the deployment of web3 applications effortless [1]. Furthermore, the design and evaluation of multi-agent collaboration frameworks have emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents [3].\\n\\nThe comparison of these frameworks highlights the importance of considering the specific needs and requirements of each application domain when selecting an AI agent framework [0]. For instance, IntellAgent is well-suited for evaluating conversational AI systems [2], while AutoAgent is ideal for creating and deploying LLM agents through natural language alone [4]. Eliza, on the other hand, is designed for web3 applications [1]. By understanding the strengths and weaknesses of each framework, developers can make informed decisions and choose the most appropriate tool for their specific use case [0]. \\n\\nThe emergence of AI agent frameworks has revolutionized the field of artificial intelligence, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments [0]. These frameworks provide innovative approaches to evaluating conversational AI systems, enabling users to create and deploy LLM agents through natural language alone [4], and making the deployment of web3 applications effortless [1]. By understanding the strengths and weaknesses of each framework, developers can make informed decisions and choose the most appropriate tool for their specific use case [0].\",\n            \"section_summary\": \"\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_section_info_for_poolish",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_section_info_for_poolish = {\n    \"Introduction\": [\n        {\n            \"section_index\": 0,\n            \"parent_section\": \"Introduction\",\n            \"search_query\": \"langgraph ai agent frameworks comparison prominent alternatives\",\n            \"section_point\": \"Comparison with other prominent AI agent frameworks.\",\n            \"section_text\": \"The emergence of Large Language Model (LLM) agents has revolutionized the field of artificial intelligence, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments . However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions . To address this challenge, several AI agent frameworks have been proposed.\\n\\nOne such framework is IntellAgent , a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations . This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics .\\n\\nAnother framework is AutoAgent , a fully-automated and zero-code framework for LLM agents that enables users to create and deploy LLM agents through natural language alone [4]. AutoAgent comprises four key components: agentic system utilities, LLM-powered actionable engine, self-managing file system, and self-play agent customization module [4]. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention [4]. AutoAgent also serves as a versatile multi-agent system for General AI Assistants and has shown consistently superior performance compared to many alternative LLM-based solutions [4].\\n\\nIn addition, Eliza is a web3-friendly AI agent operating system that makes the deployment of web3 applications effortless and seamlessly integrates with web3 [1]. Eliza emphasizes that every aspect of it is a regular Typescript program under the full control of its user and provides stable performance through the pragmatic implementation of its key components [1].\\n\\nFurthermore, the design and evaluation of multi-agent collaboration frameworks, such as the one presented in [3], have emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents [3]. This report addresses the challenges of designing the collaboration protocols and evaluating the effectiveness of these systems by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework [3]. The framework demonstrates the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to-end goal success rates of 90% [3]. \\n\\nIn summary, several AI agent frameworks have been proposed to address the challenges of evaluating and developing LLM agents, including IntellAgent [2], AutoAgent [4], and Eliza [1]. These frameworks provide innovative approaches to evaluating conversational AI systems, enabling users to create and deploy LLM agents through natural language alone, and making the deployment of web3 applications effortless [1]. Furthermore, the design and evaluation of multi-agent collaboration frameworks have emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents [3].\\n\\nThe comparison of these frameworks highlights the importance of considering the specific needs and requirements of each application domain when selecting an AI agent framework [0]. For instance, IntellAgent is well-suited for evaluating conversational AI systems [2], while AutoAgent is ideal for creating and deploying LLM agents through natural language alone [4]. Eliza, on the other hand, is designed for web3 applications [1]. By understanding the strengths and weaknesses of each framework, developers can make informed decisions and choose the most appropriate tool for their specific use case [0]. \\n\\nThe emergence of AI agent frameworks has revolutionized the field of artificial intelligence, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments [0]. These frameworks provide innovative approaches to evaluating conversational AI systems, enabling users to create and deploy LLM agents through natural language alone [4], and making the deployment of web3 applications effortless [1]. By understanding the strengths and weaknesses of each framework, developers can make informed decisions and choose the most appropriate tool for their specific use case [0].\",\n            \"section_summary\": \"\",\n            \"main_figure_data\": \"\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "example_outline_for_poolish",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "example_outline_for_poolish = {\n    \"title\": \"AI Agent Frameworks: Capabilities and Use Cases of LangGraph\",\n    \"sections\": {\n        \"Introduction\": {\n            \"section_index\": 0,\n            \"section_title\": \"Introduction\",\n            \"is_conclusion\": False,\n            \"key_points\": [\n                \"Comparison with other prominent AI agent frameworks.\",\n                \"Definition and role of AI agent frameworks.\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "global_reflection_eval_paper_exmaples_input",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "global_reflection_eval_paper_exmaples_input = {\n    \"paper_title\": \"Current Research on Lifelong Learning Machines (LLM) Agents: A Comprehensive Review and Exploration\",\n    \"user_query\": \"current research on LLM Agent\",\n    \"outline\": {\n        \"sections\": {\n            \"Introduction\": {\n                \"section_index\": 1,\n                \"key_points\": [\n                    \"Overview of LLM agents\",\n                    \"Importance of lifelong learning\",",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "paper_gen_abatract_conclusion_exmaples_input",
        "kind": 5,
        "importPath": "core.example",
        "description": "core.example",
        "peekOfCode": "paper_gen_abatract_conclusion_exmaples_input = (\n    global_reflection_eval_paper_exmaples_input\n)",
        "detail": "core.example",
        "documentation": {}
    },
    {
        "label": "create_reflection_fallback",
        "kind": 2,
        "importPath": "core.fallback",
        "description": "core.fallback",
        "peekOfCode": "def create_reflection_fallback(\n    params: Dict[str, Any], error_message: str\n) -> Dict[str, Any]:\n    \"\"\"Create fallback response for section reflection failures\"\"\"\n    return {\n        \"section_content\": {\n            \"parent_section\": params.get(\"parent_section\", \"\"),\n            \"section_text\": params.get(\"section_text\", \"\"),\n            \"section_key_point\": params.get(\"section_key_point\", \"\"),\n            \"section_summary\": f\"[Summary could not be generated due to reflection error: {error_message}]\",",
        "detail": "core.fallback",
        "documentation": {}
    },
    {
        "label": "ReportRequest",
        "kind": 6,
        "importPath": "core.graphapi",
        "description": "core.graphapi",
        "peekOfCode": "class ReportRequest(BaseModel):\n    topic: str\n    request_id: str\nclass ReportResponse(BaseModel):\n    #request_id: str\n    final_report: str\n    #generated_at: datetime\n# \nasync def process_report(topic: str, request_id: str) -> Dict[str, Any]:\n    \"\"\"",
        "detail": "core.graphapi",
        "documentation": {}
    },
    {
        "label": "ReportResponse",
        "kind": 6,
        "importPath": "core.graphapi",
        "description": "core.graphapi",
        "peekOfCode": "class ReportResponse(BaseModel):\n    #request_id: str\n    final_report: str\n    #generated_at: datetime\n# \nasync def process_report(topic: str, request_id: str) -> Dict[str, Any]:\n    \"\"\"\n    Process report generation using LangGraph service.\n    \"\"\"\n    try:",
        "detail": "core.graphapi",
        "documentation": {}
    },
    {
        "label": "generate_report_endpoint",
        "kind": 2,
        "importPath": "core.graphapi",
        "description": "core.graphapi",
        "peekOfCode": "def generate_report_endpoint(request: ReportRequest):\n    \"\"\"\n    FastAPI  LangGraphService  run_sync\n    \"\"\"\n    service = LangGraphService()\n    logger.info(request)\n    try:\n        # \n        final_report = service.run_sync(request.topic)\n        print(\"\")",
        "detail": "core.graphapi",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.graphapi",
        "description": "core.graphapi",
        "peekOfCode": "logger = logging.getLogger(__name__)\n#  FastAPI \napp = FastAPI()\napp.add_middleware(\n    CORSMiddleware,\n    # \n    allow_origins=[\"http://localhost:63342\"],  #  [\"*\"] \n    allow_credentials=True,    # cookie\n    allow_methods=[\"*\"],      # HTTP [\"POST\", \"OPTIONS\"]\n    allow_headers=[\"*\"],      # HTTP [\"Content-Type\"]",
        "detail": "core.graphapi",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "core.graphapi",
        "description": "core.graphapi",
        "peekOfCode": "app = FastAPI()\napp.add_middleware(\n    CORSMiddleware,\n    # \n    allow_origins=[\"http://localhost:63342\"],  #  [\"*\"] \n    allow_credentials=True,    # cookie\n    allow_methods=[\"*\"],      # HTTP [\"POST\", \"OPTIONS\"]\n    allow_headers=[\"*\"],      # HTTP [\"Content-Type\"]\n)\n# ",
        "detail": "core.graphapi",
        "documentation": {}
    },
    {
        "label": "RequestsClient",
        "kind": 6,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "class RequestsClient:\n    \"\"\"HTTP client with retry logic and caching\"\"\"\n    def __init__(self):\n        self.session = self._create_session()\n        self.cache = TTLCache(maxsize=100, ttl=3600)  # Cache responses for 1 hour\n    def _create_session(self) -> requests.Session:\n        \"\"\"Create a session with retry logic\"\"\"\n        session = requests.Session()\n        retries = Retry(\n            total=3,",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "LocalChatModel",
        "kind": 6,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "class LocalChatModel(BaseChatModel):\n    \"\"\"LangChain chat model implementation for local LLM models compatible with ChatOpenAI\"\"\"\n    # client: Any = Field(default_factory=RequestsClient)\n    client: RequestsClient = None\n    # \n    model_name: str\n    temperature: float\n    model_kwargs: Dict[str, Any]\n    # Streaming and retries\n    streaming: bool = False",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "AIMessageChunk",
        "kind": 6,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "class AIMessageChunk(AIMessage):\n    \"\"\"AIMessage chunk for streaming.\"\"\"\n# Compatibility type for callbacks\nCallbacks = Optional[Union[List[Callable], Callable]]\n# Utility function to get a response from a model with timeout\n@func_set_timeout(200)\ndef get_from_llm(\n    messages: Union[str, List[Dict[str, str]], List[BaseMessage]],\n    model_name: str = \"Qwen25-7B\",\n    **kwargs,",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "kind": 2,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "def get_from_llm(\n    messages: Union[str, List[Dict[str, str]], List[BaseMessage]],\n    model_name: str = \"Qwen25-7B\",\n    **kwargs,\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM using LangChain\n    Args:\n        messages: Input messages (string, list of dicts, or LangChain messages)\n        model_name: Name of the model to use",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "kind": 5,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "Callbacks = Optional[Union[List[Callable], Callable]]\nclass LocalChatModel(BaseChatModel):\n    \"\"\"LangChain chat model implementation for local LLM models compatible with ChatOpenAI\"\"\"\n    # client: Any = Field(default_factory=RequestsClient)\n    client: RequestsClient = None\n    # \n    model_name: str\n    temperature: float\n    model_kwargs: Dict[str, Any]\n    # Streaming and retries",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "Callbacks",
        "kind": 5,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "Callbacks = Optional[Union[List[Callable], Callable]]\n# Utility function to get a response from a model with timeout\n@func_set_timeout(200)\ndef get_from_llm(\n    messages: Union[str, List[Dict[str, str]], List[BaseMessage]],\n    model_name: str = \"Qwen25-7B\",\n    **kwargs,\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM using LangChain",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "LOCAL_MODELS",
        "kind": 5,
        "importPath": "core.local_model_langchain",
        "description": "core.local_model_langchain",
        "peekOfCode": "LOCAL_MODELS = {\n    model_name: LocalChatModel(\n        model_name=model_name, stop=[\"\\n\\n\"], presence_penalty=0.1\n    )\n    for model_name in MODEL_CONFIGS\n}\nif __name__ == \"__main__\":\n    # Example 1: Simple message\n    model = LocalChatModel(model_name=\"Qwen3-14B\", temperature=0.5, max_tokens=2000)\n    response = model.invoke(\"Tell me who you are? \")",
        "detail": "core.local_model_langchain",
        "documentation": {}
    },
    {
        "label": "LLMClient",
        "kind": 6,
        "importPath": "core.local_request_v2",
        "description": "core.local_request_v2",
        "peekOfCode": "class LLMClient:\n    def __init__(self):\n        self.session = self._create_session()\n        self.cache = TTLCache(maxsize=100, ttl=3600)  # Cache responses for 1 hour\n    def _create_session(self) -> requests.Session:\n        \"\"\"Create a session with retry logic\"\"\"\n        session = requests.Session()\n        retries = Retry(\n            total=3,\n            backoff_factor=0.5,",
        "detail": "core.local_request_v2",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "kind": 2,
        "importPath": "core.local_request_v2",
        "description": "core.local_request_v2",
        "peekOfCode": "def get_from_llm(\n    messages: Union[str, List[Dict[str, str]]], model_name: str = \"Qwen25-7B\", **kwargs\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM with improved error handling and retries.\n    Args:\n        messages: Input messages (string or list of message dicts)\n        model_name: Name of the model to use\n        **kwargs: Additional parameters to override defaults\n    Returns:",
        "detail": "core.local_request_v2",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "core.local_request_v2",
        "description": "core.local_request_v2",
        "peekOfCode": "client = LLMClient()\n@func_set_timeout(200)\ndef get_from_llm(\n    messages: Union[str, List[Dict[str, str]]], model_name: str = \"Qwen25-7B\", **kwargs\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM with improved error handling and retries.\n    Args:\n        messages: Input messages (string or list of message dicts)\n        model_name: Name of the model to use",
        "detail": "core.local_request_v2",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "log_dir = \"./log\"\nos.makedirs(log_dir, exist_ok=True)\n# Generate logging file path with current date\ncurrent_date = datetime.now().strftime(\"%Y%m%d\")  # Format: YYYYMMDD, e.g., 20250407\nlogging_file_path = os.path.join(log_dir, f\"server_pipe_{current_date}.log\")\n# Configure handlers\nhandlers = [logging.FileHandler(logging_file_path), logging.StreamHandler(sys.stdout)]\n# Set logging level (DEBUG overrides INFO)\nlevel = logging.INFO\nlevel = logging.DEBUG",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "current_date",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "current_date = datetime.now().strftime(\"%Y%m%d\")  # Format: YYYYMMDD, e.g., 20250407\nlogging_file_path = os.path.join(log_dir, f\"server_pipe_{current_date}.log\")\n# Configure handlers\nhandlers = [logging.FileHandler(logging_file_path), logging.StreamHandler(sys.stdout)]\n# Set logging level (DEBUG overrides INFO)\nlevel = logging.INFO\nlevel = logging.DEBUG\n# Configure basic logging\nlogging.basicConfig(\n    level=level,",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "logging_file_path",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "logging_file_path = os.path.join(log_dir, f\"server_pipe_{current_date}.log\")\n# Configure handlers\nhandlers = [logging.FileHandler(logging_file_path), logging.StreamHandler(sys.stdout)]\n# Set logging level (DEBUG overrides INFO)\nlevel = logging.INFO\nlevel = logging.DEBUG\n# Configure basic logging\nlogging.basicConfig(\n    level=level,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\",",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "handlers",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "handlers = [logging.FileHandler(logging_file_path), logging.StreamHandler(sys.stdout)]\n# Set logging level (DEBUG overrides INFO)\nlevel = logging.INFO\nlevel = logging.DEBUG\n# Configure basic logging\nlogging.basicConfig(\n    level=level,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=handlers,",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "level",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "level = logging.INFO\nlevel = logging.DEBUG\n# Configure basic logging\nlogging.basicConfig(\n    level=level,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=handlers,\n)\n# Create logger",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "level",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "level = logging.DEBUG\n# Configure basic logging\nlogging.basicConfig(\n    level=level,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    handlers=handlers,\n)\n# Create logger\nlogger = logging.getLogger(__name__)",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "core.log",
        "description": "core.log",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Example usage\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")",
        "detail": "core.log",
        "documentation": {}
    },
    {
        "label": "ProcessState",
        "kind": 6,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "class ProcessState(Enum):\n    INITIALIZED = auto()\n    OUTLINE_GENERATED = auto()\n    SECTIONS_WRITTEN = auto()\n    SECTIONS_REFLECTED = auto()\n    GLOBAL_REFLECTION_DONE = auto()\n    ABSTRACT_CONCLUSION_GENERATED = auto()\n    PAPER_POLISHED = auto()\n    COMPLETED = auto()\n    ERROR = auto()",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "PaperGenerationState",
        "kind": 6,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "class PaperGenerationState:\n    \"\"\"State container for the paper generation process\"\"\"\n    task_id: str\n    user_name: str\n    user_query: str\n    timestamp: str = \"\"\n    rewrite_query: str = \"\"\n    query_type: str = \"\"\n    query_domain: str= \"\"\n    research_field: str = \"\"",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "AsyncSemaphorePool",
        "kind": 6,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "class AsyncSemaphorePool:\n    \"\"\"Centralized semaphore management for concurrent operations\"\"\"\n    def __init__(self):\n        self.semaphores = {\n            \"section_writer\": asyncio.Semaphore(2),  # Limit section_writer concurrency\n            \"section_reflection\": asyncio.Semaphore(2),  # Limit reflection concurrency\n            \"global\": asyncio.Semaphore(5),  # General purpose concurrency limit\n        }\n    async def with_semaphore(self, name: str, func: Callable, *args, **kwargs):\n        \"\"\"Execute function with semaphore protection\"\"\"",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "PaperGenerationPipeline",
        "kind": 6,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "class PaperGenerationPipeline:\n    \"\"\"Main pipeline class for paper generation process\"\"\"\n    def __init__(\n        self,\n        user_name: str,\n        user_query: str,\n        task_id: str = \"\",\n        output_dir: str = \"temp\",\n        rag_service_url: str = DEFAULT_RAG_SERVICE_URL,\n        **kwargs,",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "find_existing_process_file",
        "kind": 2,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "def find_existing_process_file(\n    user_query: str, output_dir: str = \"temp\"\n) -> Optional[str]:\n    \"\"\"Find existing _process.json file for the given query\"\"\"\n    if not os.path.exists(output_dir):\n        return None\n    # Clean query for filename matching\n    clean_query = user_query.replace(\" \", \"_\")[:50]  # Limit length\n    # Look for files that match the pattern\n    for filename in os.listdir(output_dir):",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "load_existing_state",
        "kind": 2,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "def load_existing_state(filepath: str) -> Optional[PaperGenerationState]:\n    \"\"\"Load existing state from _process.json file\"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        state = PaperGenerationState.from_dict(data)\n        # state.step_errors = {\"abstract_conclusion\":\"error in abstract_conclusion step\"}\n        # state.process_state = ProcessState.GLOBAL_REFLECTION_DONE\n        logger.info(f\"Loaded existing state from {filepath}\")\n        logger.info(f\"Current process state: {state.process_state.name}\")",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "determine_resume_point",
        "kind": 2,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "def determine_resume_point(\n    state: PaperGenerationState, do_global_reflection\n) -> ProcessState:\n    \"\"\"Determine from which step to resume based on state and errors\"\"\"\n    # Check for step-specific errors first\n    if state.has_step_error(\"outline\"):\n        logger.info(\"Found error in outline step, resuming from outline generation\")\n        return ProcessState.INITIALIZED\n    if state.has_step_error(\"sections\"):\n        logger.info(\"Found error in sections step, resuming from section processing\")",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "with_retry_and_fallback",
        "kind": 2,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "def with_retry_and_fallback(max_retries: int = 3, retry_delay: float = 2.0):\n    \"\"\"Decorator to add retry logic and fallback mechanism to async functions\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            operation_name = func.__name__\n            retry_count = 0\n            while retry_count < max_retries:\n                try:\n                    return await func(*args, **kwargs)",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "save_results",
        "kind": 2,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "def save_results(results: Dict[str, Any], output_dir: str = \"temp\") -> str:\n    \"\"\"Save the process results to files\"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    # Generate filename based on metadata\n    user_query = results.get(\"user_query\", \"query\").replace(\" \", \"_\")[:50]\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    # filename_base = f\"{output_dir}/{user_query}_{timestamp}\"\n    filename_base = f\"{output_dir}/{user_query}\"\n    # Save full process data",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "semaphore_pool",
        "kind": 5,
        "importPath": "core.main_workflow_opt_for_paper",
        "description": "core.main_workflow_opt_for_paper",
        "peekOfCode": "semaphore_pool = AsyncSemaphorePool()\n# Decorator for error handling and retry logic\ndef with_retry_and_fallback(max_retries: int = 3, retry_delay: float = 2.0):\n    \"\"\"Decorator to add retry logic and fallback mechanism to async functions\"\"\"\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            operation_name = func.__name__\n            retry_count = 0\n            while retry_count < max_retries:",
        "detail": "core.main_workflow_opt_for_paper",
        "documentation": {}
    },
    {
        "label": "llm_map",
        "kind": 5,
        "importPath": "core.model_factory",
        "description": "core.model_factory",
        "peekOfCode": "llm_map = {\n    \"gpt-4\": AzureChatOpenAI(\n        openai_api_type=\"azure\",\n        openai_api_version=\"your api version\",\n        azure_deployment=\"gpt-4\",\n        azure_endpoint=\"you endpoint\",\n        api_key=\"your-api-key-here\",\n    ),\n    \"gpt-4o-mini\": AzureChatOpenAI(\n        openai_api_type=\"azure\",",
        "detail": "core.model_factory",
        "documentation": {}
    },
    {
        "label": "chat_models",
        "kind": 5,
        "importPath": "core.model_factory",
        "description": "core.model_factory",
        "peekOfCode": "chat_models = {\n    key: value for key, value in llm_map.items() if \"r1\" not in key and \"qwq\" not in key\n}\nprint(\"ALL chat_models\", list(chat_models.keys()))\nlocal_chat_models = {\n    key: value\n    for key, value in LOCAL_MODELS.items()\n    if \"r1\" not in key and \"qwq\" not in key and \"openscholar\" not in key\n}\nprint(\"LOCAL chat_models\", list(local_chat_models.keys()))",
        "detail": "core.model_factory",
        "documentation": {}
    },
    {
        "label": "local_chat_models",
        "kind": 5,
        "importPath": "core.model_factory",
        "description": "core.model_factory",
        "peekOfCode": "local_chat_models = {\n    key: value\n    for key, value in LOCAL_MODELS.items()\n    if \"r1\" not in key and \"qwq\" not in key and \"openscholar\" not in key\n}\nprint(\"LOCAL chat_models\", list(local_chat_models.keys()))",
        "detail": "core.model_factory",
        "documentation": {}
    },
    {
        "label": "QueryIntent",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class QueryIntent(BaseModel):\n    research_field: str = Field(\n        description=\"The academic or research field the query belongs to\"\n    )\n    paper_type: str = Field(description=\"The type of query\")\n    topic: str = Field(description=\"Specific topic of the paper\")\n    explanation: Optional[str] = Field(description=\"Brief explanation for the classification\")\nclass Reference(BaseModel):\n    title: str = Field(description=\"Title of the reference\")\n    authors: Optional[str] = Field(description=\"Authors of the reference\")",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "Reference",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class Reference(BaseModel):\n    title: str = Field(description=\"Title of the reference\")\n    authors: Optional[str] = Field(description=\"Authors of the reference\")\n    conference: Optional[str] = Field(description=\"Conference of the reference\")\n    source: Optional[str] = Field(description=\"Source (e.g., journal, arXiv)\")\n    url: Optional[str] = Field(description=\"URL if available\")\n    abstract: Optional[str] = Field(\n        default=\"\", description=\"Abstract of the reference\")\nclass Figure(BaseModel):\n    caption: str = Field(description=\"Caption of the figure\")",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "Figure",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class Figure(BaseModel):\n    caption: str = Field(description=\"Caption of the figure\")\n    description: str = Field(description=\"Description of the figure content\")\n    placeholder_path: Optional[str] = Field(\n        default=None, description=\"Placeholder for figure file path\"\n    )\nclass SectionContent(BaseModel):\n    section_name: str = Field(description=\"Name of the section\")\n    section_index: Optional[Any] = Field(description=\"Index of the section\")\n    parent_section: Optional[str] = Field(",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "SectionContent",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class SectionContent(BaseModel):\n    section_name: str = Field(description=\"Name of the section\")\n    section_index: Optional[Any] = Field(description=\"Index of the section\")\n    parent_section: Optional[str] = Field(\n        default=None, description=\"Parent section name\"\n    )\n    section_key_points: str = Field(description=\"The key point of this section\")\n    section_search_querys: str = Field(description=\"Search query \")\n    section_text: str = Field(description=\"The generated content from the RAG service\")\n    main_figure_data: str = Field(\"The base64 encoded image data for the main figure\")",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "SectionSummary",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class SectionSummary(BaseModel):\n    section_name: str = Field(description=\"Name of the section\")\n    summary: List[str] = Field(\n        description=\"Up to 3 sentences summarizing the section content\"\n    )\n    parent_section: Optional[str] = Field(\n        default=None, description=\"Parent section name, if any\"\n    )\nclass Feedback(BaseModel):\n    meets_requirements: bool = Field(",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "Feedback",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class Feedback(BaseModel):\n    meets_requirements: bool = Field(\n        description=\"Whether the content meets requirements\"\n    )\n    reasons: Optional[List[str]] = Field(\n        default=None, description=\"Reasons for not meeting requirements\"\n    )\n    suggested_improvements: Optional[List[str]] = Field(\n        default=None, description=\"Suggestions for improvement\"\n    )",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "GlobalReflection",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class GlobalReflection(BaseModel):\n    meets_requirements: bool = Field(\n        description=\"Whether the full paper meets requirements\"\n    )\n    reasons: Optional[List[str]] = Field(\n        default=None, description=\"Reasons for not meeting requirements\"\n    )\n    section_feedback: Optional[Dict[str, List[str]]] = Field(\n        default=None, description=\"Per-section improvement suggestions\"\n    )",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "FinalPaper",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class FinalPaper(BaseModel):\n    user_query: str = Field(description=\"Original user query\")\n    meta_info: Dict[str, Any] = Field(description=\"Meta info of the paper\")\n    title: str = Field(description=\"Paper title\")\n    abstract: str = Field(description=\"Paper abstract\")\n    sections: Dict[str, SectionContent] = Field(\n        description=\"Section contents with references and figures\"\n    )\n    conclusion: Optional[str] = Field(default=None, description=\"Conclusion content\")\n    global_references: List[Reference] = Field(",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "IntermediateState",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class IntermediateState(BaseModel):\n    user_name: str = Field(default=\"anonymous\", description=\"User identifier\")\n    user_query: str = Field(description=\"Original user query\")\n    timestamp: str = Field(description=\"Timestamp of process start\")\n    data: Dict[str, Any] = Field(description=\"Intermediate results with descriptions\")\ndef generate_filename_prefix(user_name: str, user_query: str, timestamp: str) -> str:\n    \"\"\"Generate a unique filename prefix for intermediate storage.\"\"\"\n    query_snippet = user_query[:50].replace(\" \", \"_\").replace(\"/\", \"_\")\n    return f\"{user_name}_{query_snippet}_{timestamp}\"\ndef initialize_intermediate_state(user_name: str, user_query: str) -> IntermediateState:",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "SectionData",
        "kind": 6,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "class SectionData:\n    \"\"\"Data structure for section information\"\"\"\n    section_name: str\n    parent_section: str\n    section_index: int\n    key_points: List[str] = field(default_factory=list)\n    search_queries: List[str] = field(default_factory=list)\n    content: Dict[str, Any] = field(default_factory=dict)\n    reflection_results: Dict[str, Any] = field(default_factory=dict)\n    section_summary: str = \"\"",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "generate_filename_prefix",
        "kind": 2,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "def generate_filename_prefix(user_name: str, user_query: str, timestamp: str) -> str:\n    \"\"\"Generate a unique filename prefix for intermediate storage.\"\"\"\n    query_snippet = user_query[:50].replace(\" \", \"_\").replace(\"/\", \"_\")\n    return f\"{user_name}_{query_snippet}_{timestamp}\"\ndef initialize_intermediate_state(user_name: str, user_query: str) -> IntermediateState:\n    \"\"\"Initialize the intermediate state dictionary.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    return IntermediateState(\n        user_name=user_name, user_query=user_query, timestamp=timestamp, data={}\n    )",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "initialize_intermediate_state",
        "kind": 2,
        "importPath": "core.models",
        "description": "core.models",
        "peekOfCode": "def initialize_intermediate_state(user_name: str, user_query: str) -> IntermediateState:\n    \"\"\"Initialize the intermediate state dictionary.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    return IntermediateState(\n        user_name=user_name, user_query=user_query, timestamp=timestamp, data={}\n    )\n@dataclass\nclass SectionData:\n    \"\"\"Data structure for section information\"\"\"\n    section_name: str",
        "detail": "core.models",
        "documentation": {}
    },
    {
        "label": "AbstractConclusionResult",
        "kind": 6,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "class AbstractConclusionResult(BaseModel):\n    \"\"\"Model for abstract and conclusion generation results.\"\"\"\n    abstract: str = Field(description=\"Generated paper abstract\")\n    conclusion: str = Field(description=\"Generated paper conclusion\")\n    meets_requirements: bool = Field(description=\"Whether both meet requirements\")\n    feedback: Optional[str] = Field(\n        default=None, description=\"Feedback for improvement if needed\"\n    )\nclass AbstractConclusionState(BaseModel):\n    \"\"\"State container for the abstract/conclusion workflow.\"\"\"",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "AbstractConclusionState",
        "kind": 6,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "class AbstractConclusionState(BaseModel):\n    \"\"\"State container for the abstract/conclusion workflow.\"\"\"\n    user_query: str = Field(description=\"Original user query\")\n    paper_title: str = Field(description=\"Paper title\")\n    outline: Dict[str, Any] = Field(description=\"Paper outline structure\")\n    summaries: List[dict] = Field(description=\"Section summaries\")\n    abstract: Optional[str] = Field(default=None, description=\"Generated abstract\")\n    conclusion: Optional[str] = Field(default=None, description=\"Generated conclusion\")\n    reflection_count: int = Field(default=0, description=\"Current reflection iteration\")\n    max_reflections: int = Field(default=3, description=\"Maximum reflection cycles\")",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "GenerationError",
        "kind": 6,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "class GenerationError(Exception):\n    \"\"\"Exception raised when generation fails after multiple retries.\"\"\"\n    pass\nasync def generate_section_introduction_content(\n    paper_title: str,\n    user_query: str,\n    section: Dict[str, Any],\n    llm: Any,  # Language model instance\n) -> str:\n    \"\"\"Generates an introductory paragraph for a given section based on its key points.\"\"\"",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "create_fallback_abstract",
        "kind": 2,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "def create_fallback_abstract(\n    paper_title: str, user_query: str, errors: List[str]\n) -> str:\n    \"\"\"Create a fallback abstract when generation fails.\"\"\"\n    logger.warning(f\"Creating fallback abstract for '{paper_title}'\")\n    fallback_text = (\n        f\"This paper addresses the topic of '{paper_title}'. \"\n        f\"It explores key aspects related to {user_query} through systematic analysis. \"\n        f\"The research examines theoretical foundations, current methodologies, and practical applications. \"\n        f\"Findings suggest important implications for both theory and practice in this domain.\"",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "create_fallback_conclusion",
        "kind": 2,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "def create_fallback_conclusion(\n    paper_title: str, abstract: str, errors: List[str]\n) -> str:\n    \"\"\"Create a fallback conclusion when generation fails.\"\"\"\n    logger.warning(f\"Creating fallback conclusion for '{paper_title}'\")\n    fallback_text = (\n        f\"This paper has presented a comprehensive analysis of topics related to '{paper_title}'. \"\n        f\"The research has explored key theoretical concepts and practical applications in this domain. \"\n        f\"The findings contribute to the existing literature by providing insights into critical aspects \"\n        f\"of the subject matter. Future research should expand on these foundations by addressing \"",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "should_continue_reflection",
        "kind": 2,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "def should_continue_reflection(state: AbstractConclusionState) -> str:\n    \"\"\"Decision node to determine if more reflection is needed.\"\"\"\n    # If evaluation is disabled, always return the result\n    if not state.evaluate_required:\n        logger.info(\"Evaluation disabled, returning result\")\n        return \"return_result\"\n    logger.info(\n        f\"Checking if more reflection needed: meets_requirements={state.meets_requirements}, count={state.reflection_count}/{state.max_reflections}\"\n    )\n    if state.meets_requirements:",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "build_abstract_conclusion_workflow",
        "kind": 2,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "def build_abstract_conclusion_workflow():\n    \"\"\"Build and compile the workflow for abstract and conclusion generation.\"\"\"\n    workflow = StateGraph(AbstractConclusionState)\n    # Define nodes\n    workflow.add_node(\"generate_abstract\", generate_abstract)\n    workflow.add_node(\"generate_conclusion\", generate_conclusion)\n    workflow.add_node(\"evaluate\", evaluate_abstract_conclusion)\n    # Define edges\n    workflow.add_edge(\"generate_abstract\", \"generate_conclusion\")\n    workflow.add_edge(\"generate_conclusion\", \"evaluate\")",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "validate_input_params",
        "kind": 2,
        "importPath": "core.paper_abstract_conclusion_opt",
        "description": "core.paper_abstract_conclusion_opt",
        "peekOfCode": "def validate_input_params(params: Dict[str, Any]) -> None:\n    \"\"\"Validate input parameters and raise appropriate exceptions.\"\"\"\n    required_keys = [\"paper_title\", \"user_query\", \"outline\"]\n    missing_keys = [\n        key for key in required_keys if key not in params or not params[key]\n    ]\n    if missing_keys:\n        raise ValueError(f\"Missing required parameters: {', '.join(missing_keys)}\")\n    # Validate sections_content existence\n    if \"sections_content\" not in params:",
        "detail": "core.paper_abstract_conclusion_opt",
        "documentation": {}
    },
    {
        "label": "ImprovementAction",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class ImprovementAction(TypedDict):\n    section: str\n    issues: List[str]\n    rewrite: bool\nclass GlobalReflectionResult(BaseModel):\n    \"\"\"Model for global paper reflection results.\"\"\"\n    meets_requirements: bool = Field(description=\"Whether paper meets academic requirements\")\n    feedback: str = Field(description=\"Feedback for improvement if needed\")\n    improvement_actions: List[Dict[str, Any]] = Field(\n        default_factory=list,",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "GlobalReflectionResult",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class GlobalReflectionResult(BaseModel):\n    \"\"\"Model for global paper reflection results.\"\"\"\n    meets_requirements: bool = Field(description=\"Whether paper meets academic requirements\")\n    feedback: str = Field(description=\"Feedback for improvement if needed\")\n    improvement_actions: List[Dict[str, Any]] = Field(\n        default_factory=list,\n        description=\"Suggested actions for improving specific sections\",\n    )\nclass SectionRecommendation(TypedDict):\n    section: str",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "SectionRecommendation",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class SectionRecommendation(TypedDict):\n    section: str\n    issues: List[str]\n    rewrite_votes: int\n    total_votes: int\nclass ModelEvaluationDetail(TypedDict):\n    model: str\n    meets_requirements: bool\n    feedback: str\n    improvement_actions: List[Dict[str, Any]]",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "ModelEvaluationDetail",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class ModelEvaluationDetail(TypedDict):\n    model: str\n    meets_requirements: bool\n    feedback: str\n    improvement_actions: List[Dict[str, Any]]\n    raw_output: str\n    parse_error: Optional[str]\n    error: Optional[str]\n    traceback: Optional[str]\nclass EvaluationHistory(TypedDict):",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "EvaluationHistory",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class EvaluationHistory(TypedDict):\n    iteration: int\n    timestamp: str\n    model_evaluations: List[ModelEvaluationDetail]\n    merged_result: Dict[str, Any]\nclass RewriteResult(TypedDict):\n    section_name: str\n    section_index: Optional[int]\n    parent_section: Optional[str]\n    rewrite_successful: bool",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "RewriteResult",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class RewriteResult(TypedDict):\n    section_name: str\n    section_index: Optional[int]\n    parent_section: Optional[str]\n    rewrite_successful: bool\n    section_content: Optional[List[Dict[str, Any]]]\n    enhanced_search_queries: Optional[List[str]]\n    improvement_issues: Optional[List[str]]\n    error: Optional[str]\n    traceback: Optional[str]",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "RewriteHistory",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class RewriteHistory(TypedDict):\n    iteration: int\n    timestamp: str\n    sections_rewritten: int\n    rewrite_details: List[RewriteResult]\nclass SectionToRewrite(TypedDict):\n    section_name: str\n    section_info: Union[List[Dict[str, Any]], Dict[str, Any]]\n    issues: List[str]\nclass IssueAnalysisResult(BaseModel):",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "SectionToRewrite",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class SectionToRewrite(TypedDict):\n    section_name: str\n    section_info: Union[List[Dict[str, Any]], Dict[str, Any]]\n    issues: List[str]\nclass IssueAnalysisResult(BaseModel):\n    issue_mapping: Dict[str, List[str]] = Field(description=\"Mapping from existing key points to the list of issues relevant to them.\")\n    issues_for_new_keypoints: List[str] = Field(description=\"List of issues that do not strongly relate to any existing key point and require new ones.\")\n@lru_cache(maxsize=32)\ndef get_evaluation_models(seed: Optional[int] = None) -> List[str]:\n    \"\"\"Get a consistent set of evaluation models based on seed.",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "IssueAnalysisResult",
        "kind": 6,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "class IssueAnalysisResult(BaseModel):\n    issue_mapping: Dict[str, List[str]] = Field(description=\"Mapping from existing key points to the list of issues relevant to them.\")\n    issues_for_new_keypoints: List[str] = Field(description=\"List of issues that do not strongly relate to any existing key point and require new ones.\")\n@lru_cache(maxsize=32)\ndef get_evaluation_models(seed: Optional[int] = None) -> List[str]:\n    \"\"\"Get a consistent set of evaluation models based on seed.\n    Args:\n        seed: Optional seed value for reproducible random selection\n    Returns:\n        List of selected model names",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "get_evaluation_models",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def get_evaluation_models(seed: Optional[int] = None) -> List[str]:\n    \"\"\"Get a consistent set of evaluation models based on seed.\n    Args:\n        seed: Optional seed value for reproducible random selection\n    Returns:\n        List of selected model names\n    \"\"\"\n    # if seed is not None:\n    #     random.seed(seed)\n    # available_models = [",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "extract_json_from_text",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def extract_json_from_text(text: str) -> str:\n    \"\"\"Extract JSON string from model output text.\n    Args:\n        text: Raw text that may contain JSON\n    Returns:\n        Extracted JSON string\n    \"\"\"\n    # Try to extract JSON from code blocks first\n    json_match = re.search(r\"```json\\s*(.*?)\\s*```\", text, re.DOTALL)\n    if json_match:",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "parse_and_normalize_evaluation",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def parse_and_normalize_evaluation(json_str: str) -> Dict[str, Any]:\n    \"\"\"Parse and normalize evaluation JSON.\n    Args:\n        json_str: JSON string to parse\n    Returns:\n        Normalized dictionary with standardized fields\n    \"\"\"\n    try:\n        parsed_data = json.loads(json_str)\n        # Create normalized result structure",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "extract_issues_from_action",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def extract_issues_from_action(action: Dict[str, Any]) -> List[str]:\n    \"\"\"Extract issues from action with various field formats.\n    Args:\n        action: Dictionary containing action details\n    Returns:\n        List of issue strings\n    \"\"\"\n    issues = []\n    # Handle \"issues\" field in list format\n    if \"issues\" in action and isinstance(action[\"issues\"], list):",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "determine_rewrite_status",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def determine_rewrite_status(action: Dict[str, Any]) -> bool:\n    \"\"\"Determine if a section needs rewriting based on action fields.\n    Args:\n        action: Dictionary containing action details\n    Returns:\n        Boolean indicating if rewrite is needed\n    \"\"\"\n    # Direct rewrite field\n    if \"rewrite\" in action:\n        return bool(action[\"rewrite\"])",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "create_fallback_result",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def create_fallback_result(model_name: str, error_message: str, evaluation_detail: ModelEvaluationDetail) -> Tuple[GlobalReflectionResult, ModelEvaluationDetail]:\n    \"\"\"Create fallback result when evaluation fails.\n    Args:\n        model_name: Name of the model\n        error_message: Error message to include\n        evaluation_detail: Partial evaluation detail to complete\n    Returns:\n        Tuple of (fallback result, completed evaluation detail)\n    \"\"\"\n    fallback_result = GlobalReflectionResult(",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "merge_feedback",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def merge_feedback(results: List[GlobalReflectionResult], meets_requirements: bool) -> str:\n    \"\"\"Merge feedback from multiple evaluations.\n    Args:\n        results: List of evaluation results\n        meets_requirements: Whether the paper meets requirements overall\n    Returns:\n        Merged feedback string\n    \"\"\"\n    all_feedback = []\n    for r in results:",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "consolidate_improvement_actions",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def consolidate_improvement_actions(\n    results: List[GlobalReflectionResult],\n) -> List[Dict[str, Any]]:\n    \"\"\"Consolidate improvement actions from multiple evaluations.\n    Args:\n        results: List of evaluation results\n    Returns:\n        Consolidated list of improvement actions\n    \"\"\"\n    # Collect all section recommendations",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "add_unique_issues",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def add_unique_issues(target_list: List[str], issues: Any) -> None:\n    \"\"\"Add unique issues to the target list.\n    Args:\n        target_list: List to add issues to\n        issues: Issues to add (may be string or list)\n    \"\"\"\n    if isinstance(issues, list):\n        for issue in issues:\n            if issue and issue not in target_list:\n                target_list.append(issue)",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "extract_improvement_issues",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def extract_improvement_issues(section_info: List[Dict[str, Any]]) -> List[str]:\n    \"\"\"Extract improvement issues from section info.\n    Args:\n        section_info: List of section information dictionaries\n    Returns:\n        List of improvement issues\n    \"\"\"\n    improvement_issues = []\n    for item in section_info:\n        if not isinstance(item, dict):",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "extract_section_metadata",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def extract_section_metadata(section_info: List[Dict[str, Any]], section_name: str) -> Tuple[List[str], List[str], Dict[str, Any]]:\n    \"\"\"Extract metadata from section information.\n    Args:\n        section_info: List of section information dictionaries\n        section_name: Name of the section\n    Returns:\n        Tuple of (search queries, key points, section metadata)\n    \"\"\"\n    original_search_queries = []\n    original_key_points = []",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "identify_sections_to_rewrite",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def identify_sections_to_rewrite(\n    reflection_result: GlobalReflectionResult,\n    sections_content: Dict[str, List[Dict[str, Any]]],\n) -> List[SectionToRewrite]:\n    \"\"\"Identify sections that need to be rewritten.\n    Args:\n        reflection_result: Result of paper evaluation\n        sections_content: Content of all paper sections\n    Returns:\n        List of sections that need rewriting",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "prepare_rewrite_tasks",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def prepare_rewrite_tasks(\n    sections_to_rewrite: List[SectionToRewrite],\n    paper_title: str,\n    user_query: str,\n    outline: Dict[str, Any],\n    rag_service_url: str,\n) -> List[asyncio.Task]:\n    \"\"\"Prepare rewrite tasks for sections.\n    Args:\n        sections_to_rewrite: List of sections that need rewriting",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "prepare_section_info_for_rewriting",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def prepare_section_info_for_rewriting(\n    section: SectionToRewrite,\n) -> List[Dict[str, Any]]:\n    \"\"\"Prepare section information for rewriting.\n    Args:\n        section: Section information\n    Returns:\n        List of section information dictionaries with improvement issues\n    \"\"\"\n    # Convert section_info to list format",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "update_sections_content",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def update_sections_content(\n    rewrite_results: List[RewriteResult],\n    sections_content: Dict[str, List[Dict[str, Any]]],\n) -> bool:\n    \"\"\"Update sections content with rewritten content.\n    Args:\n        rewrite_results: Results of rewriting sections\n        sections_content: Original sections content to update\n    Returns:\n        Boolean indicating if any sections were updated",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "get_test_params",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def get_test_params() -> Dict[str, Any]:\n    \"\"\"Return example parameters for testing.\"\"\"\n    # from example import example_global_reflection_in\n    # return example_global_reflection_in\n    from utils import format_sections_for_global_reflection\n    src_file = \"./example_full_data.json\"\n    with open(src_file, \"r\") as f:\n        example_global_reflection_in = json.load(f)\n    formatted_sections = {}\n    for section_name, section_data in example_global_reflection_in[\"sections\"].items():",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "test_global_reflection",
        "kind": 2,
        "importPath": "core.paper_global_reflection_opt",
        "description": "core.paper_global_reflection_opt",
        "peekOfCode": "def test_global_reflection() -> None:\n    \"\"\"Test the synchronous global_reflection function.\"\"\"\n    try:\n        test_params = get_test_params()\n        global_reflection_results = asyncio.run(global_reflection(test_params))\n        print(json.dumps(global_reflection_results, indent=2))\n        dest_file = \"./temp/global_reflection_test.json\"\n        with open(dest_file, \"w\") as f:\n            json.dump(global_reflection_results, f, ensure_ascii=False, indent=2)\n        print(f\"Results saved to: {dest_file}\")",
        "detail": "core.paper_global_reflection_opt",
        "documentation": {}
    },
    {
        "label": "OutlineSection",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class OutlineSection(BaseModel):\n    \"\"\"Section structure for the paper outline\"\"\"\n    title: str = Field(description=\"Section title\")\n    key_points: List[str] = Field(description=\"Key points of the section content\")\n    subsections: Optional[List[\"OutlineSection\"]] = Field(\n        default=None, description=\"Subsections of this section\"\n    )\nclass PaperOutline(BaseModel):\n    \"\"\"Complete paper outline structure\"\"\"\n    title: str = Field(description=\"Paper title\")",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "PaperOutline",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class PaperOutline(BaseModel):\n    \"\"\"Complete paper outline structure\"\"\"\n    title: str = Field(description=\"Paper title\")\n    abstract: str = Field(description=\"Paper abstract\")\n    sections: List[OutlineSection] = Field(description=\"Paper section structure\")\nclass ReflectionResult(BaseModel):\n    \"\"\"Results of outline reflection\"\"\"\n    meets_requirements: bool = Field(\n        description=\"Whether it meets the paper writing requirements\"\n    )",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "ReflectionResult",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class ReflectionResult(BaseModel):\n    \"\"\"Results of outline reflection\"\"\"\n    meets_requirements: bool = Field(\n        description=\"Whether it meets the paper writing requirements\"\n    )\n    reasons: Optional[List[str]] = Field(\n        default=None, description=\"Reasons for not meeting requirements\"\n    )\nclass ContentSearchQuery(BaseModel):\n    \"\"\"Search query for a specific content point\"\"\"",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "ContentSearchQuery",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class ContentSearchQuery(BaseModel):\n    \"\"\"Search query for a specific content point\"\"\"\n    content_point: str = Field(description=\"The specific content point\")\n    query: str = Field(description=\"Search query for this content point\")\nclass SectionSearchQuery(BaseModel):\n    \"\"\"Search queries for an outline section\"\"\"\n    section_title: str = Field(description=\"Title of the section\")\n    content_queries: List[ContentSearchQuery] = Field(\n        description=\"List of content-specific search queries for this section\"\n    )",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "SectionSearchQuery",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class SectionSearchQuery(BaseModel):\n    \"\"\"Search queries for an outline section\"\"\"\n    section_title: str = Field(description=\"Title of the section\")\n    content_queries: List[ContentSearchQuery] = Field(\n        description=\"List of content-specific search queries for this section\"\n    )\n    importance: float = Field(\n        description=\"Importance score of this section\", ge=0, le=1\n    )\n    is_conclusion: bool = Field(",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "State",
        "kind": 6,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "class State(BaseModel):\n    \"\"\"State container for the workflow\"\"\"\n    user_query: str\n    research_field: Optional[QueryIntent] = None\n    outlines: Optional[List[PaperOutline]] = None\n    synthesized_outline: Optional[PaperOutline] = None\n    reflection_result: Optional[ReflectionResult] = None\n    reflection_count: int = 0\n    max_reflections: int = 3\n    max_sections: int = 4",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "is_conclusion_reference_section",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def is_conclusion_reference_section(section_title: str) -> bool:\n    \"\"\"\n    Determine if a section is a conclusion section based on its title\n    Args:\n        section_title: The title of the section\n    Returns:\n        Boolean indicating whether the section is a conclusion section\n    \"\"\"\n    section_title_lower = section_title.lower()\n    # First check our cached keyword set",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "is_conclusion_reference_section_model_based",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def is_conclusion_reference_section_model_based(section_title: str) -> bool:\n    \"\"\"\n    Use a language model to determine if a section is a conclusion section.\n    Args:\n        section_title: The title of the section\n    Returns:\n        Boolean indicating whether the section is a conclusion section\n    \"\"\"\n    prompt = get_outline_conclusion_judge_prompt(section_title)\n    chain = prompt | default_llm | StrOutputParser()",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "determine_research_field",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def determine_research_field(state: State) -> State:\n    \"\"\"\n    Determine research field and paper type from user query\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with research field\n    \"\"\"\n    logger.info(\"Starting determine_research_field step\")\n    parser = PydanticOutputParser(pydantic_object=QueryIntent)",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_single_outline",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_single_outline(\n    model_name: str,\n    field: QueryIntent,\n    user_query: str,\n    parser: PydanticOutputParser,\n    seed_outline: List[dict],\n    max_sections: int = 4,\n    min_depth: int = 2,\n) -> PaperOutline:\n    \"\"\"",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_outlines",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_outlines(state: State) -> State:\n    \"\"\"\n    Generate multiple paper outlines using different models\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with generated outlines\n    \"\"\"\n    logger.info(\"Starting generate_outlines step\")\n    research_field = state.research_field",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "synthesize_outlines",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def synthesize_outlines(state: State) -> State:\n    \"\"\"\n    Synthesize multiple outlines into one optimal outline\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with synthesized outline\n    \"\"\"\n    logger.info(\"Starting synthesize_outlines step\")\n    outlines = state.outlines",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "reflection",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def reflection(state: State) -> State:\n    \"\"\"\n    Review the synthesized outline and provide feedback\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with reflection results\n    \"\"\"\n    logger.info(\"Starting reflection step\")\n    synthesized_outline = state.synthesized_outline",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "improve_outline",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def improve_outline(state: State) -> State:\n    \"\"\"\n    Improve the outline based on reflection feedback\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with improved outline\n    \"\"\"\n    logger.info(\"Starting improve_outline step\")\n    reflection_result = state.reflection_result",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "should_continue_improvement",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def should_continue_improvement(state: State) -> str:\n    \"\"\"\n    Decision logic: determine if we should continue improvement or finalize\n    Args:\n        state: Current workflow state\n    Returns:\n        Decision string (\"finalize\" or \"improve\")\n    \"\"\"\n    logger.info(\n        f\"Checking improvement continuation: reflection_count={state.reflection_count}, max_reflections={state.max_reflections}\"",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "finalize_outline",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def finalize_outline(state: State) -> State:\n    \"\"\"\n    Finalize the outline\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with final outline\n    \"\"\"\n    logger.info(\"Finalizing outline\")\n    return state.update(final_outline=state.synthesized_outline)",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_content_query",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_content_query(\n    content_point: str,\n    section_title: str,\n    paper_title: str,\n    max_retries: int = MAX_RETRY_ATTEMPTS,\n) -> ContentSearchQuery:\n    \"\"\"\n    Generate a search query for a specific content point\n    Args:\n        content_point: The specific content point",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_section_queries",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_section_queries(\n    outline_section: OutlineSection,\n    paper_title: str,\n    parent_title: str = \"\",\n    max_workers: int = QUERY_GENERATION_CONCURRENCY,\n) -> List[SectionSearchQuery]:\n    \"\"\"\n    Generate search queries recursively for sections and subsections\n    Args:\n        outline_section: The section to process",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_search_queries",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_search_queries(state: State) -> State:\n    \"\"\"\n    Generate search queries for each section of the final outline\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with search queries\n    \"\"\"\n    logger.info(\"Starting generate_search_queries step\")\n    outline = state.final_outline",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "prepare_final_result",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def prepare_final_result(state: State) -> State:\n    \"\"\"\n    Prepare the final result with all components\n    Args:\n        state: Current workflow state\n    Returns:\n        Updated workflow state with final result\n    \"\"\"\n    logger.info(\"Preparing final result\")\n    try:",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "build_paper_outline_workflow",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def build_paper_outline_workflow(determain_intent=True) -> Any:\n    \"\"\"\n    Build and compile the workflow graph\n    Returns:\n        Compiled workflow\n    \"\"\"\n    logger.info(\"Building paper outline workflow\")\n    workflow = StateGraph(State)\n    # Add nodes\n    if determain_intent:",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_paper_outline",
        "kind": 2,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "def generate_paper_outline(\n    user_query: str,\n    query_intent: dict = None,\n    max_reflections: int = 3,\n    max_sections: int = 4,\n    min_depth: int = 2,\n) -> Dict[str, Any]:\n    \"\"\"\n    Generate an academic paper outline based on user query with search queries\n    Args:",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "MAX_RETRY_ATTEMPTS",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "MAX_RETRY_ATTEMPTS = 3\nTHREAD_POOL_SIZE = 4\nQUERY_GENERATION_CONCURRENCY = 4\nCONCLUSION_IMPORTANCE_SCORE = 0.2\nREGULAR_SECTION_IMPORTANCE_SCORE = 0.8\n# Default LLM cache - this avoids repeatedly referencing the same model\ndefault_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "THREAD_POOL_SIZE",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "THREAD_POOL_SIZE = 4\nQUERY_GENERATION_CONCURRENCY = 4\nCONCLUSION_IMPORTANCE_SCORE = 0.2\nREGULAR_SECTION_IMPORTANCE_SCORE = 0.8\n# Default LLM cache - this avoids repeatedly referencing the same model\ndefault_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "QUERY_GENERATION_CONCURRENCY",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "QUERY_GENERATION_CONCURRENCY = 4\nCONCLUSION_IMPORTANCE_SCORE = 0.2\nREGULAR_SECTION_IMPORTANCE_SCORE = 0.8\n# Default LLM cache - this avoids repeatedly referencing the same model\ndefault_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",\n        \"conclusions\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "CONCLUSION_IMPORTANCE_SCORE",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "CONCLUSION_IMPORTANCE_SCORE = 0.2\nREGULAR_SECTION_IMPORTANCE_SCORE = 0.8\n# Default LLM cache - this avoids repeatedly referencing the same model\ndefault_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",\n        \"conclusions\",\n        \"concluding remarks\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "REGULAR_SECTION_IMPORTANCE_SCORE",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "REGULAR_SECTION_IMPORTANCE_SCORE = 0.8\n# Default LLM cache - this avoids repeatedly referencing the same model\ndefault_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",\n        \"conclusions\",\n        \"concluding remarks\",\n        \"summary\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "default_llm",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "default_llm = llm_map[DEFAULT_MODEL_FOR_OUTLINE]\n# Cached set of conclusion-related keywords\nCONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",\n        \"conclusions\",\n        \"concluding remarks\",\n        \"summary\",\n        \"final remarks\",\n        \"discussion and conclusion\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "CONCLUSION_KEYWORDS",
        "kind": 5,
        "importPath": "core.paper_outline_opt",
        "description": "core.paper_outline_opt",
        "peekOfCode": "CONCLUSION_KEYWORDS = frozenset(\n    {\n        \"conclusion\",\n        \"conclusions\",\n        \"concluding remarks\",\n        \"summary\",\n        \"final remarks\",\n        \"discussion and conclusion\",\n        \"future work\",\n        \"future directions\",",
        "detail": "core.paper_outline_opt",
        "documentation": {}
    },
    {
        "label": "generate_markdown_from_processed",
        "kind": 2,
        "importPath": "core.paper_poolish_opt",
        "description": "core.paper_poolish_opt",
        "peekOfCode": "def generate_markdown_from_processed(\n    processed_data, abstract_conclusion, conclustion_section_content\n):\n    \"\"\"Generates the final markdown string from processed data.\"\"\"\n    markdown_content = []\n    conclusion_added_in_sections = (\n        False  # Flag to track if conclusion was part of sections\n    )\n    # Add title if available (assuming it might be in processed_data top level)\n    if processed_data.get(\"paper_title\"):  # Add a check for title key if applicable",
        "detail": "core.paper_poolish_opt",
        "documentation": {}
    },
    {
        "label": "create_fallback_data",
        "kind": 2,
        "importPath": "core.paper_poolish_opt",
        "description": "core.paper_poolish_opt",
        "peekOfCode": "def create_fallback_data(outline, sections_content, abstract_conclusion):\n    \"\"\"Create fallback data using original content when processing fails\"\"\"\n    logger.info(\"Creating fallback data from original content\")\n    fallback_data = {\n        \"outline\": copy.deepcopy(outline),\n        \"sections_content\": [],\n        \"reportIndexList\": [],\n        \"Abstract\": abstract_conclusion.get(\"Abstract\", \"\"),  # Consistent key\n        \"Conclusion\": abstract_conclusion.get(\"Conclusion\", \"\"),  # Consistent key\n        \"markdown_content\": \"\",  # Add markdown field",
        "detail": "core.paper_poolish_opt",
        "documentation": {}
    },
    {
        "label": "QueryRewrite",
        "kind": 6,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "class QueryRewrite(BaseModel):\n    original_query: str = Field(description=\"The original user query\")\n    rewritten_query: str = Field(description=\"The rewritten query with improved clarity\")\n    needs_rewrite: bool = Field(description=\"Whether the query needed rewriting\")\n    explanation: str = Field(description=\"Explanation of changes made or why no changes were needed\")\nclass QueryTranslation(BaseModel):\n    original_query: str = Field(description=\"The original user query\")\n    is_english: bool = Field(description=\"Whether the original query is in English\")\n    translated_query: str = Field(description=\"The query translated to English if needed\")\n    detected_language: str = Field(description=\"The detected language of the original query\")",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "QueryTranslation",
        "kind": 6,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "class QueryTranslation(BaseModel):\n    original_query: str = Field(description=\"The original user query\")\n    is_english: bool = Field(description=\"Whether the original query is in English\")\n    translated_query: str = Field(description=\"The query translated to English if needed\")\n    detected_language: str = Field(description=\"The detected language of the original query\")\nclass QueryTypeClassification(BaseModel):\n    query_type: str = Field(description=\"The type of query: 'academic' or 'general'\")\n    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n    reasoning: str = Field(description=\"Explanation for the classification decision\")\nclass QueryProcessingState(BaseModel):",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "QueryTypeClassification",
        "kind": 6,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "class QueryTypeClassification(BaseModel):\n    query_type: str = Field(description=\"The type of query: 'academic' or 'general'\")\n    confidence: float = Field(description=\"Confidence score between 0 and 1\")\n    reasoning: str = Field(description=\"Explanation for the classification decision\")\nclass QueryProcessingState(BaseModel):\n    user_query: str = Field(description=\"The original user query\")\n    intent: Optional[QueryIntent] = Field(default=None, description=\"The detected intent of the query\")\n    rewrite: Optional[QueryRewrite] = Field(default=None, description=\"The query rewrite information\")\n    translation: Optional[QueryTranslation] = Field(default=None, description=\"The query translation information\")\n    classification: Optional[QueryTypeClassification] = Field(default=None, description=\"The query type classification\")",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "QueryProcessingState",
        "kind": 6,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "class QueryProcessingState(BaseModel):\n    user_query: str = Field(description=\"The original user query\")\n    intent: Optional[QueryIntent] = Field(default=None, description=\"The detected intent of the query\")\n    rewrite: Optional[QueryRewrite] = Field(default=None, description=\"The query rewrite information\")\n    translation: Optional[QueryTranslation] = Field(default=None, description=\"The query translation information\")\n    classification: Optional[QueryTypeClassification] = Field(default=None, description=\"The query type classification\")\n    final_query: str = Field(default=\"\", description=\"The final processed query ready for use\")\n    errors: List[str] = Field(default_factory=list, description=\"List of errors encountered during processing\")\n    retry_count: Dict[str, int] = Field(\n        default_factory=lambda: {",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "classify_query_type",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def classify_query_type(state: QueryProcessingState) -> QueryProcessingState:\n    \"\"\"\n    LLM(academic)(general)\n    \"\"\"\n    logger.info(\"classify_query_type ...\")\n    try:\n        llm = llm_map[DEFAULT_MODEL_FOR_QUERY_INTENT]\n        classification_parser = PydanticOutputParser(pydantic_object=QueryTypeClassification)\n        classification_prompt = get_query_type_classification_prompt(\n            query=state.user_query,",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "classify_query_type_standalone",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def classify_query_type_standalone(query: str) -> Dict[str, Any]:\n    \"\"\"\n    \n    Args:\n        query: \n    Returns:\n        \n    \"\"\"\n    try:\n        llm = llm_map[DEFAULT_MODEL_FOR_QUERY_INTENT]",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "detect_language_and_translate",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def detect_language_and_translate(state: QueryProcessingState) -> QueryProcessingState:\n    logger.info(\"detect_language_and_translate ...\")\n    llm = llm_map[DEFAULT_MODEL_FOR_QUERY_INTENT]\n    translation_parser = PydanticOutputParser(pydantic_object=QueryTranslation)\n    translation_prompt = get_language_detection_prompt(\n        query=state.user_query,\n        format_instructions=translation_parser.get_format_instructions(),\n    )\n    chain = translation_prompt | llm | translation_parser\n    result = safe_invoke(",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "detect_query_intent",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def detect_query_intent(state: QueryProcessingState) -> QueryProcessingState:\n    logger.info(\"detect_query_intent ...\")\n    llm = llm_map[DEFAULT_MODEL_FOR_QUERY_INTENT]\n    intent_parser = PydanticOutputParser(pydantic_object=QueryIntent)\n    intent_prompt = get_intent_classification_prompt(\n        query=state.user_query,\n        format_instructions=intent_parser.get_format_instructions(),\n    )\n    chain = intent_prompt | llm | intent_parser\n    result = safe_invoke(",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "rewrite_query",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def rewrite_query(state: QueryProcessingState) -> QueryProcessingState:\n    logger.info(\"rewrite_query\")\n    llm = llm_map[DEFAULT_MODEL_FOR_QUERY_INTENT]\n    rewrite_parser = PydanticOutputParser(pydantic_object=QueryRewrite)\n    rewrite_prompt = get_query_rewrite_prompt(\n        query=state.user_query,\n        research_domain=state.intent.research_field,\n        query_type=state.intent.paper_type,\n        format_instructions=rewrite_parser.get_format_instructions(),\n    )",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "finalize_query",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def finalize_query(state: QueryProcessingState) -> QueryProcessingState:\n    # Prioritize the rewritten query if available\n    if state.rewrite and state.rewrite.needs_rewrite:\n        state.final_query = state.rewrite.rewritten_query\n    # Otherwise use the translated query if translation was needed\n    elif state.translation and not state.translation.is_english:\n        state.final_query = state.translation.translated_query\n    # Otherwise use the original query\n    else:\n        state.final_query = state.user_query",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "create_query_processor_graph",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def create_query_processor_graph(include_classification: bool = True):\n    \"\"\"\n    \n    Args:\n        include_classification: \n    \"\"\"\n    workflow = StateGraph(QueryProcessingState)\n    # Add nodes\n    workflow.add_node(\"detect_language_and_translate\", detect_language_and_translate)\n    workflow.add_node(\"detect_query_intent\", detect_query_intent)",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "process_query",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def process_query(user_query: str, max_retries: int = 3, include_classification: bool = True) -> Dict[str, Any]:\n    \"\"\"\n    \n    Args:\n        user_query: \n        max_retries: \n        include_classification: \n    \"\"\"\n    try:\n        logger.info(f\"Processing new query: {user_query}\")",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "core.paper_understant_query",
        "description": "core.paper_understant_query",
        "peekOfCode": "def test():\n    \"\"\"\"\"\"\n    # Example queries to test\n    test_queries = [\n        \"What are the latest advances in transformer models?\",  # Academic\n        \"\",  # Academic (Chinese)\n        \"what is better, lstm or transformers for nlp tasks\",  # Academic\n        \"2024\",  # General (Chinese)\n        \"\",  # General\n        \"COVID-19 vaccine policy updates\",  # General",
        "detail": "core.paper_understant_query",
        "documentation": {}
    },
    {
        "label": "get_seed_outline_template",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_seed_outline_template(paper_type: str) -> List[Dict]:\n    \"\"\"Return a list of dictionaries describing the outline template for a given\n    paper type.  Each dictionary must include:  title, key_points, and an optional\n    subsections list that follows the same schema recursively.\n    \"\"\"\n    # ------------------------------- SURVEY ---------------------------------- #\n    paper_type = paper_type.lower()\n    if paper_type == \"survey\":\n        return [\n            {",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_intent_classification_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_intent_classification_prompt(\n    query, format_instructions: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for classifying the intent of a user query.\n    Args:\n        format_instructions: Instructions for the expected output format (e.g., Pydantic schema).\n    Returns:\n        A ChatPromptTemplate object configured for intent classification.\n    \"\"\"",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_language_detection_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_language_detection_prompt(\n    query, format_instructions: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for detecting language and translating if necessary.\n    Args:\n        format_instructions: Instructions for the expected output format (e.g., Pydantic schema).\n    Returns:\n        A ChatPromptTemplate object configured for language detection and translation.\n    \"\"\"",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_rewrite_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_query_rewrite_prompt(\n    query, research_domain, query_type, format_instructions: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for evaluating and potentially rewriting a user query.\n    Args:\n        format_instructions: Instructions for the expected output format (e.g., Pydantic schema).\n    Returns:\n        A ChatPromptTemplate object configured for query rewriting.\n    \"\"\"",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_subsection_intro_content_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_subsection_intro_content_prompt(\n    paper_title: str,\n    user_query: str,\n    subsection_name: str,\n    key_points: list,\n    key_points_content: list,\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating introductory content for a subsection header\n    based on its key points and their content.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_intro_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_summary_intro_prompt(\n    paper_title: str, user_query: str, section_name: str, key_points_info: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating introductory content for a section header\n    based on its key points.\n    Args:\n        paper_title: The title of the paper.\n        user_query: The original user query.\n        section_name: The name of the section.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_conclusion_section_content_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_conclusion_section_content_prompt(\n    paper_title: str, user_query: str, section_title: str, section_summaries: list\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating content for a conclusion-related section.\n    Args:\n        paper_title: The title of the paper.\n        user_query: The original user query.\n        section_title: The title of the conclusion section to generate.\n        section_summaries: Summaries of all non-conclusion sections of the paper.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_abstract_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_abstract_prompt(\n    paper_title: str, user_query: str, outline: dict, summaries: list\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating the paper's abstract.\n    Args:\n        paper_title: The title of the paper.\n        user_query: The original user query.\n        outline: The structured outline of the paper.\n        summaries: A list of summaries for the paper's sections.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_conclusion_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_conclusion_prompt(\n    paper_title: str, user_query: str, abstract: str, outline: dict, summaries: list\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating the paper's conclusion.\n    Args:\n        paper_title: The title of the paper.\n        user_query: The original user query.\n        abstract: The generated abstract of the paper.\n        outline: The structured outline of the paper.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_abstract_conclusion_evaluation_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_abstract_conclusion_evaluation_prompt(\n    paper_title: str,\n    user_query: str,\n    abstract: str,\n    conclusion: str,\n    summaries: dict,  # Assuming summaries might be a dict {section_name: summary} here\n    format_instructions: str,\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for evaluating the generated abstract and conclusion.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_research_field_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_research_field_prompt(\n    user_query: str, format_instructions: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template to determine the research field, paper type, and topic from a user query.\n    Args:\n        user_query: The user's initial query.\n        format_instructions: Instructions for the expected output format (e.g., Pydantic schema).\n    Returns:\n        A ChatPromptTemplate object configured for research field analysis.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_generation_prompt(\n    field: str,\n    paper_type: str,\n    topic: str,\n    user_query: str,\n    format_instructions: str,\n    max_sections: int = 4,\n    min_depth: int = 2,\n    seed_outline: Optional[List[Dict]] = None,  # \n) -> ChatPromptTemplate:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_strick_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_generation_strick_prompt(\n    field: str, paper_type: str, topic: str, user_query: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a paper outline with a predefined JSON structure example.\n    (Note: Less flexible than using format_instructions with Pydantic, potentially more brittle).\n    Args:\n        field: The determined research field.\n        paper_type: The determined paper type.\n        topic: The specific research topic.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_synthesis_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_synthesis_prompt(\n    outlines_json: str,\n    field: str,\n    paper_type: str,\n    topic: str,\n    format_instructions: str,\n) -> ChatPromptTemplate:\n    return ChatPromptTemplate.from_messages(\n        [\n            SystemMessage(",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_reflection_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_reflection_prompt(\n    user_query: str,\n    field: str,\n    paper_type: str,\n    topic: str,\n    outline_json: str,\n    format_instructions: str,\n) -> ChatPromptTemplate:\n    return ChatPromptTemplate.from_messages(\n        [",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_generation_prompt_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_generation_prompt_v2(\n    field: str,\n    paper_type: str,\n    topic: str,\n    user_query: str,\n    format_instructions: str,\n    max_sections: int = 4,\n    min_depth: int = 2,\n    seed_outline: Optional[List[Dict]] = None,\n) -> ChatPromptTemplate:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_reflection_prompt_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_reflection_prompt_v2(\n    user_query: str,\n    field: str,\n    paper_type: str,\n    topic: str,\n    outline_json: str,\n    format_instructions: str,\n    max_sections: int = 4,  # \n    min_depth: int = 2,     # \n) -> ChatPromptTemplate:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_improve_prompt_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_improve_prompt_v2(\n    user_query: str,\n    field: str,\n    paper_type: str,\n    topic: str,\n    outline: str,  # JSON string\n    improvement_feedback: str,\n    schema: str,\n    max_sections: int = 4,  # \n    min_depth: int = 2,     # ",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_generation_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_query_generation_prompt(\n    paper_title: str, section_title: str, content_point: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a targeted web search query for a specific content point.\n    Args:\n        paper_title: The title of the paper.\n        section_title: The title of the section containing the content point.\n        content_point: The specific key point or detail requiring a search query.\n    Returns:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_generation_prompt_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_query_generation_prompt_v2(\n    paper_title: str, section_title: str, content_point: str\n) -> ChatPromptTemplate:\n    return ChatPromptTemplate.from_messages(\n        [\n            SystemMessage(\n                content=\"\"\"\nYou are a research assistant skilled in generating precise and context-aware search queries for academic surveys and literature reviews.\nYour task is to generate **one single, well-targeted** search query that supports deeper investigation of a specific content point from a scholarly paper. The goal is to help researchers find **high-quality, thematically relevant, and critically useful** sources from platforms such as Google Scholar, Semantic Scholar, or arXiv.\n[Instructions for Query Generation]",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_conclusion_judge_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_conclusion_judge_prompt(section_title: str) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template to quickly classify if a section title likely refers\n    to a Conclusion, Summary, Discussion, or References section.\n    This is used to potentially skip detailed processing for these standard closing sections.\n    Args:\n        section_title: The section title to classify.\n    Returns:\n        A ChatPromptTemplate object configured for simple title classification,\n        expecting a \"true\" or \"false\" string response.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_outline_improve_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_outline_improve_prompt(\n    user_query: str,\n    field: str,\n    paper_type: str,\n    topic: str,\n    outline: str,  # Assuming outline is passed as a JSON string\n    improvement_feedback: str,  # Feedback on what needs improvement\n    schema: str,  # Schema instructions for the improved outline\n) -> ChatPromptTemplate:\n    \"\"\"",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_evaluation_system_prompts",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_reflection_evaluation_system_prompts() -> List[str]:\n    \"\"\"\n    Provides a list of system prompts (personas) for evaluating a draft subsection in the context of its key point.\n    \"\"\"\n    return [\n        # Persona 1: Standard Academic Reviewer\n        \"\"\"You are a professional academic reviewer evaluating a short section of a paper that corresponds to a specific key point in the outline.\nYour task is to assess the academic quality of this section based on the following dimensions:\n1. Academic Formality and Tone  Is the style formal, terminology precise, and tone appropriate for a peer-reviewed journal?\n2. Clarity and Logical Flow  Are ideas expressed with clarity and logically connected?",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_evaluation_system_prompts_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_reflection_evaluation_system_prompts_v2() -> List[str]:\n    \"\"\"\n    Provides a list of system prompts (personas) for evaluating a draft subsection in the context of its key point.\n    \"\"\"\n    return [\n        # Persona 1: Standard Academic Reviewer\n        \"\"\"You are a professional academic reviewer evaluating a short section of a paper corresponding to a specific key point in the outline.\nYour task is to assess the academic quality of this section based on the following five dimensions:\n1. **Academic Formality and Tone**  The writing must adhere strictly to academic standards: precise terminology, no colloquial expressions, and consistently formal tone. Even minor informality warrants deduction.\n2. **Clarity and Logical Flow**  Ideas must be expressed with exceptional clarity and logical progression. Sentences should build upon each other and transitions must be seamless.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_reflection_eval_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_reflection_eval_prompt(\n    prompt_text: str,  # The specific system prompt/persona for this evaluation\n    paper_title: str,\n    user_query: str,\n    section_name: str,\n    parent_section: Optional[str],  # Made optional to handle top-level sections\n    section_key_point: str,\n    section_content: dict,  # Content related to the specific key point\n    format_instructions: str,  # Schema for the evaluation output\n) -> ChatPromptTemplate:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_intro_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_summary_intro_prompt(\n    paper_title: str, user_query: str, section_name: str, key_points_info: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating an introductory summary paragraph for a section.\n    Args:\n        paper_title: The title of the paper.\n        user_query: The original user query that initiated the paper generation.\n        section_name: The name of the section for which the summary is being generated.\n        key_points_info: A string containing summarized information from the section's key points.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_name_refinement_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_name_refinement_prompt(\n    paper_title: str, section_name: str, content_preview: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for refining a section's title based on its content.\n    Args:\n        paper_title: The title of the paper.\n        section_name: The current title of the section.\n        content_preview: A short preview or summary of the section's content.\n    Returns:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_prompt_template",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_summary_prompt_template(\n    paper_title: str, section_name: str, section_text: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a concise summary of a given section's text.\n    Args:\n        paper_title: The title of the paper.\n        section_name: The name of the section to summarize.\n        section_text: The full text content of the section.\n    Returns:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_section_summary_prompt_template_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_section_summary_prompt_template_v2(\n    paper_title: str, section_name: str, section_text: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a concise summary of a given section's text.\n    Args:\n        paper_title: The title of the paper.\n        section_name: The name of the section to summarize.\n        section_text: The full text content of the section.\n    Returns:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_system",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_global_reflection_eval_system() -> List[str]:\n    \"\"\"\n    Provides a list of different system prompts (personas) for evaluating the entire paper\n    structure and content during the global reflection phase.\n    These prompts focus on a holistic assessment of the paper based on its outline and\n    section summaries/content, checking for overall coherence, completeness, and quality.\n    Returns:\n        A list of strings, each a system prompt defining a global evaluator persona.\n    \"\"\"\n    return [",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_system_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_global_reflection_eval_system_v2() -> List[str]:\n    \"\"\"\n    Provides a list of different system prompts (personas) for evaluating the entire paper\n    structure and content during the global reflection phase.\n    These prompts focus on a holistic assessment of the paper based on its outline and\n    section summaries/content, checking for overall coherence, completeness, and quality.\n    Returns:\n        A list of strings, each a system prompt defining a global evaluator persona.\n    \"\"\"\n    return [",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_global_reflection_eval_paper_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_global_reflection_eval_paper_prompt(\n    prompt_text: str,  # The specific system prompt/persona for this evaluation\n    paper_title: str,\n    user_query: str,\n    outline: Dict[str, Any],  # The full paper outline\n    sections_data: List[\n        Dict[str, Any]\n    ],  # List of section data (e.g., summaries or key content)\n    # format_instructions: str # Removed as JSON format is now embedded in prompt\n) -> ChatPromptTemplate:",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_issue_analysis_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_issue_analysis_prompt(\n    section_name: str,\n    key_points: List[str],\n    issues: List[str],\n    format_instructions: str,  # Pydantic schema instructions\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt to analyze improvement issues against existing key points.\n    Args:\n        section_name: The name of the section.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_new_key_point_generation_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_new_key_point_generation_prompt(\n    section_name: str,\n    issue: str,  # The specific issue requiring a new key point\n    paper_title: str,\n    user_query: str,\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt to create a new key point based on an unaddressed issue.\n    Args:\n        section_name: The name of the section where the new key point will belong.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_enhanced_search_query_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_enhanced_search_query_prompt(\n    paper_title: str,\n    user_query: str,\n    section_name: str,\n    key_point: str,\n    original_query: Optional[str],  # Made optional as it might not always exist\n    improvement_issues: List[str],\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a new, improved search query specifically",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_enhanced_search_query_prompt_v2",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_enhanced_search_query_prompt_v2(\n    paper_title: str,\n    user_query: str,\n    section_name: str,\n    key_point: str,\n    original_query: Optional[str],  # Made optional as it might not always exist\n    improvement_issues: List[str],\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for creating a new, improved search query specifically",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "get_query_type_classification_prompt",
        "kind": 2,
        "importPath": "core.prompt_manager",
        "description": "core.prompt_manager",
        "peekOfCode": "def get_query_type_classification_prompt(\n    query: str, format_instructions: str\n) -> ChatPromptTemplate:\n    \"\"\"\n    Generates a prompt template for classifying the type of user query: academic or general.\n    Args:\n        query: The user query to classify.\n        format_instructions: Instructions for the expected output format (e.g., Pydantic schema).\n    Returns:\n        A ChatPromptTemplate object configured for query type classification.",
        "detail": "core.prompt_manager",
        "documentation": {}
    },
    {
        "label": "SectionReflectionResult",
        "kind": 6,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "class SectionReflectionResult(BaseModel):\n    \"\"\"Model for section reflection results.\"\"\"\n    meets_requirements: bool = Field(description=\"Whether section meets requirements\")\n    feedback: str = Field(description=\"Feedback for improvement if needed\")\n    improvement_queries: List[str] = Field(\n        default_factory=list, description=\"Suggested queries for regenerating content\"\n    )\n    @validator(\"feedback\")\n    def validate_feedback(cls, v):\n        \"\"\"Ensure feedback is not empty.\"\"\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "ReflectionCacheKey",
        "kind": 6,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "class ReflectionCacheKey:\n    \"\"\"Cache key for reflection results.\"\"\"\n    def __init__(self, section_name: str, content_hash: str):\n        self.section_name = section_name\n        self.content_hash = content_hash\n    def __hash__(self):\n        return hash((self.section_name, self.content_hash))\n    def __eq__(self, other):\n        if not isinstance(other, ReflectionCacheKey):\n            return False",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "SectionReflectionState",
        "kind": 6,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "class SectionReflectionState(BaseModel):\n    \"\"\"State container for section reflection workflow.\"\"\"\n    section_name: str = Field(description=\"Name of the section\")\n    parent_section: Optional[str] = Field(\n        default=None, description=\"Parent section name\"\n    )\n    user_query: str = Field(description=\"Original user query\")\n    section_content: Dict[str, Any] = Field(description=\"Section content to reflect on\")\n    paper_title: str = Field(description=\"Paper title\")\n    outline: Dict[str, Any] = Field(description=\"Paper outline\")",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "EvaluationDetail",
        "kind": 6,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "class EvaluationDetail(BaseModel):\n    \"\"\"Model for storing detailed evaluation information.\"\"\"\n    model: str = Field(description=\"Model used for evaluation\")\n    meets_requirements: bool = Field(description=\"Whether requirements are met\")\n    feedback: str = Field(description=\"Detailed feedback\")\n    improvement_queries: List[str] = Field(\n        default_factory=list, description=\"Suggested improvement queries\"\n    )\n    error: Optional[str] = Field(default=None, description=\"Error message if any\")\nclass SummaryDetail(BaseModel):",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "SummaryDetail",
        "kind": 6,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "class SummaryDetail(BaseModel):\n    \"\"\"Model for storing summary generation details.\"\"\"\n    model_used: str = Field(description=\"Model used for summary\")\n    raw_summary: str = Field(description=\"Raw summary text\")\n    final_sentences: List[str] = Field(description=\"Processed summary sentences\")\n    error: Optional[str] = Field(default=None, description=\"Error message if any\")\ndef content_hash(section_content: Dict[str, Any]) -> str:\n    \"\"\"Generate a hash of section content for caching and comparison.\"\"\"\n    content_str = json.dumps(section_content, sort_keys=True)\n    return hashlib.md5(content_str.encode()).hexdigest()",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "content_hash",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def content_hash(section_content: Dict[str, Any]) -> str:\n    \"\"\"Generate a hash of section content for caching and comparison.\"\"\"\n    content_str = json.dumps(section_content, sort_keys=True)\n    return hashlib.md5(content_str.encode()).hexdigest()\n@lru_cache(maxsize=32)\ndef get_evaluation_models(seed: int = None) -> List[str]:\n    \"\"\"Get a consistent set of evaluation models based on seed.\n    Args:\n        seed: Random seed for reproducible model selection\n    Returns:",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "get_evaluation_models",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def get_evaluation_models(seed: int = None) -> List[str]:\n    \"\"\"Get a consistent set of evaluation models based on seed.\n    Args:\n        seed: Random seed for reproducible model selection\n    Returns:\n        List of model names to use for evaluation\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    # Use local models preferentially - they're faster and cheaper",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "get_section_info_from_outline",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def get_section_info_from_outline(\n    section_name: str, outline: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Extract section information from outline.\n    Args:\n        section_name: Name of the section to find\n        outline: Complete paper outline\n    Returns:\n        Section information dictionary or empty dict if not found\n    \"\"\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "merge_feedback",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def merge_feedback(\n    results: List[SectionReflectionResult], meets_requirements: bool\n) -> str:\n    \"\"\"Merge feedback from multiple evaluations.\n    Args:\n        results: List of evaluation results\n        meets_requirements: Whether the section meets requirements overall\n    Returns:\n        Consolidated feedback string\n    \"\"\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "select_top_queries",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def select_top_queries(\n    results: List[SectionReflectionResult], max_queries: int = 3\n) -> List[str]:\n    \"\"\"Select top queries from all evaluations.\n    Args:\n        results: List of evaluation results\n        max_queries: Maximum number of queries to return\n    Returns:\n        List of unique, prioritized queries\n    \"\"\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "process_summary_sentences",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def process_summary_sentences(summary_raw: str) -> List[str]:\n    \"\"\"Process raw summary text into proper sentences.\n    Args:\n        summary_raw: Raw summary text from LLM\n    Returns:\n        List of processed summary sentences\n    \"\"\"\n    # Split by line breaks first\n    summary_sentences = [s.strip() for s in summary_raw.split(\"\\n\") if s.strip()]\n    if not summary_sentences:",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "build_section_reflection_workflow",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def build_section_reflection_workflow():\n    \"\"\"\n    Build and compile a workflow for section reflection and improvement.\n    This function creates a workflow that:\n    1. Evaluates a section's content quality and relevance\n    2. Improves the section through iterative refinement if needed\n    3. Generates a concise summary of the final section\n    Returns:\n        A compiled StateGraph workflow for section reflection\n    \"\"\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "test_section_reflection",
        "kind": 2,
        "importPath": "core.section_reflection_opt",
        "description": "core.section_reflection_opt",
        "peekOfCode": "def test_section_reflection():\n    \"\"\"Test the synchronous section_reflection function.\"\"\"\n    from example import example_section_reflection_inp\n    test_params = example_section_reflection_inp\n    try:\n        reflection_results = asyncio.run(section_reflection(test_params))\n        print(\"Reflection Results (Sync):\")\n        print(json.dumps(reflection_results, indent=2, ensure_ascii=False))\n        # Save results to file\n        dest_file = \"./temp/test_section_reflection.json\"",
        "detail": "core.section_reflection_opt",
        "documentation": {}
    },
    {
        "label": "RagResult",
        "kind": 6,
        "importPath": "core.section_writer_opt",
        "description": "core.section_writer_opt",
        "peekOfCode": "class RagResult(BaseModel):\n    \"\"\"Structured result from RAG service.\"\"\"\n    search_query: str = Field(description=\"The query sent to the RAG service\")\n    section_key_point: str = Field(description=\"The key point of this section\")\n    section_text: str = Field(description=\"The generated content from the RAG service\")\n    main_figure_data: str = Field(\n        description=\"The base64 encoded image data for the main figure\", default=\"\"\n    )\n    main_figure_caption: str = Field(\n        description=\"The caption for the main figure\", default=\"\"",
        "detail": "core.section_writer_opt",
        "documentation": {}
    },
    {
        "label": "SectionWriterState",
        "kind": 6,
        "importPath": "core.section_writer_opt",
        "description": "core.section_writer_opt",
        "peekOfCode": "class SectionWriterState(BaseModel):\n    \"\"\"State container for the section writer workflow.\"\"\"\n    section_name: str = Field(description=\"Name of the section\")\n    section_index: int = Field(description=\"Index of the section\")\n    parent_section: Optional[str] = Field(\n        default=None, description=\"Parent section name\"\n    )\n    user_query: str = Field(description=\"Original user query\")\n    section_key_points: List[str] = Field(description=\"Content key points from outline\")\n    paper_title: str = Field(description=\"Paper title\")",
        "detail": "core.section_writer_opt",
        "documentation": {}
    },
    {
        "label": "RagServiceConfig",
        "kind": 6,
        "importPath": "core.section_writer_opt",
        "description": "core.section_writer_opt",
        "peekOfCode": "class RagServiceConfig:\n    \"\"\"Configuration for RAG service connection with resource management.\"\"\"\n    def __init__(\n        self, url: str, timeout: int = 1200, max_retries: int = 2, retry_delay: int = 5\n    ):\n        \"\"\"\n        Initialize RAG service configuration.\n        Args:\n            url: URL for RAG service API\n            timeout: Request timeout in seconds",
        "detail": "core.section_writer_opt",
        "documentation": {}
    },
    {
        "label": "convert_ctx_to_entities",
        "kind": 2,
        "importPath": "core.section_writer_opt",
        "description": "core.section_writer_opt",
        "peekOfCode": "def convert_ctx_to_entities(response: Dict) -> List[Reference]:\n    \"\"\"\n    Convert RAG response's context to Reference objects.\n    Args:\n        response: The RAG service response dictionary\n    Returns:\n        List of structured Reference objects\n    \"\"\"\n    report_index_list = response.get(\"ctx\", [])\n    # Early return for empty list to avoid unnecessary processing",
        "detail": "core.section_writer_opt",
        "documentation": {}
    },
    {
        "label": "build_section_writer_workflow",
        "kind": 2,
        "importPath": "core.section_writer_opt",
        "description": "core.section_writer_opt",
        "peekOfCode": "def build_section_writer_workflow(\n    rag_service_url: str, timeout: int = 1200, max_retries: int = 2\n):\n    \"\"\"\n    Build and compile the section writer workflow graph.\n    Uses LRU cache to avoid rebuilding the workflow for repeated calls\n    with the same parameters.\n    Args:\n        rag_service_url: URL for RAG service API\n        timeout: Request timeout in seconds",
        "detail": "core.section_writer_opt",
        "documentation": {}
    },
    {
        "label": "RagResult",
        "kind": 6,
        "importPath": "core.section_writer_opt_local",
        "description": "core.section_writer_opt_local",
        "peekOfCode": "class RagResult(BaseModel):\n    \"\"\"Structured result from RAG service.\"\"\"\n    search_query: str = Field(description=\"The query sent to the RAG service\")\n    section_key_point: str = Field(description=\"The key point of this section\")\n    section_text: str = Field(description=\"The generated content from the RAG service\")\n    main_figure_data: str = Field(\n        description=\"The base64 encoded image data for the main figure\", default=\"\"\n    )\n    main_figure_caption: str = Field(\n        description=\"The caption for the main figure\", default=\"\"",
        "detail": "core.section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "SectionWriterState",
        "kind": 6,
        "importPath": "core.section_writer_opt_local",
        "description": "core.section_writer_opt_local",
        "peekOfCode": "class SectionWriterState(BaseModel):\n    \"\"\"State container for the section writer workflow.\"\"\"\n    section_name: str = Field(description=\"Name of the section\")\n    section_index: int = Field(description=\"Index of the section\")\n    parent_section: Optional[str] = Field(\n        default=None, description=\"Parent section name\"\n    )\n    user_query: str = Field(description=\"Original user query\")\n    query_domain: str=Field(description=\"Query domain type: [academic or general]\")\n    section_key_points: List[str] = Field(description=\"Content key points from outline\")",
        "detail": "core.section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "RagServiceConfig",
        "kind": 6,
        "importPath": "core.section_writer_opt_local",
        "description": "core.section_writer_opt_local",
        "peekOfCode": "class RagServiceConfig:\n    \"\"\"Configuration for RAG service connection with resource management.\"\"\"\n    def __init__(\n        self, url: str, timeout: int = 1200, max_retries: int = 2, retry_delay: int = 5\n    ):\n        \"\"\"\n        Initialize RAG service configuration.\n        Args:\n            url: URL for RAG service API\n            timeout: Request timeout in seconds",
        "detail": "core.section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "convert_ctx_to_entities",
        "kind": 2,
        "importPath": "core.section_writer_opt_local",
        "description": "core.section_writer_opt_local",
        "peekOfCode": "def convert_ctx_to_entities(response: Dict) -> List[Reference]:\n    \"\"\"\n    Convert RAG response's context to Reference objects.\n    Args:\n        response: The RAG service response dictionary\n    Returns:\n        List of structured Reference objects\n    \"\"\"\n    report_index_list = response.get(\"ctx\", [])\n    # Early return for empty list to avoid unnecessary processing",
        "detail": "core.section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "build_section_writer_workflow",
        "kind": 2,
        "importPath": "core.section_writer_opt_local",
        "description": "core.section_writer_opt_local",
        "peekOfCode": "def build_section_writer_workflow(\n    rag_service_url: str, timeout: int = 1200, max_retries: int = 2\n):\n    \"\"\"\n    Build and compile the section writer workflow graph.\n    Uses LRU cache to avoid rebuilding the workflow for repeated calls\n    with the same parameters.\n    Args:\n        rag_service_url: URL for RAG service API\n        timeout: Request timeout in seconds",
        "detail": "core.section_writer_opt_local",
        "documentation": {}
    },
    {
        "label": "safe_invoke",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def safe_invoke(\n    chain_func: Callable,\n    inputs: Dict[str, Any],\n    default_value: T,\n    error_msg: str,\n    max_retries: int=3,\n) -> T:\n    \"\"\"\n    Safely invoke a chain with retry logic and error handling\n    Args:",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "visualize_graph",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def visualize_graph(graph: StateGraph):\n    dot = Digraph(format=\"png\")\n    dot.node(\"START\", shape=\"ellipse\", color=\"green\")\n    dot.node(\"END\", shape=\"ellipse\", color=\"red\")\n    for node_name in graph.nodes:\n        dot.node(node_name, shape=\"box\", style=\"rounded\")\n    for edge in graph.edges:\n        dot.edge(edge.source, edge.target)\n    return dot\ndef prepare_sections_data(",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "prepare_sections_data",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def prepare_sections_data(\n    outline: Dict[str, Any], sections_content: Dict[str, List[Dict[str, Any]]]\n) -> List[Dict[str, Any]]:\n    \"\"\"Format section data from outline and sections_content for evaluation.\"\"\"\n    sections_data = []\n    conclusion_sections = []\n    # Sort the sections by their index if available\n    logger.info(f\"prepare_sections_data outline: {outline}\")\n    # logger.info(f\"prepare_sections_data sections_content: {sections_content}\")\n    if \"sections\" in outline:",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "aggregate_references",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def aggregate_references(\n    text: str, snippet_references: List[Dict[str, Any]], local_ref_map: Dict[Any, int]\n) -> str:\n    \"\"\"\n    Replaces citation markers in the text based on a mapping to final indices.\n    Also handles potential aggregation of consecutive identical citations (optional).\n    Args:\n        text: The original text containing citation markers (e.g., \"[1]\", \"[URL]\").\n              The exact format of markers depends on how they were initially generated.\n              Assuming markers correspond to the order in snippet_references or use URLs.",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "format_sections_for_global_reflection",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def format_sections_for_global_reflection(\n    sections: Dict[str, SectionData],\n) -> Dict[str, List[Dict[str, Any]]]:\n    \"\"\"Format sections data for global reflection input\"\"\"\n    formatted_sections = {}\n    for section_name, section_data in sections.items():\n        section_list = []\n        reflection_data = section_data.reflection_results\n        if not reflection_data and section_data.content:\n            reflection_data = section_data.content",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "generate_mind_map_from_outline",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def generate_mind_map_from_outline(processed_data, abstract_conclusion, save_to_file=True, output_dir=\"./output\"):\n    \"\"\"\n    Generate mind map representations (Mermaid and Graphviz) from the processed outline structure.\n    Optionally save Graphviz visualization as PNG file.\n    Args:\n        processed_data: The processed data containing outline and sections\n        abstract_conclusion: Abstract and conclusion data\n        save_to_file: Whether to save the mind map as PNG file\n        output_dir: Directory to save the output files\n    Returns:",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "save_mind_map_as_png",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def save_mind_map_as_png(paper_title, sections_content, abstract_conclusion, output_dir=\"./output\"):\n    \"\"\"\n    Generate and save mind map as PNG file using Graphviz.\n    Args:\n        paper_title: Title of the paper\n        sections_content: Processed sections content\n        abstract_conclusion: Abstract and conclusion data\n        output_dir: Directory to save the output file\n    Returns:\n        str: Path to the saved PNG file",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "generate_graphviz_mind_map",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def generate_graphviz_mind_map(paper_title, sections_content, abstract_conclusion):\n    \"\"\"Generate a Graphviz DOT representation with improved layout.\"\"\"\n    dot_lines = [\n        \"digraph PaperMindMap {\",\n        \"  rankdir=LR;\",  # Left to Right layout\n        \"  size=\\\"12,8!\\\";\",  # Constrain size\n        \"  ratio=auto;\",\n        \"  nodesep=0.5;\",\n        \"  ranksep=1.0;\",\n        \"  splines=ortho;\",",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "generate_mind_map_png_only",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def generate_mind_map_png_only(processed_data, abstract_conclusion, output_dir=\"./output\"):\n    \"\"\"\n    Convenience function to generate and save only the PNG mind map.\n    Args:\n        processed_data: The processed data containing outline and sections\n        abstract_conclusion: Abstract and conclusion data\n        output_dir: Directory to save the output file\n    Returns:\n        str: Path to the saved PNG file or None if failed\n    \"\"\"",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "generate_mermaid_mind_map",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def generate_mermaid_mind_map(paper_title, sections_content, abstract_conclusion):\n    \"\"\"Generate a Mermaid mind map representation.\"\"\"\n    mermaid_lines = [\"mindmap\"]\n    mermaid_lines.append(\"  root)(\" + sanitize_mermaid_text(paper_title) + \")\")\n    # Add abstract if available (title only)\n    abstract_text = abstract_conclusion.get(\"Abstract\", \"\")\n    if abstract_text:\n        mermaid_lines.append(\"    Abstract\")\n    # Sort sections by index\n    sorted_sections = sorted(sections_content, key=lambda x: x.get(\"section_index\", 0))",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "sanitize_mermaid_text",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def sanitize_mermaid_text(text):\n    \"\"\"Sanitize text for Mermaid mind map format.\"\"\"\n    if not text:\n        return \"\"\n    # Remove or replace characters that might break Mermaid syntax\n    text = text.replace(\"(\", \"\").replace(\")\", \"\")\n    text = text.replace(\"[\", \"\").replace(\"]\", \"\")\n    text = text.replace(\"{\", \"\").replace(\"}\", \"\")\n    text = text.replace('\"', \"'\")\n    text = text.replace('\\n', ' ').replace('\\r', ' ')",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "sanitize_graphviz_text",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def sanitize_graphviz_text(text):\n    \"\"\"Sanitize text for Graphviz DOT format with line wrapping at 30 characters.\"\"\"\n    if not text:\n        return \"\"\n    # Escape quotes and backslashes\n    text = text.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n    text = text.replace('\\n', ' ').replace('\\r', '')\n    # Split into words and wrap at 30 characters per line\n    words = text.split()\n    lines = []",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "extract_content_preview",
        "kind": 2,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "def extract_content_preview(content):\n    \"\"\"Extract a meaningful preview from content text.\"\"\"\n    if not content:\n        return \"\"\n    # Remove citations and clean up\n    content = re.sub(r'\\[\\d+\\]', '', content)\n    content = content.strip()\n    # Try to get first sentence\n    sentences = content.split('.')\n    if sentences and len(sentences[0]) > 10:",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "T = TypeVar(\"T\")\nStateT = TypeVar(\"StateT\", bound=\"State\")\ndef safe_invoke(\n    chain_func: Callable,\n    inputs: Dict[str, Any],\n    default_value: T,\n    error_msg: str,\n    max_retries: int=3,\n) -> T:\n    \"\"\"",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "StateT",
        "kind": 5,
        "importPath": "core.utils",
        "description": "core.utils",
        "peekOfCode": "StateT = TypeVar(\"StateT\", bound=\"State\")\ndef safe_invoke(\n    chain_func: Callable,\n    inputs: Dict[str, Any],\n    default_value: T,\n    error_msg: str,\n    max_retries: int=3,\n) -> T:\n    \"\"\"\n    Safely invoke a chain with retry logic and error handling",
        "detail": "core.utils",
        "documentation": {}
    },
    {
        "label": "APIModel",
        "kind": 6,
        "importPath": "eval.evaluation.API.model",
        "description": "eval.evaluation.API.model",
        "peekOfCode": "class APIModel:\n    def __init__(self, model, api_key, api_url) -> None:\n        self.__api_key = api_key\n        self.__api_url = api_url\n        self.model = model\n    def __req(self, text, temperature, max_try=5):\n        url = f\"{self.__api_url}\"\n        pay_load_dict = {\"model\": f\"{self.model}\", \"messages\": [{\n            \"role\": \"user\",\n            \"temperature\": temperature,",
        "detail": "eval.evaluation.API.model",
        "documentation": {}
    },
    {
        "label": "AtomicFactGenerator",
        "kind": 6,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "class AtomicFactGenerator(object):\n    def __init__(self, demon_dir):\n        self.is_bio = True\n        self.model = \"gemini-2.0-flash-thinking-exp-1219\"\n        self.infer_type = \"OpenAI\"\n        self.api_model = APIModel(self.model, self.infer_type)\n        self.group_size =300\n    def _deduplicate_group(self, facts_list: list) -> list:\n        if not facts_list:\n            return facts_list",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "clean_claims",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def clean_claims(claims):\n    \"\"\"\n    Clean up claims like 'Claim 1: ' and similar patterns from the start of each claim.\n    \"\"\"\n    cleaned_claims = []\n    for claim in claims:\n        cleaned_claim = re.sub(r'^Claim \\d+: ', '', claim)  # Remove 'Claim N: ' at the start\n        cleaned_claims.append(cleaned_claim)\n    return cleaned_claims\nclass AtomicFactGenerator(object):",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "text_to_sentences",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def text_to_sentences(text):\n    \"\"\"\n    Use regular expressions to extract \"1. xxx\" type entries from the model's returned text\n    and return a list of each entry.\n    \"\"\"\n    sentences = re.findall(r'\\d+\\.\\s*([^\\n]+)', text)\n    # If the last fact doesn't have a period, add one (optional)\n    if len(sentences) > 0 and sentences[-1] and sentences[-1][-1] != '.':\n        sentences[-1] = sentences[-1] + '.'\n    return sentences",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "normalize_answer",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    import re\n    def remove_articles(text):\n        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n        return re.sub(regex, ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "is_num",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def is_num(text):\n    try:\n        text = int(text)\n        return True\n    except Exception:\n        return False\ndef is_date(text):\n    text = normalize_answer(text)\n    for token in text.split(\" \"):\n        if (not is_num(token)) and token not in MONTHS:",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "is_date",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def is_date(text):\n    text = normalize_answer(text)\n    for token in text.split(\" \"):\n        if (not is_num(token)) and token not in MONTHS:\n            return False\n    return True\ndef extract_numeric_values(text):\n    pattern = r'\\b\\d+\\b'  # regular expression pattern for integers\n    numeric_values = re.findall(pattern, text)  # find all numeric values in the text\n    return set([value for value in numeric_values])  # convert the values to float and return as a list",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "extract_numeric_values",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def extract_numeric_values(text):\n    pattern = r'\\b\\d+\\b'  # regular expression pattern for integers\n    numeric_values = re.findall(pattern, text)  # find all numeric values in the text\n    return set([value for value in numeric_values])  # convert the values to float and return as a list\ndef detect_entities(text, nlp):\n    doc = nlp(text)\n    entities = set()\n    def _add_to_entities(text):\n        if \"-\" in text:\n            for _text in text.split(\"-\"):",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "detect_entities",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def detect_entities(text, nlp):\n    doc = nlp(text)\n    entities = set()\n    def _add_to_entities(text):\n        if \"-\" in text:\n            for _text in text.split(\"-\"):\n                entities.add(_text.strip())\n        else:\n            entities.add(text)\n    for ent in doc.ents:",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "postprocess_atomic_facts",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def postprocess_atomic_facts(_atomic_facts, para_breaks, nlp):\n    \"\"\"\n    Currently not needed for sentence-level breakdown and postprocessing,\n    this function can be kept or simplified as needed.\n    \"\"\"\n    verbs = [\"born.\", \" appointed.\", \" characterized.\", \" described.\", \" known.\", \" member.\", \" advocate.\", \"served.\", \"elected.\"]\n    permitted_verbs = [\"founding member.\"]\n    atomic_facts = []\n    new_atomic_facts = []\n    new_para_breaks = []",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "ensure_directory_exists",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def ensure_directory_exists(filepath):\n    \"\"\"Ensure the directory for the given filepath exists.\"\"\"\n    directory = os.path.dirname(filepath)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory)\ndef process_section(section, fact_generator):\n    atomic_facts = fact_generator.get_atomic_facts(section)\n    total_sentences = len(sent_tokenize(section)) \n    atomic_facts = clean_claims(atomic_facts)\n    return atomic_facts, total_sentences",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "process_section",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def process_section(section, fact_generator):\n    atomic_facts = fact_generator.get_atomic_facts(section)\n    total_sentences = len(sent_tokenize(section)) \n    atomic_facts = clean_claims(atomic_facts)\n    return atomic_facts, total_sentences\ndef extract_and_deduplicate_facts(survey, topic):\n    fact_generator = AtomicFactGenerator(\"demos\")\n    sections = re.findall(\n        r'(^## \\d+(?:\\.\\s|\\s|$).*?)(?=^## \\d+(?:\\.\\s|\\s|$)|^## References|\\Z)',\n        survey,",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "extract_and_deduplicate_facts",
        "kind": 2,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "def extract_and_deduplicate_facts(survey, topic):\n    fact_generator = AtomicFactGenerator(\"demos\")\n    sections = re.findall(\n        r'(^## \\d+(?:\\.\\s|\\s|$).*?)(?=^## \\d+(?:\\.\\s|\\s|$)|^## References|\\Z)',\n        survey,\n        flags=re.DOTALL | re.MULTILINE\n    )\n    total_sentences = 0\n    all_atomic_facts = []\n    print(f\"Processing {len(sections)} sections for topic: {topic}, start get facts\")",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "MONTHS",
        "kind": 5,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\nMONTHS = [m.lower() for m in MONTHS]\ndef is_num(text):\n    try:\n        text = int(text)\n        return True\n    except Exception:\n        return False\ndef is_date(text):\n    text = normalize_answer(text)",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "MONTHS",
        "kind": 5,
        "importPath": "eval.evaluation.agents.atomic_facts",
        "description": "eval.evaluation.agents.atomic_facts",
        "peekOfCode": "MONTHS = [m.lower() for m in MONTHS]\ndef is_num(text):\n    try:\n        text = int(text)\n        return True\n    except Exception:\n        return False\ndef is_date(text):\n    text = normalize_answer(text)\n    for token in text.split(\" \"):",
        "detail": "eval.evaluation.agents.atomic_facts",
        "documentation": {}
    },
    {
        "label": "Judge",
        "kind": 6,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "class Judge:\n    def __init__(self, jsonl_file: str, model: str, infer_type) -> None:\n        self.model = model\n        self.api_model = APIModel(self.model, infer_type,\"\")\n        self.jsonl_file = jsonl_file\n        self.input_token_usage, self.output_token_usage = 0, 0\n    def __generate_prompt(self, template, paras):\n        prompt = template\n        for k in paras.keys():\n            prompt = prompt.replace(f\"[{k}]\", paras[k])",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "claim_precision",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def claim_precision(pairs):\n    total_claim_num = len(pairs)\n    correct_claim_num = 0\n    for i in range(total_claim_num):\n        for j in range(len(pairs[i])):\n            if not pairs[i][j] == -1:\n                correct_claim_num += 1\n                break\n    return correct_claim_num, total_claim_num\ndef citation_precision(pairs):",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "citation_precision",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def citation_precision(pairs):\n    total_citation_num = 0\n    correct_citation_num = 0\n    for i in range(len(pairs)):\n        for j in range(len(pairs[i])):\n            total_citation_num += 1\n            if not pairs[i][j] == -1:\n                correct_citation_num += 1\n    return correct_citation_num, total_citation_num\ndef reference_precision(pairs, total_paper_num):",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "reference_precision",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def reference_precision(pairs, total_paper_num):\n    reference_set = set()\n    for i in range(len(pairs)):\n        for j in range(len(pairs[i])):\n            if not pairs[i][j] == -1:\n                reference_set.add(pairs[i][j])\n    return len(reference_set) / total_paper_num\ndef reference_coverage(claims, sources_ids, total_paper_num):\n    reference_set = set()\n    for i in range(len(claims)):",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "reference_coverage",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def reference_coverage(claims, sources_ids, total_paper_num):\n    reference_set = set()\n    for i in range(len(claims)):\n        for j in range(len(sources_ids[i])):\n            citation_idx = sources_ids[i][j] - 1\n            reference_set.add(citation_idx)\n    return len(reference_set) / total_paper_num\ndef count_sentences(text):\n    sentences = re.split(r\"[.!?\\n]+(?:\\s|\\n|$)\", text.strip())\n    sentences = [s for s in sentences if s]",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "count_sentences",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def count_sentences(text):\n    sentences = re.split(r\"[.!?\\n]+(?:\\s|\\n|$)\", text.strip())\n    sentences = [s for s in sentences if s]\n    return len(sentences)\ndef citation_density(sources_ids, survey):\n    total_citation_num = 0\n    for i in range(len(sources_ids)):\n        for _ in range(len(sources_ids[i])):\n            total_citation_num += 1\n    total_sentence_num = count_sentences(survey)",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "citation_density",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def citation_density(sources_ids, survey):\n    total_citation_num = 0\n    for i in range(len(sources_ids)):\n        for _ in range(len(sources_ids[i])):\n            total_citation_num += 1\n    total_sentence_num = count_sentences(survey)\n    return total_citation_num / total_sentence_num\ndef avg_citation_per_claim(claims, sources_ids):\n    total_citation_num = 0\n    for i in range(len(claims)):",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "avg_citation_per_claim",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def avg_citation_per_claim(claims, sources_ids):\n    total_citation_num = 0\n    for i in range(len(claims)):\n        for _ in range(len(sources_ids[i])):\n            total_citation_num += 1\n    return total_citation_num / len(claims)\ndef print_result(result_dict):\n    print(\"########## Metric with Judgement ##########\")\n    print(f\"Claim Precision: {result_dict['claim_precision']}\")\n    print(f\"Citation Precision: {result_dict['citation_precision']}\")",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "print_result",
        "kind": 2,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "def print_result(result_dict):\n    print(\"########## Metric with Judgement ##########\")\n    print(f\"Claim Precision: {result_dict['claim_precision']}\")\n    print(f\"Citation Precision: {result_dict['citation_precision']}\")\n    print(f\"Reference Precision: {result_dict['reference_precision']}\")\n    print(f\"######## Metric without Judgement #########\")\n    print(f\"Reference Coverage: {result_dict['reference_coverage']}\")\n    print(f\"Citation Density: {result_dict['citation_density']}\")\n    print(f\"Avg Citation per Claim: {result_dict['avg_citation_per_claim']}\")\n    print(",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "eval.evaluation.agents.judge",
        "description": "eval.evaluation.agents.judge",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass Judge:\n    def __init__(self, jsonl_file: str, model: str, infer_type) -> None:\n        self.model = model\n        self.api_model = APIModel(self.model, infer_type,\"\")\n        self.jsonl_file = jsonl_file\n        self.input_token_usage, self.output_token_usage = 0, 0\n    def __generate_prompt(self, template, paras):\n        prompt = template\n        for k in paras.keys():",
        "detail": "eval.evaluation.agents.judge",
        "documentation": {}
    },
    {
        "label": "get_extraction_prompt",
        "kind": 2,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "def get_extraction_prompt(text: str) -> str:\n    \"\"\"\n    Generate optimized prompt for claim extraction (v2)\n    \"\"\"\n    return f\"\"\"Analyze the following text and decompose it into independent claims following strict consolidation rules:\n[Claim Definition]\nA verifiable objective factual statement that functions as an independent knowledge unit. Each claim must:\n1. Contain complete subject-predicate-object structure\n2. Exist independently without contextual dependency\n3. Exclude subjective evaluations",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "get_deduplication_prompt",
        "kind": 2,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "def get_deduplication_prompt(facts_list: list) -> str:\n    \"\"\"\n     facts  Prompt\n    \"\"\"\n    numbered_facts = \"\\n\".join([f\"{i+1}. {fact}\" for i, fact in enumerate(facts_list)])\n    return f\"\"\"Below is a numbered list of claims. Your task is to identify and group\nclaims that convey the same information, removing all redundancy.\n[Guidelines]:\n- Claims that express the same fact or knowledge in different wording or detail are duplicates.\n- If one claim is fully included within another or repeats the same idea, consider it a duplicate.",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "CRITERIA",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "CRITERIA = {'Coverage': {'description': 'Coverage: Coverage assesses the extent to which the survey encapsulates all relevant aspects of the topic, ensuring comprehensive discussion on both central and peripheral topics.',\n                         'score 1': 'The survey has very limited coverage, only touching on a small portion of the topic and lacking discussion on key areas.',\n                         'score 2': 'The survey covers some parts of the topic but has noticeable omissions, with significant areas either underrepresented or missing.',\n                         'score 3': 'The survey is generally comprehensive in coverage but still misses a few key points that are not fully discussed.',\n                         'score 4': 'The survey covers most key areas of the topic comprehensively, with only very minor topics left out.',\n                         'score 5': 'The survey comprehensively covers all key and peripheral topics, providing detailed discussions and extensive information.', },\n            'Structure': {'description': 'Structure: Structure evaluates the logical organization and coherence of sections and subsections, ensuring that they are logically connected.',\n                          'score 1': 'The survey lacks logic, with no clear connections between sections, making it difficult to understand the overall framework.',\n                          'score 2': 'The survey has weak logical flow with some content arranged in a disordered or unreasonable manner.',\n                          'score 3': 'The survey has a generally reasonable logical structure, with most content arranged orderly, though some links and transitions could be improved such as repeated subsections.',",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "NLI_PROMPT",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "NLI_PROMPT = '''\n---\nClaim:\n[CLAIM]\n---\nSource:\n[SOURCE]\n---\nClaim:\n[CLAIM]",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "OUTLINE_SCORE",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "OUTLINE_SCORE = \"\"\"\nYou are an expert academic writing reviewer. Your task is to evaluate the **structural quality** of a research paper outline.\n**Evaluation criteria:**\n1. Logical flow across sections and subsections.\n2. Natural academic progression (e.g., Introduction  Related Work  Methodology  Results  Discussion  Conclusion).\n3. Consistent depth and number of subsections.\n4. Absence of redundancy or overlapping content.\n5. Smooth and coherent transitions between major sections.\n6. Alignment with structural norms of the specified paper type and research field.\n**Scoring rubric:**",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "CHECK_CITATION_PROMPT",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "CHECK_CITATION_PROMPT = '''\nYou are an expert in artificial intelligence who wants to write a overall and comprehensive survey about [TOPIC].\\n\\\nBelow are a list of papers for references:\n---\n[PAPER LIST]\n---\nYou have written a subsection below:\\n\\\n---\n[SUBSECTION]\n---",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "SUBSECTION_WRITING_PROMPT",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "SUBSECTION_WRITING_PROMPT = '''\nYou are an expert in artificial intelligence who wants to write a overall and comprehensive survey about [TOPIC].\\n\\\nYou have created a overall outline below:\\n\\\n---\n[OVERALL OUTLINE]\n---\nBelow are a list of papers for references:\n---\n[PAPER LIST]\n---",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "LCE_PROMPT",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "LCE_PROMPT = '''\nYou are an expert in artificial intelligence who wants to write a overall and comprehensive survey about [TOPIC].\nNow you need to help to refine one of the subsection to improve th ecoherence of your survey.\nYou are provied with the content of the subsection along with the previous subsections and following subsections.\nPrevious Subsection:\n---\n[PREVIOUS]\n---\nFollowing Subsection:\n---",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "LCE_PROMPT_LATEX",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "LCE_PROMPT_LATEX = '''\nYou are an expert in artificial intelligence who wants to write a overall and comprehensive survey about [TOPIC].\nNow you need to help to refine one of the subsections to improve the coherence of your survey.\nYou are provided with the content of the subsection along with the previous subsections and following subsections.\nPrevious Subsection:\n---\n[PREVIOUS]\n---\nFollowing Subsection:\n---",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "OUTLINE_EVALUATION_PROMPT_V2",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "OUTLINE_EVALUATION_PROMPT_V2 = \"\"\"\n[Task]\nRigorously evaluate the quality of an academic survey outline about [TOPIC] by scoring three dimensions (each 0-10) and calculating the average as the final score.\n[Evaluation Criteria]\nEvaluate each dimension on a 0-10 scale based strictly on the highest standards below. The final score is the average of the three dimension scores.\n1. **Structural Coherence & Narrative Logic** (10 points):\n   - **Ideal Standard**: The outline demonstrates a clear and logical flow, with sections and subsections organized to guide the reader effectively. Transitions are smooth, and the structure supports a coherent narrative.\n   - **Scoring**: Deduct points for issues like imbalanced section lengths, weak transitions, or subsections that disrupt the narrative flow. A perfect score (10) requires no observable flaws.\n2. **Conceptual Depth & Thematic Coverage** (10 points):\n   - **Ideal Standard**: The outline comprehensively covers key themes, concepts, and subtopics, balancing depth and breadth. It reflects a mastery of the fields core debates and evolution.",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "LANGUAGE_EVALUATION_PROMPT_V2",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "LANGUAGE_EVALUATION_PROMPT_V2 = \"\"\"\n[Task]\nRigorously evaluate the quality of an academic survey about [TOPIC] by scoring three dimensions (each 0-10) and calculating the average as the final score.\n[Evaluation Criteria]\nEvaluate each dimension on a 0-10 scale based strictly on the highest standards below. The final score is the average of the three dimension scores.\n1. **Academic Formality** (10 points):\n   - Demonstrates *flawless* academic rigor. Uses precise terminology consistently, avoids colloquial language entirely, and maintains a strictly scholarly tone. Sentence structures are sophisticated and purposefully crafted to enhance analytical depth. **Even a single instance of informal phrasing or imprecise terminology disqualifies a perfect score**.\n2. **Clarity & Readability** (10 points):\n   - Writing is *exceptionally* clear and concise. Sentences are logically structured, with no ambiguity. Transitions between ideas are seamless, and the argument progresses with precision. **Any unnecessary complexity or minor ambiguity precludes full marks.**\n3. **Redundancy** (10 points):",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "CRITICAL_EVALUATION_PROMPT_V2",
        "kind": 5,
        "importPath": "eval.evaluation.agents.prompt",
        "description": "eval.evaluation.agents.prompt",
        "peekOfCode": "CRITICAL_EVALUATION_PROMPT_V2 = \"\"\"\n[Task]\nRigorously evaluate the quality of an academic survey about [TOPIC] by scoring three dimensions (each 0-10) and calculating the average as the final score.\n[Evaluation Criteria]\nThe final score is the sum of the individual scores from the following three dimensions. Please evaluate each dimension thoroughly and rigorously.\n1. **Critical Analysis** (10 points):\n   - Offers a deep, incisive critique of methodologies, results, and underlying assumptions. Provides a clear identification of significant gaps, weaknesses, and areas for improvement. Challenges assumptions with well-supported arguments, offering clear alternatives or improvements.\n2. **Original Insights** (10 points):\n   - Proposes novel, well-supported interpretations or frameworks based on the reviewed literature. Demonstrates a strong understanding of the subject matter and provides genuinely original contributions that challenge the status quo. Insights are clearly connected to existing research, offering fresh perspectives or unique ways forward.\n3. **Future Directions** (10 points):",
        "detail": "eval.evaluation.agents.prompt",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "eval.evaluation.all_eval",
        "description": "eval.evaluation.all_eval",
        "peekOfCode": "def evaluate(jsonl_file, eval_model, infer_type, skip_titles, skip_line_number=0):\n    def eval_single_survey(judge, survey, topic, outline, papers):\n        print(f\"Start evaluating additional dimensions\")\n        dimension_scores = judge.evaluate_all_dimensions(survey, topic)\n        print(f\"Start batch_criteria_based_judging\")\n        criterion = [\"Structure\", \"Relevance\"]\n        scores = judge.batch_criteria_based_judging(survey, topic, criterion)\n        result_dict = {}\n        print(f\"Start outline evaluation\")\n        outlinse_score = judge.evaluate_outline(outline, topic)",
        "detail": "eval.evaluation.all_eval",
        "documentation": {}
    },
    {
        "label": "save_or_update_scores",
        "kind": 2,
        "importPath": "eval.evaluation.all_eval",
        "description": "eval.evaluation.all_eval",
        "peekOfCode": "def save_or_update_scores(args, scores, content_score):\n    if content_score is not None:\n        merged_scores = content_score\n        columns_to_extract = scores.columns.tolist()\n        merged_scores_extracted = merged_scores.reindex(columns=columns_to_extract)\n        scores.set_index(\"name\", inplace=True)\n        merged_scores_extracted.set_index(\"name\", inplace=True)\n        combined_scores = pd.concat([scores, merged_scores_extracted], axis=0)\n        scores = combined_scores.groupby(combined_scores.index).last().reset_index()\n    else:",
        "detail": "eval.evaluation.all_eval",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "eval.evaluation.all_eval",
        "description": "eval.evaluation.all_eval",
        "peekOfCode": "logger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\ndef evaluate(jsonl_file, eval_model, infer_type, skip_titles, skip_line_number=0):\n    def eval_single_survey(judge, survey, topic, outline, papers):\n        print(f\"Start evaluating additional dimensions\")\n        dimension_scores = judge.evaluate_all_dimensions(survey, topic)\n        print(f\"Start batch_criteria_based_judging\")\n        criterion = [\"Structure\", \"Relevance\"]\n        scores = judge.batch_criteria_based_judging(survey, topic, criterion)\n        result_dict = {}",
        "detail": "eval.evaluation.all_eval",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "eval.evaluation.args",
        "description": "eval.evaluation.args",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\n        \"--jsonl_file\",\n        type=str,\n        help=\"Path to the input JSONL file\",\n    )\n    parser.add_argument(\n        \"--saving_path\",\n        default=\"./output/eval\",",
        "detail": "eval.evaluation.args",
        "documentation": {}
    },
    {
        "label": "load_json_as_dict",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def load_json_as_dict(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    if isinstance(data, dict):\n        return data\n    else:\n        raise ValueError(\"The JSON file content is not of dictionary type.\")\ndef insert_zero_after_first_hash(s: str) -> str:\n    index = s.find('#')\n    if index != -1 and index + 1 < len(s) and s[index + 1] != '#':",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "insert_zero_after_first_hash",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def insert_zero_after_first_hash(s: str) -> str:\n    index = s.find('#')\n    if index != -1 and index + 1 < len(s) and s[index + 1] != '#':\n        return s[:index + 1] + \" 0.\" + s[index + 1:]\n    return s\ndef extract_headings(text):\n    pattern = r\"(?:^|\\n)(#{1,3} .+?)(?=\\n\\s*\\n|$)\"\n    matches = re.findall(pattern, text, flags=re.DOTALL)\n    return \"\\n\\n\".join(match.strip() for match in matches)\ndef save_list_to_jsonl(data_list, file_path):",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "extract_headings",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def extract_headings(text):\n    pattern = r\"(?:^|\\n)(#{1,3} .+?)(?=\\n\\s*\\n|$)\"\n    matches = re.findall(pattern, text, flags=re.DOTALL)\n    return \"\\n\\n\".join(match.strip() for match in matches)\ndef save_list_to_jsonl(data_list, file_path):\n    with open(file_path, 'w', encoding='utf-8') as f:\n        for item in data_list:\n            json_line = json.dumps(item, ensure_ascii=False)\n            f.write(json_line + '\\n')\ndef function_translate(data_dict, title, save_path, db_data_dict):",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "save_list_to_jsonl",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def save_list_to_jsonl(data_list, file_path):\n    with open(file_path, 'w', encoding='utf-8') as f:\n        for item in data_list:\n            json_line = json.dumps(item, ensure_ascii=False)\n            f.write(json_line + '\\n')\ndef function_translate(data_dict, title, save_path, db_data_dict):\n    result_list = []\n    result = {\"title\": title, \"papers\": [], \"outline\": \"\", \"content\": \"\"}\n    content = insert_zero_after_first_hash(data_dict[\"survey\"])\n    result[\"content\"] = content",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "function_translate",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def function_translate(data_dict, title, save_path, db_data_dict):\n    result_list = []\n    result = {\"title\": title, \"papers\": [], \"outline\": \"\", \"content\": \"\"}\n    content = insert_zero_after_first_hash(data_dict[\"survey\"])\n    result[\"content\"] = content\n    outline = extract_headings(data_dict[\"survey\"])\n    outline = insert_zero_after_first_hash(outline)\n    result[\"outline\"] = outline\n    for id_name in data_dict[\"reference\"]:\n        arxiv_id = data_dict[\"reference\"][id_name]",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "batch_translate_folder",
        "kind": 2,
        "importPath": "eval.AutoSurvey_trainslate_main_file",
        "description": "eval.AutoSurvey_trainslate_main_file",
        "peekOfCode": "def batch_translate_folder(json_folder_path, db_json_path, output_folder_path):\n    db_data = load_json_as_dict(db_json_path)\n    os.makedirs(output_folder_path, exist_ok=True)\n    for filename in os.listdir(json_folder_path):\n        if filename.endswith('.json'):\n            json_file_path = os.path.join(json_folder_path, filename)\n            title = os.path.splitext(filename)[0]\n            output_file_path = os.path.join(output_folder_path, title + \".jsonl\")\n            try:\n                autosurvey_data = load_json_as_dict(json_file_path)",
        "detail": "eval.AutoSurvey_trainslate_main_file",
        "documentation": {}
    },
    {
        "label": "find_result_csv_files",
        "kind": 2,
        "importPath": "eval.calculate_mean",
        "description": "eval.calculate_mean",
        "peekOfCode": "def find_result_csv_files(root_dir):\n    result_files = []\n    for dirpath, _, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if filename == \"result.csv\":\n                result_files.append(os.path.join(dirpath, filename))\n    return result_files\ndef parse_and_accumulate(csv_file, sums, counts):\n    with open(csv_file, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)",
        "detail": "eval.calculate_mean",
        "documentation": {}
    },
    {
        "label": "parse_and_accumulate",
        "kind": 2,
        "importPath": "eval.calculate_mean",
        "description": "eval.calculate_mean",
        "peekOfCode": "def parse_and_accumulate(csv_file, sums, counts):\n    with open(csv_file, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        for id,row in enumerate(reader):\n            if id < 1:\n                continue\n            numeric_values = [float(x) for x in row[1:-2]]\n            print(numeric_values)\n            for i, value in enumerate(numeric_values):\n                if i in [0, 1] and value == 0.0:",
        "detail": "eval.calculate_mean",
        "documentation": {}
    },
    {
        "label": "compute_averages",
        "kind": 2,
        "importPath": "eval.calculate_mean",
        "description": "eval.calculate_mean",
        "peekOfCode": "def compute_averages(sums, counts):\n    return [s / c if c > 0 else 0.0 for s, c in zip(sums, counts)]\ndef main(root_dir):\n    result_files = find_result_csv_files(root_dir)\n    if not result_files:\n        print(\"No result.csv files found.\")\n        return\n    column_count = 7\n    sums = [0.0] * column_count\n    counts = [0] * column_count",
        "detail": "eval.calculate_mean",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "eval.calculate_mean",
        "description": "eval.calculate_mean",
        "peekOfCode": "def main(root_dir):\n    result_files = find_result_csv_files(root_dir)\n    if not result_files:\n        print(\"No result.csv files found.\")\n        return\n    column_count = 7\n    sums = [0.0] * column_count\n    counts = [0] * column_count\n    for file in result_files:\n        parse_and_accumulate(file, sums, counts)",
        "detail": "eval.calculate_mean",
        "documentation": {}
    },
    {
        "label": "read_json_as_list",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def read_json_as_list(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict):\n            return [data]\n        else:\n            return [data]",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "function_outline",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def function_outline(data_dict):\n    all_str = \"# 0. \" + data_dict[\"title\"] + \"\\n\\n\"\n    for one_idx, one_key in enumerate(data_dict[\"sections\"].keys()):\n        all_str += f\"## {one_idx + 1}. {one_key}\\n\\n\"\n        for two_idx, two_key in enumerate(data_dict[\"sections\"][one_key][\"subsection_info\"].keys()):\n            all_str += f\"### {one_idx + 1}.{two_idx + 1} {two_key}\\n\\n\"\n            try:\n                for three_idx, three_key in enumerate(\n                    data_dict[\"sections\"][one_key][\"subsection_info\"][two_key][\"subsection_info\"].keys()\n                ):",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "number_markdown_headings",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def number_markdown_headings(text: str) -> str:\n    top_count = -1\n    sub_count = 0\n    last_top_number = \"\"\n    def replacer(match):\n        nonlocal top_count, sub_count, last_top_number\n        hashes, title = match.groups()\n        level = len(hashes)\n        if level in [1, 2]:\n            top_count += 1",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "function_main",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def function_main(file_path):\n    data = read_json_as_list(file_path)\n    result_list = []\n    for item in data:\n        result = {\n            \"title\": item[\"final_paper\"][\"paper_title\"],\n            \"papers\": [{\"title\": p[\"title\"], \"txt\": p[\"abstract\"]}\n                       for p in item[\"final_paper\"][\"reportIndexList\"]],\n            \"content\": number_markdown_headings(item[\"final_paper\"][\"markdown_content\"]),\n            \"outline\": function_outline(item[\"outline_structure_wo_query\"])",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "save_list_to_jsonl",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def save_list_to_jsonl(data_list, file_path):\n    try:\n        with open(file_path, 'w', encoding='utf-8') as f:\n            for item in data_list:\n                json_line = json.dumps(item, ensure_ascii=False)\n                f.write(json_line + '\\n')\n        print(f\"Saved to {file_path}\")\n    except Exception as e:\n        print(f\"Failed to save: {e}\")\ndef process_file(input_path):",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "process_file",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def process_file(input_path):\n    input_path = Path(input_path)\n    stem = input_path.stem\n    translated_output_path = output_translate_dir / f\"{stem}_translate.jsonl\"\n    eval_output_path = output_eval_dir / f\"{stem}_eval\"\n    try:\n        # Convert and save\n        data_list = function_main(str(input_path))\n        save_list_to_jsonl(data_list, translated_output_path)\n        # Call the bash eval script",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "get_all_json_files",
        "kind": 2,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "def get_all_json_files(folder_path):\n    \"\"\"\n    Traverse the specified folder and get the full paths of all .json files within, returning them as a list.\n    Parameters:\n        folder_path (str): The folder path to traverse\n    Returns:\n        List[str]: List containing full paths of all .json files\n    \"\"\"\n    json_files = []\n    for root, dirs, files in os.walk(folder_path):",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "input_json_file",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "input_json_file = \"Path to your SciSage jsonl folder\"\ndest_dir = \"Path to your SurveyEval folder\"\n# Create output directories\noutput_translate_dir = Path(f\"./{dest_dir}/translate_data\") # \noutput_eval_dir = Path(f\"./{dest_dir}/eval_data\") # \nlog_dir = Path(f\"./{dest_dir}/logs\") # \noutput_translate_dir.mkdir(parents=True, exist_ok=True)\noutput_eval_dir.mkdir(parents=True, exist_ok=True)\nlog_dir.mkdir(parents=True, exist_ok=True)\n# Log files",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "dest_dir",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "dest_dir = \"Path to your SurveyEval folder\"\n# Create output directories\noutput_translate_dir = Path(f\"./{dest_dir}/translate_data\") # \noutput_eval_dir = Path(f\"./{dest_dir}/eval_data\") # \nlog_dir = Path(f\"./{dest_dir}/logs\") # \noutput_translate_dir.mkdir(parents=True, exist_ok=True)\noutput_eval_dir.mkdir(parents=True, exist_ok=True)\nlog_dir.mkdir(parents=True, exist_ok=True)\n# Log files\nfinished_log = log_dir / \"finished.log\"",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "output_translate_dir",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "output_translate_dir = Path(f\"./{dest_dir}/translate_data\") # \noutput_eval_dir = Path(f\"./{dest_dir}/eval_data\") # \nlog_dir = Path(f\"./{dest_dir}/logs\") # \noutput_translate_dir.mkdir(parents=True, exist_ok=True)\noutput_eval_dir.mkdir(parents=True, exist_ok=True)\nlog_dir.mkdir(parents=True, exist_ok=True)\n# Log files\nfinished_log = log_dir / \"finished.log\"\nfailed_log = log_dir / \"failed.log\"\ndef read_json_as_list(file_path):",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "output_eval_dir",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "output_eval_dir = Path(f\"./{dest_dir}/eval_data\") # \nlog_dir = Path(f\"./{dest_dir}/logs\") # \noutput_translate_dir.mkdir(parents=True, exist_ok=True)\noutput_eval_dir.mkdir(parents=True, exist_ok=True)\nlog_dir.mkdir(parents=True, exist_ok=True)\n# Log files\nfinished_log = log_dir / \"finished.log\"\nfailed_log = log_dir / \"failed.log\"\ndef read_json_as_list(file_path):\n    try:",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "log_dir = Path(f\"./{dest_dir}/logs\") # \noutput_translate_dir.mkdir(parents=True, exist_ok=True)\noutput_eval_dir.mkdir(parents=True, exist_ok=True)\nlog_dir.mkdir(parents=True, exist_ok=True)\n# Log files\nfinished_log = log_dir / \"finished.log\"\nfailed_log = log_dir / \"failed.log\"\ndef read_json_as_list(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "finished_log",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "finished_log = log_dir / \"finished.log\"\nfailed_log = log_dir / \"failed.log\"\ndef read_json_as_list(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict):\n            return [data]",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "failed_log",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "failed_log = log_dir / \"failed.log\"\ndef read_json_as_list(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        if isinstance(data, list):\n            return data\n        elif isinstance(data, dict):\n            return [data]\n        else:",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "input_json_paths",
        "kind": 5,
        "importPath": "eval.eval_scisage",
        "description": "eval.eval_scisage",
        "peekOfCode": "input_json_paths = get_all_json_files(input_json_file)\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_file, path) for path in input_json_paths]\n    for future in as_completed(futures):\n        pass\nprint(\"Completed\")",
        "detail": "eval.eval_scisage",
        "documentation": {}
    },
    {
        "label": "ModelConfig",
        "kind": 6,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "class ModelConfig:\n    \"\"\"Configuration for LLM models\"\"\"\n    url: str\n    max_len: int\n    temperature: float = 0.8\n    top_p: float = 0.9\n    retry_attempts: int = 20\n    timeout: int = 200\n    think_bool: bool = False\n    openai_client: Optional[Any] = None",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "LLMClient",
        "kind": 6,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "class LLMClient:\n    def __init__(self):\n        self.session = self._create_session()\n        self.cache = TTLCache(maxsize=100, ttl=3600)  # Cache responses for 1 hour\n    def _create_session(self) -> requests.Session:\n        \"\"\"Create a session with retry logic\"\"\"\n        session = requests.Session()\n        retries = Retry(\n            total=3,\n            backoff_factor=0.5,",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "get_from_llm",
        "kind": 2,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "def get_from_llm(\n    messages: Union[str, List[Dict[str, str]]], model_name: str = \"Qwen25-7B\", **kwargs\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM with improved error handling and retries.\n    Args:\n        messages: Input messages (string or list of message dicts)\n        model_name: Name of the model to use\n        **kwargs: Additional parameters to override defaults\n    Returns:",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for LLM models\"\"\"\n    url: str\n    max_len: int\n    temperature: float = 0.8\n    top_p: float = 0.9\n    retry_attempts: int = 20\n    timeout: int = 200",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "MODEL_CONFIGS",
        "kind": 5,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "MODEL_CONFIGS = {\n    \"your_model\": ModelConfig(\n        url=\"\",\n        max_len=32768,\n        think_bool=False,\n        openai_client=OpenAI(\n            api_key=\"EMPTY\",\n            base_url=\"\",\n        ))\n}",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "client",
        "kind": 5,
        "importPath": "eval.local_request",
        "description": "eval.local_request",
        "peekOfCode": "client = LLMClient()\n@func_set_timeout(200)\ndef get_from_llm(\n    messages: Union[str, List[Dict[str, str]]], model_name: str = \"Qwen25-7B\", **kwargs\n) -> Optional[str]:\n    \"\"\"\n    Get response from LLM with improved error handling and retries.\n    Args:\n        messages: Input messages (string or list of message dicts)\n        model_name: Name of the model to use",
        "detail": "eval.local_request",
        "documentation": {}
    },
    {
        "label": "remove_version_tags",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def remove_version_tags(text):\n    \"\"\"\n    Remove 'v1', 'v2', 'v3' from a string.\n    Args:\n        text (str): input string\n    Returns:\n        str: string after removing version tags\n    \"\"\"\n    return re.sub(r'\\bv[1-3]\\b', '', text)\ndef string_similarity(s1, s2):",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "string_similarity",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def string_similarity(s1, s2):\n    s1 = s1.lower()\n    s2 = s2.lower()\n    return SequenceMatcher(None, s1, s2).ratio()\ndef is_similar(sentence, paragraph, threshold=0.8):\n    sentence = sentence.strip().lower()\n    sub_sentences = re.split(r'[.!?\\n]', paragraph.lower())\n    for sub in sub_sentences:\n        if string_similarity(sentence, sub.strip()) >= threshold:\n            print(\"sentence:\", sentence)",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "is_similar",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def is_similar(sentence, paragraph, threshold=0.8):\n    sentence = sentence.strip().lower()\n    sub_sentences = re.split(r'[.!?\\n]', paragraph.lower())\n    for sub in sub_sentences:\n        if string_similarity(sentence, sub.strip()) >= threshold:\n            print(\"sentence:\", sentence)\n            print(\"sub:\", sub.strip())\n            return 1\n    return 0\ndef read_jsonl_file(filepath):",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "read_jsonl_file",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def read_jsonl_file(filepath):\n    \"\"\"\n    Read a .jsonl file and return a list of parsed JSON objects, one per line.\n    Args:\n        filepath (str): path to the .jsonl file\n    Returns:\n        List[dict]: list of JSON objects from each line\n    \"\"\"\n    data = []\n    with open(filepath, 'r', encoding='utf-8') as f:",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "write_jsonl",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def write_jsonl(data_list, save_path):\n    \"\"\"\n    Write a list of dictionaries to a JSONL file.\n    Args:\n    - data_list: List[dict], data to write, each dict is one JSON line.\n    - save_path: str, path to save the file, e.g., \"output.jsonl\"\n    \"\"\"\n    with open(save_path, 'w', encoding='utf-8') as f:\n        for item in data_list:\n            json_line = json.dumps(item, ensure_ascii=False)",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "extract_all_second_sentences",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def extract_all_second_sentences(text):\n    \"\"\"\n    Extract from the text:\n    1. The second sentence following any [xxx] or xxx brackets;\n    2. The second sentence of every paragraph after line breaks.\n    Returns a list of all extracted sentences.\n    \"\"\"\n    def get_second_sentence(fragment):\n        # Simple sentence splitting: split by . ! ? followed by space or end of string\n        sentences = re.split(r'(?<=[.?!])\\s+', fragment.strip())",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "load_all_jsonl_from_folder",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def load_all_jsonl_from_folder(folder_path):\n    \"\"\"\n    Traverse all .jsonl files in a folder and merge their contents into a single list.\n    Args:\n        folder_path: path to the folder containing multiple .jsonl files\n    Returns:\n        list: combined list of all entries from all .jsonl files\n    \"\"\"\n    all_data = []\n    for filename in os.listdir(folder_path):",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "clean_arxiv_url",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def clean_arxiv_url(url):\n    \"\"\"\n    Remove version numbers (like v1, v2, v3) from arXiv URLs.\n    Args:\n        url (str): string containing an arXiv URL\n    Returns:\n        str: URL with version numbers removed\n    \"\"\"\n    return re.sub(r'(http://arxiv\\.org/abs/\\d+\\.\\d+)(v\\d+)', r'\\1', url)\ndef optimized_similarity_match(datas_SurveyScope, generate_papers_file_path):",
        "detail": "eval.reference_eval",
        "documentation": {}
    },
    {
        "label": "optimized_similarity_match",
        "kind": 2,
        "importPath": "eval.reference_eval",
        "description": "eval.reference_eval",
        "peekOfCode": "def optimized_similarity_match(datas_SurveyScope, generate_papers_file_path):\n    reference_n = 0\n    SurveyScope_references_n = 0\n    paper_references_n = 0\n    # Step 1:  .jsonl \n    jsonl_files = []\n    for root, _, files in os.walk(generate_papers_file_path):\n        for filename in files:\n            if filename.endswith('.jsonl'):\n                jsonl_files.append((filename, os.path.join(root, filename)))",
        "detail": "eval.reference_eval",
        "documentation": {}
    }
]