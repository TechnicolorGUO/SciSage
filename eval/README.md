# 📘 Evaluation

This project contains multiple scripts designed to help convert survey paper data generated by systems like **AutoSurvey** and **SciSage** into formats compatible with the **LLMxMapReduce_v2** evaluation framework, and to perform both automatic and manual evaluations.

For details on the LLMxMapReduce_v2 evaluation method, please refer to:
https://github.com/thunlp/LLMxMapReduce/tree/main/LLMxMapReduce_V2

---

## 🧩 Script Descriptions & Usage

### ✅ 1. `AutoSurvey_trainslate_main_file.py`

Converts JSON files generated by AutoSurvey into JSONL files that can be evaluated using LLMxMapReduce_v2.

#### 📌 Functionality
AutoSurvey's `.json` files include both the `survey` content and referenced papers (`reference`). This script restructures them into the following format:

- **`title`**  : Title of the paper
- **`outline`** : Extracted and standardized outline
- **`content`** : Main body content
- **`papers`**  : Titles and abstracts of cited papers

#### 🚀 Usage
Before evaluation, download the AutoSurvey paper database file `arxiv_paper_db.json` to complete reference information:
https://1drv.ms/u/c/8761b6d10f143944/EaqWZ4_YMLJIjGsEB_qtoHsBoExJ8bdppyBc1uxgijfZBw?e=2EIzti

Modify the following paths in the `main` function:

```python
db_path = 'path/to/arxiv_paper_db.json'
autosurvey_json_folder = 'path/to/AutoSurvey/output/folder'
output_folder = 'path/to/converted/output/folder'
```

Then run:

```bash
python AutoSurvey_trainslate_main_file.py
```

---

### 📂 2. `eval_all_our.sh`

Batch-evaluates all `.jsonl` files in a folder using the LLMxMapReduce2 evaluation method.

Before evaluation, make sure to download `punkt_tab`:

```python
import nltk
nltk.download('punkt_tab')
```

#### 🚀 Usage
Both local model and API-based evaluation are supported. You can add local model configurations to `MODEL_CONFIGS` in `local_request_v2.py`.

Modify the `APIModel.chat()` and `APIModel.__chat()` functions in `./evaluation/API/model.py` to call your desired model:

```python
get_from_llm(text, model_name="your_model")
```

After configuration, simply run:

```bash
bash eval_all_our.sh "path/to/folder_to_evaluate"
```

---

### 📊 3. `calculate_mean.py`

Traverses a folder containing LLMxMapReduce_v2 evaluation results and computes the average from all `result.csv` files.

#### 🚀 Usage

```bash
python calculate_mean.py "path/to/eval_folder"
```

---

### 🔁 4. `ourdata_eval.py`

Converts data generated by SciSage or other systems into the SurveyEval-compatible evaluation format and runs automatic evaluation using LLMxMapReduce_v2.

#### 🚀 Usage

Modify the following paths in the `main` function:

```python
input_json_file = "path/to/generated_scisage_json"
output_translate_dir = "path/to/translated_output"
output_eval_dir     = "path/to/eval_results_output"
log_dir             = "path/to/log_directory"
```

Then run:

```bash
python ourdata_eval.py
```

---

### 📑 5. `reference_eval.py`

Evaluates the overlap between references in generated survey papers and human-written references in the test set. Outputs include overlap count, overlap ratio, and F1 score.

#### 🚀 Usage
Before evaluation, download the SurveyEval and SurveyScope datasets:

- SurveyEval test set:
  https://huggingface.co/datasets/R0k1e/SurveyEval
- SurveyScope benchmark:
  https://huggingface.co/datasets/BAAI/SurveyScope

Modify the following paths in the `main` function:

```python
jsonl_path = "path/to/SurveyEval_test.jsonl"
jsonl_path_ourBenchMark = "path/to/SurveyScope.jsonl"
our_data_papers_file_path = "path/to/generated_papers_folder"
```

Then run:

```bash
python reference_eval.py
```

---

## ⚠️ Notes

1. Use English-only paths; avoid Chinese characters or spaces.
2. Input files must be in `.json` or `.jsonl` format.
3. `db_path` refers to the AutoSurvey paper database.
4. All outputs are in `.jsonl` format to ensure compatibility with the LLMxMapReduce_v2 evaluation system.
