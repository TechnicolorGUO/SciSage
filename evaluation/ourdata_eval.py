import os
import json
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import subprocess


"""
This script is mainly used to convert the data generated by SciSage into a format compatible with SurveyEval for evaluation. 
You only need to modify the following paths: output_translate_dir, output_eval_dir, and log_dir.
"""

input_json_file = "Path to your SciSage jsonl folder"

# Create output directories
output_translate_dir = Path("./surveryeval_v1/translate_data")
output_eval_dir = Path("./surveryeval_v1/eval_data")
log_dir = Path("./surveryeval_v1/logs")
output_translate_dir.mkdir(parents=True, exist_ok=True)
output_eval_dir.mkdir(parents=True, exist_ok=True)
log_dir.mkdir(parents=True, exist_ok=True)

# Log files
finished_log = log_dir / "finished.log"
failed_log = log_dir / "failed.log"


def read_json_as_list(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            return [data]
        else:
            return [data]
    except Exception as e:
        print(f"Failed to read: {e}")
        return []


def function_outline(data_dict):
    all_str = "# 0. " + data_dict["title"] + "\n\n"
    for one_idx, one_key in enumerate(data_dict["sections"].keys()):
        all_str += f"## {one_idx + 1}. {one_key}\n\n"
        for two_idx, two_key in enumerate(data_dict["sections"][one_key]["subsection_info"].keys()):
            all_str += f"### {one_idx + 1}.{two_idx + 1} {two_key}\n\n"
            try:
                for three_idx, three_key in enumerate(
                    data_dict["sections"][one_key]["subsection_info"][two_key]["subsection_info"].keys()
                ):
                    all_str += f"#### {one_idx + 1}.{two_idx + 1}.{three_idx + 1} {three_key}\n\n"
            except KeyError as e:
                print(f"Error occurred: {e}")
    return all_str


def number_markdown_headings(text: str) -> str:
    top_count = -1
    sub_count = 0
    last_top_number = ""

    def replacer(match):
        nonlocal top_count, sub_count, last_top_number
        hashes, title = match.groups()
        level = len(hashes)
        if level in [1, 2]:
            top_count += 1
            sub_count = 0
            last_top_number = f"{top_count}"
            return f"{hashes} {top_count}. {title.strip()}"
        elif level == 3:
            sub_count += 1
            return f"### {last_top_number}.{sub_count} {title.strip()}"
        else:
            return match.group(0)

    pattern = re.compile(r'^(#{1,3})\s+(.*)$', flags=re.MULTILINE)
    return pattern.sub(replacer, text)


def function_main(file_path):
    data = read_json_as_list(file_path)
    result_list = []
    for item in data:
        result = {
            "title": item["final_paper"]["paper_title"],
            "papers": [{"title": p["title"], "txt": p["abstract"]}
                       for p in item["final_paper"]["reportIndexList"]],
            "content": number_markdown_headings(item["final_paper"]["markdown_content"]),
            "outline": function_outline(item["outline_structure_wo_query"])
        }
        result_list.append(result)
    return result_list


def save_list_to_jsonl(data_list, file_path):
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in data_list:
                json_line = json.dumps(item, ensure_ascii=False)
                f.write(json_line + '\n')
        print(f"Saved to {file_path}")
    except Exception as e:
        print(f"Failed to save: {e}")


def process_file(input_path):
    input_path = Path(input_path)
    stem = input_path.stem
    translated_output_path = output_translate_dir / f"{stem}_translate.jsonl"
    eval_output_path = output_eval_dir / f"{stem}_eval"

    try:
        # Convert and save
        data_list = function_main(str(input_path))
        save_list_to_jsonl(data_list, translated_output_path)

        # Call the bash eval script
        result = subprocess.run(
            ["bash", "eval_all.sh", str(translated_output_path), str(eval_output_path)],
            capture_output=True, text=True
        )

        # Success or failure handling
        if result.returncode == 0:
            with open(finished_log, "a", encoding="utf-8") as f:
                f.write(f"{input_path}\n")
        else:
            with open(failed_log, "a", encoding="utf-8") as f:
                f.write(f"{input_path}\n")
            print(f"❌ Failed: {input_path}\n{result.stderr}")
    except Exception as e:
        with open(failed_log, "a", encoding="utf-8") as f:
            f.write(f"{input_path}\n")
        print(f"❌ Failed: {input_path}\nException: {e}")


def get_all_json_files(folder_path):
    """
    Traverse the specified folder and get the full paths of all .json files within, returning them as a list.

    Parameters:
        folder_path (str): The folder path to traverse

    Returns:
        List[str]: List containing full paths of all .json files
    """
    json_files = []
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.json'):
                json_files.append(os.path.join(root, file))
    return json_files


# Run concurrent processing
input_json_paths = get_all_json_files(input_json_file)

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(process_file, path) for path in input_json_paths]
    for future in as_completed(futures):
        pass

print("Completed")
